"""`evalvault run` ëª…ë ¹ ì „ìš© Typer ë“±ë¡ ëª¨ë“ˆ."""

from __future__ import annotations

import asyncio
from collections.abc import Sequence
from datetime import datetime
from pathlib import Path
from typing import Any

import click
import typer
from rich.console import Console

from evalvault.adapters.outbound.dataset import get_loader
from evalvault.adapters.outbound.domain_memory.sqlite_adapter import SQLiteDomainMemoryAdapter
from evalvault.adapters.outbound.llm import get_llm_adapter
from evalvault.adapters.outbound.phoenix.sync_service import (
    PhoenixDatasetInfo,
    PhoenixSyncError,
    PhoenixSyncService,
    build_experiment_metadata,
)
from evalvault.adapters.outbound.storage.sqlite_adapter import SQLiteStorageAdapter
from evalvault.adapters.outbound.tracer.phoenix_tracer_adapter import PhoenixTracerAdapter
from evalvault.config.phoenix_support import ensure_phoenix_instrumentation
from evalvault.config.settings import Settings, apply_profile
from evalvault.domain.services.evaluator import RagasEvaluator
from evalvault.domain.services.memory_aware_evaluator import MemoryAwareEvaluator
from evalvault.domain.services.memory_based_analysis import MemoryBasedAnalysis
from evalvault.domain.services.prompt_registry import (
    PromptInput,
    build_prompt_bundle,
    build_prompt_summary,
)
from evalvault.domain.services.ragas_prompt_overrides import (
    PromptOverrideError,
    load_ragas_prompt_overrides,
)
from evalvault.domain.services.stage_event_builder import StageEventBuilder
from evalvault.ports.outbound.korean_nlp_port import RetrieverPort

from ..utils.console import print_cli_error, print_cli_warning, progress_spinner
from ..utils.options import db_option, memory_db_option, profile_option
from ..utils.presets import format_preset_help, get_preset, list_presets
from ..utils.progress import evaluation_progress, streaming_progress
from ..utils.validators import parse_csv_option, validate_choice, validate_choices
from . import run_helpers
from .run_helpers import (
    RUN_MODE_PRESETS,
    _build_streaming_dataset_template,
    _collect_prompt_metadata,
    _display_memory_insights,
    _display_results,
    _evaluate_streaming_run,
    _is_oss_open_model,
    _log_to_tracker,
    _option_was_provided,
    _print_run_mode_banner,
    _resolve_thresholds,
    _save_results,
    _save_to_db,
    _write_stage_events_jsonl,
    enrich_dataset_with_memory,
    format_dataset_preprocess_summary,
    load_knowledge_graph,
    load_retriever_documents,
    log_phoenix_traces,
)

DEFAULT_RUN_MODE = "full"
_merge_evaluation_runs = run_helpers._merge_evaluation_runs
apply_retriever_to_dataset = run_helpers.apply_retriever_to_dataset


def _build_dense_retriever(
    *,
    documents: list[str],
    settings: Settings,
    profile_name: str | None,
) -> Any:
    """Build and index a dense retriever, preferring Ollama embeddings when available."""

    from evalvault.adapters.outbound.nlp.korean.dense_retriever import KoreanDenseRetriever

    embedding_model = settings.ollama_embedding_model
    if settings.llm_provider == "ollama":
        model_info = KoreanDenseRetriever.SUPPORTED_MODELS.get(embedding_model)
        if model_info and model_info.get("type") == "ollama":
            from evalvault.adapters.outbound.llm.ollama_adapter import OllamaAdapter

            ollama_adapter = OllamaAdapter(settings)
            if profile_name in {"dev", "prod"}:
                dense_retriever = KoreanDenseRetriever(
                    profile=profile_name,
                    ollama_adapter=ollama_adapter,
                )
            else:
                dense_retriever = KoreanDenseRetriever(
                    model_name=embedding_model,
                    ollama_adapter=ollama_adapter,
                )
            dense_retriever.index(documents)
            return dense_retriever

    dense_retriever = KoreanDenseRetriever()
    dense_retriever.index(documents)
    return dense_retriever


def _log_timestamp(console: Console, verbose: bool, message: str) -> None:
    if not verbose:
        return
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    console.print(f"[dim]{timestamp} {message}[/dim]")


def _log_duration(
    console: Console,
    verbose: bool,
    message: str,
    started_at: datetime,
) -> None:
    if not verbose:
        return
    elapsed = (datetime.now() - started_at).total_seconds()
    _log_timestamp(console, verbose, f"{message} ({elapsed:.2f}s)")


def register_run_commands(
    app: typer.Typer,
    console: Console,
    available_metrics: Sequence[str],
) -> None:
    """Attach the legacy `run` command to the given Typer app."""

    @app.command()
    def run(  # noqa: PLR0913 - CLI arguments intentionally flat
        dataset: Path = typer.Argument(
            ...,
            help="Path to dataset file (CSV, Excel, or JSON).",
            exists=True,
            readable=True,
        ),
        evaluation_preset: str | None = typer.Option(
            None,
            "--preset",
            help=(
                "Use a preset configuration (quick/production/summary/comprehensive). "
                f"{format_preset_help()}"
            ),
        ),
        summary: bool = typer.Option(
            False,
            "--summary",
            help=(
                "Enable summarization evaluation preset "
                "(summary_score, summary_faithfulness, entity_preservation)."
            ),
            rich_help_panel="Simple mode preset",
        ),
        metrics: str = typer.Option(
            "faithfulness,answer_relevancy",
            "--metrics",
            "-m",
            help="Comma-separated list of metrics to evaluate. Overrides preset if both are specified.",
            rich_help_panel="Simple mode preset",
        ),
        threshold_profile: str | None = typer.Option(
            None,
            "--threshold-profile",
            help="Apply a threshold profile (summary/qa) to matching metrics.",
            rich_help_panel="Full mode options",
        ),
        profile: str | None = profile_option(
            help_text="Model profile (dev, prod, openai). Overrides .env setting.",
        ),
        model: str | None = typer.Option(
            None,
            "--model",
            help="Model to use for evaluation (overrides profile).",
        ),
        output: Path | None = typer.Option(
            None,
            "--output",
            "-o",
            help="Output file for results (JSON format).",
        ),
        retriever: str | None = typer.Option(
            None,
            "--retriever",
            "-r",
            help="Retriever to fill empty contexts (bm25, dense, hybrid, graphrag).",
            rich_help_panel="Full mode options",
        ),
        retriever_docs: Path | None = typer.Option(
            None,
            "--retriever-docs",
            help="Documents file for retriever (.json/.jsonl/.txt).",
            rich_help_panel="Full mode options",
        ),
        kg: Path | None = typer.Option(
            None,
            "--kg",
            "-k",
            help="Knowledge graph JSON file for GraphRAG retriever.",
            rich_help_panel="Full mode options",
        ),
        retriever_top_k: int = typer.Option(
            5,
            "--retriever-top-k",
            help="Top-K documents to retrieve (default: 5).",
            rich_help_panel="Full mode options",
        ),
        stage_events: Path | None = typer.Option(
            None,
            "--stage-events",
            help="Write stage events as JSONL for later ingestion.",
        ),
        stage_store: bool = typer.Option(
            False,
            "--stage-store/--no-stage-store",
            help="Store stage events in the SQLite database (requires --db).",
        ),
        tracker: str = typer.Option(
            "none",
            "--tracker",
            "-t",
            help="Tracker to log results: 'langfuse', 'mlflow', 'phoenix', or 'none'.",
            rich_help_panel="Simple mode preset",
        ),
        langfuse: bool = typer.Option(
            False,
            "--langfuse",
            "-l",
            help="[Deprecated] Use --tracker langfuse instead.",
            hidden=True,
        ),
        phoenix_max_traces: int | None = typer.Option(
            None,
            "--phoenix-max-traces",
            help="Max per-test-case traces to send to Phoenix (default: send all).",
            rich_help_panel="Full mode options",
        ),
        phoenix_dataset: str | None = typer.Option(
            None,
            "--phoenix-dataset",
            help="Upload the dataset/test cases to Phoenix under this name.",
            rich_help_panel="Full mode options",
        ),
        phoenix_dataset_description: str | None = typer.Option(
            None,
            "--phoenix-dataset-description",
            help="Description stored on the Phoenix dataset (default: dataset metadata).",
            rich_help_panel="Full mode options",
        ),
        phoenix_experiment: str | None = typer.Option(
            None,
            "--phoenix-experiment",
            help="Create a Phoenix experiment record for this run (requires dataset upload).",
            rich_help_panel="Full mode options",
        ),
        phoenix_experiment_description: str | None = typer.Option(
            None,
            "--phoenix-experiment-description",
            help="Description stored on the Phoenix experiment.",
            rich_help_panel="Full mode options",
        ),
        prompt_manifest: Path | None = typer.Option(
            Path("agent/prompts/prompt_manifest.json"),
            "--prompt-manifest",
            help="Path to Phoenix prompt manifest JSON.",
            rich_help_panel="Full mode options",
        ),
        prompt_files: str | None = typer.Option(
            None,
            "--prompt-files",
            help="Comma-separated prompt files to capture in Phoenix metadata.",
            rich_help_panel="Full mode options",
        ),
        prompt_set_name: str | None = typer.Option(
            None,
            "--prompt-set-name",
            help="Name for the prompt set snapshot stored in the DB.",
            rich_help_panel="Full mode options",
        ),
        prompt_set_description: str | None = typer.Option(
            None,
            "--prompt-set-description",
            help="Description for the prompt set snapshot.",
            rich_help_panel="Full mode options",
        ),
        system_prompt: str | None = typer.Option(
            None,
            "--system-prompt",
            help="System prompt text for the target LLM (stored for comparison).",
            rich_help_panel="Full mode options",
        ),
        system_prompt_file: Path | None = typer.Option(
            None,
            "--system-prompt-file",
            help="Path to a system prompt file to store alongside this run.",
            rich_help_panel="Full mode options",
        ),
        system_prompt_name: str | None = typer.Option(
            None,
            "--system-prompt-name",
            help="Optional name for the system prompt snapshot.",
            rich_help_panel="Full mode options",
        ),
        ragas_prompts: Path | None = typer.Option(
            None,
            "--ragas-prompts",
            help="YAML file with Ragas metric prompt overrides.",
            rich_help_panel="Full mode options",
        ),
        mode: str = typer.Option(
            DEFAULT_RUN_MODE,
            "--mode",
            help="ì‹¤í–‰ ëª¨ë“œ ì„ íƒ: 'simple'ì€ ê°„í¸ ì‹¤í–‰, 'full'ì€ ëª¨ë“  ì˜µì…˜ ë…¸ì¶œ.",
            rich_help_panel="Run modes",
        ),
        db_path: Path | None = db_option(
            help_text="Path to SQLite database file for storing results.",
        ),
        use_domain_memory: bool = typer.Option(
            False,
            "--use-domain-memory",
            help="Leverage Domain Memory for threshold adjustment and insights.",
            rich_help_panel="Domain Memory (full mode)",
        ),
        memory_domain: str | None = typer.Option(
            None,
            "--memory-domain",
            help="Domain name for Domain Memory (defaults to dataset metadata).",
            rich_help_panel="Domain Memory (full mode)",
        ),
        memory_language: str = typer.Option(
            "ko",
            "--memory-language",
            help="Language code for Domain Memory lookups (default: ko).",
            rich_help_panel="Domain Memory (full mode)",
        ),
        memory_db: Path = memory_db_option(
            help_text="Path to Domain Memory database (default: data/db/evalvault_memory.db).",
        ),
        memory_augment_context: bool = typer.Option(
            False,
            "--augment-context",
            help="Append retrieved factual memories to each test case context.",
            rich_help_panel="Domain Memory (full mode)",
        ),
        verbose: bool = typer.Option(
            False,
            "--verbose",
            "-v",
            help="Show detailed output.",
        ),
        parallel: bool = typer.Option(
            False,
            "--parallel",
            "-P",
            help="Enable parallel evaluation for faster processing.",
        ),
        batch_size: int = typer.Option(
            5,
            "--batch-size",
            "-b",
            help="Batch size for parallel evaluation (default: 5).",
        ),
        stream: bool = typer.Option(
            False,
            "--stream",
            "-s",
            help="Enable streaming evaluation for large datasets (process file in chunks).",
        ),
        stream_chunk_size: int = typer.Option(
            200,
            "--stream-chunk-size",
            help="Chunk size when streaming evaluation is enabled (default: 200).",
        ),
    ) -> None:
        """Run RAG evaluation on a dataset.

        \b
        Run Modes:
          â€¢ Simple â€” Safe defaults (2 metrics + Phoenix tracker + no Domain Memory).
          â€¢ Full â€” Expose all prompt/Domain Memory/streaming options.

        \b
        Presets:
          â€¢ quick â€” Fast iteration with faithfulness metric only.
          â€¢ production â€” Balanced evaluation with 4 core metrics.
          â€¢ summary â€” Summarization evaluation with 3 summary-focused metrics.
          â€¢ comprehensive â€” Complete evaluation with all 6 metrics.

        \b
        Examples:
          # Basic evaluation with default metrics
          evalvault run data.json -m faithfulness

          # Use preset for quick iteration
          evalvault run --preset quick dataset.json

          # Summarization evaluation
          evalvault run --summary dataset.json

          # Production run with JSON output
          evalvault run --preset production dataset.json -o results.json

          # With retriever (auto-fill contexts)
          evalvault run questions.json -r hybrid --retriever-docs docs.json

          # Full mode with Domain Memory
          evalvault run --mode full data.json --use-domain-memory

          # Parallel evaluation for faster processing
          evalvault run data.json -m faithfulness -P -b 10

          # Streaming for large datasets
          evalvault run large.json -m faithfulness --stream

        \b
        See also:
          evalvault metrics     â€” List available metrics
          evalvault history     â€” View past evaluation runs
          evalvault analyze     â€” Analyze run results
          evalvault benchmark   â€” Run retrieval benchmarks
        """
        try:
            ctx = click.get_current_context()
        except RuntimeError:
            ctx = None
        alias_invoked = ctx.meta.get("run_mode_alias") if ctx else None
        run_mode_value = (mode or DEFAULT_RUN_MODE).lower()
        preset = RUN_MODE_PRESETS.get(run_mode_value)
        if not preset:
            print_cli_error(
                console,
                "--mode ê°’ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.",
                fixes=[f"ì‚¬ìš© ê°€ëŠ¥: {', '.join(sorted(RUN_MODE_PRESETS))}"],
            )
            raise typer.Exit(2)

        if (
            preset.name == "simple"
            or _option_was_provided(ctx, "mode")
            or alias_invoked is not None
        ):
            _print_run_mode_banner(console, preset)

        summary_flag = summary
        if summary_flag and preset.default_metrics:
            print_cli_warning(
                console,
                "Simple ëª¨ë“œì—ì„œëŠ” ìš”ì•½ í‰ê°€ ì˜µì…˜ì´ ì ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
                tips=["--mode fullë¡œ ì „í™˜í•´ ìš”ì•½ ë©”íŠ¸ë¦­ì„ ì‚¬ìš©í•˜ì„¸ìš”."],
            )

        if summary_flag and evaluation_preset and evaluation_preset.lower() != "summary":
            print_cli_error(
                console,
                "--summary ì˜µì…˜ì€ ë‹¤ë¥¸ presetê³¼ í•¨ê»˜ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                fixes=["--summary ë˜ëŠ” --preset summary ì¤‘ í•˜ë‚˜ë§Œ ì‚¬ìš©í•˜ì„¸ìš”."],
            )
            raise typer.Exit(1)

        if summary_flag and not preset.default_metrics:
            evaluation_preset = evaluation_preset or "summary"

        # Handle evaluation preset
        eval_preset_config = None
        if evaluation_preset:
            eval_preset_config = get_preset(evaluation_preset)
            if not eval_preset_config:
                print_cli_error(
                    console,
                    f"Invalid preset: {evaluation_preset}",
                    fixes=[
                        f"Available presets: {', '.join(list_presets())}",
                        "Run 'evalvault run --help' to see preset descriptions",
                    ],
                )
                raise typer.Exit(1)
            console.print(
                f"[dim]Using preset '{eval_preset_config.name}': {eval_preset_config.description}[/dim]"
            )

        metric_list = parse_csv_option(metrics)
        metrics_override = _option_was_provided(ctx, "metrics")
        if summary_flag and metrics_override:
            print_cli_warning(
                console,
                "--metricsê°€ ì§€ì •ë˜ì–´ ìš”ì•½ í”„ë¦¬ì…‹ ì ìš©ì„ ê±´ë„ˆëœë‹ˆë‹¤.",
                tips=["ìš”ì•½ ì „ìš© ë©”íŠ¸ë¦­ì„ ì‚¬ìš©í•˜ë ¤ë©´ --metrics ì˜µì…˜ì„ ì œê±°í•˜ì„¸ìš”."],
            )

        # Apply preset metrics if preset is specified and metrics not explicitly overridden
        if eval_preset_config and not metrics_override:
            metric_list = list(eval_preset_config.metrics)
            console.print(f"[dim]Preset metrics: {', '.join(metric_list)}[/dim]")
        if preset.default_metrics:
            preset_metrics = list(preset.default_metrics)
            if metrics_override and set(metric_list) != set(preset_metrics):
                print_cli_warning(
                    console,
                    "Simple ëª¨ë“œëŠ” faithfulness/answer_relevancyë¥¼ ê°•ì œí•©ë‹ˆë‹¤.",
                    tips=["ê³ ê¸‰ ë©”íŠ¸ë¦­ êµ¬ì„±ì´ í•„ìš”í•˜ë©´ --mode fullë¡œ ì‹¤í–‰í•˜ì„¸ìš”."],
                )
            metric_list = preset_metrics
        validate_choices(metric_list, available_metrics, console, value_label="metric")

        tracker_override = _option_was_provided(ctx, "tracker") or langfuse
        selected_tracker = tracker
        if preset.default_tracker:
            if tracker_override and tracker != preset.default_tracker:
                print_cli_warning(
                    console,
                    f"Simple ëª¨ë“œëŠ” tracker={preset.default_tracker}ë¡œ ê³ ì •ë©ë‹ˆë‹¤.",
                    tips=["ë‹¤ë¥¸ Trackerë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ --mode fullì„ ì‚¬ìš©í•˜ì„¸ìš”."],
                )
            selected_tracker = preset.default_tracker
        tracker = selected_tracker

        prompt_manifest_value = prompt_manifest
        prompt_files_value = prompt_files
        if not preset.allow_prompt_metadata:
            if prompt_files or _option_was_provided(ctx, "prompt_manifest"):
                print_cli_warning(
                    console,
                    "Simple ëª¨ë“œì—ì„œëŠ” Prompt manifest/diff ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.",
                    tips=["í”„ë¡¬í”„íŠ¸ ì¶”ì ì´ í•„ìš”í•˜ë©´ --mode fullì„ ì‚¬ìš©í•˜ì„¸ìš”."],
                )
            prompt_manifest_value = None
            prompt_files_value = None

        prompt_manifest_path = prompt_manifest_value.expanduser() if prompt_manifest_value else None
        prompt_file_list = [
            Path(item).expanduser() for item in parse_csv_option(prompt_files_value)
        ]
        prompt_metadata_entries: list[dict[str, Any]] = []
        if prompt_file_list:
            prompt_metadata_entries = _collect_prompt_metadata(
                manifest_path=prompt_manifest_path,
                prompt_files=prompt_file_list,
                console=console,
            )
            if prompt_metadata_entries:
                console.print(
                    "[dim]Collected Phoenix prompt metadata for "
                    f"{len(prompt_metadata_entries)} file(s).[/dim]"
                )
                unsynced = [
                    entry for entry in prompt_metadata_entries if entry.get("status") != "synced"
                ]
                if unsynced:
                    print_cli_warning(
                        console,
                        "Prompt íŒŒì¼ì´ manifestì™€ ë‹¤ë¦…ë‹ˆë‹¤.",
                        tips=["`uv run evalvault phoenix prompt-diff`ë¡œ ë³€ê²½ ì‚¬í•­ì„ í™•ì¸í•˜ì„¸ìš”."],
                    )

        if system_prompt and system_prompt_file:
            print_cli_error(
                console,
                "--system-promptì™€ --system-prompt-fileì€ í•¨ê»˜ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                fixes=["ë‘˜ ì¤‘ í•˜ë‚˜ë§Œ ì„¤ì •í•˜ì„¸ìš”."],
            )
            raise typer.Exit(1)

        prompt_inputs: list[PromptInput] = []
        system_prompt_text: str | None = None
        system_prompt_source: str | None = None
        if system_prompt_file:
            try:
                resolved_prompt_file = system_prompt_file.expanduser()
                system_prompt_text = resolved_prompt_file.read_text(encoding="utf-8")
                system_prompt_source = str(resolved_prompt_file)
            except FileNotFoundError:
                print_cli_error(
                    console,
                    "ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    details=str(system_prompt_file),
                )
                raise typer.Exit(1)
        elif system_prompt:
            system_prompt_text = system_prompt
            system_prompt_source = "inline"

        if system_prompt_text:
            prompt_name = system_prompt_name or (
                system_prompt_file.stem if system_prompt_file else "system_prompt"
            )
            prompt_inputs.append(
                PromptInput(
                    content=system_prompt_text,
                    name=prompt_name,
                    kind="system",
                    role="system",
                    source=system_prompt_source,
                )
            )

        ragas_prompt_overrides: dict[str, str] = {}
        ragas_prompt_source: str | None = None
        if ragas_prompts:
            ragas_prompts_path = ragas_prompts.expanduser()
            ragas_prompt_source = str(ragas_prompts_path)
            try:
                ragas_prompt_overrides = load_ragas_prompt_overrides(ragas_prompt_source)
            except PromptOverrideError as exc:
                print_cli_error(
                    console,
                    "Ragas í”„ë¡¬í”„íŠ¸ YAMLì„ íŒŒì‹±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
                    details=str(exc),
                )
                raise typer.Exit(1)
            except FileNotFoundError:
                print_cli_error(
                    console,
                    "Ragas í”„ë¡¬í”„íŠ¸ YAML íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    details=ragas_prompt_source,
                )
                raise typer.Exit(1)

        if ragas_prompt_overrides:
            for metric_name, prompt_text in ragas_prompt_overrides.items():
                if metric_name not in metric_list:
                    print_cli_warning(
                        console,
                        f"Ragas í”„ë¡¬í”„íŠ¸ ì˜¤ë²„ë¼ì´ë“œê°€ ì„ íƒëœ ë©”íŠ¸ë¦­ì— ì—†ìŠµë‹ˆë‹¤: {metric_name}",
                        tips=["--metricsì— í•´ë‹¹ ë©”íŠ¸ë¦­ì„ ì¶”ê°€í•˜ê±°ë‚˜ YAMLì„ ì •ë¦¬í•˜ì„¸ìš”."],
                    )
                prompt_inputs.append(
                    PromptInput(
                        content=prompt_text,
                        name=f"ragas.{metric_name}",
                        kind="ragas",
                        role=metric_name,
                        source=ragas_prompt_source,
                    )
                )
        prompt_bundle = None
        if prompt_inputs and not db_path:
            print_cli_warning(
                console,
                "Prompt snapshotì€ --db ì €ì¥ ì‹œì—ë§Œ DBì— ê¸°ë¡ë©ë‹ˆë‹¤.",
                tips=["--db data/db/evalvault.db ì˜µì…˜ì„ ì¶”ê°€í•˜ì„¸ìš”."],
            )

        if stream_chunk_size <= 0:
            print_cli_error(
                console,
                "--stream-chunk-size ê°’ì€ 1 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.",
                fixes=["ì˜ˆ: --stream-chunk-size 200"],
            )
            raise typer.Exit(1)

        domain_memory_requested = (
            use_domain_memory or memory_domain is not None or memory_augment_context
        )
        if not preset.allow_domain_memory and domain_memory_requested:
            print_cli_warning(
                console,
                "Simple ëª¨ë“œì—ì„œëŠ” Domain Memoryë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                tips=["--mode fullë¡œ ì „í™˜í•´ Domain Memory ë° ì»¨í…ìŠ¤íŠ¸ ì¦ê°•ì„ í™œì„±í™”í•˜ì„¸ìš”."],
            )
            use_domain_memory = False
            memory_domain = None
            memory_augment_context = False
            domain_memory_requested = False

        if stream and domain_memory_requested:
            print_cli_error(
                console,
                "Streaming ëª¨ë“œì—ì„œëŠ” Domain Memory ì˜µì…˜ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                fixes=["ìŠ¤íŠ¸ë¦¬ë°ì„ ë„ê±°ë‚˜ --mode fullì—ì„œ Domain Memoryë¥¼ ë¹„í™œì„±í™”í•˜ì„¸ìš”."],
            )
            raise typer.Exit(1)
        if stream and (phoenix_dataset or phoenix_experiment):
            print_cli_error(
                console,
                "Streaming ëª¨ë“œì—ì„œëŠ” Phoenix Dataset/Experiment ì—…ë¡œë“œê°€ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
                fixes=["ìŠ¤íŠ¸ë¦¬ë° ì—†ì´ ì—…ë¡œë“œí•˜ê±°ë‚˜ Phoenix ì—…ë¡œë“œ ì˜µì…˜ì„ ì œê±°í•˜ì„¸ìš”."],
            )
            raise typer.Exit(1)

        settings = Settings()

        # Apply profile (CLI > .env > default)
        profile_name = profile or settings.evalvault_profile
        if profile_name:
            settings = apply_profile(settings, profile_name)

        # Override model if specified
        if model:
            if _is_oss_open_model(model) and settings.llm_provider != "vllm":
                settings.llm_provider = "ollama"
                settings.ollama_model = model
                console.print(
                    "[dim]OSS model detected. Routing request through Ollama backend.[/dim]"
                )
            elif settings.llm_provider == "ollama":
                settings.ollama_model = model
            elif settings.llm_provider == "vllm":
                settings.vllm_model = model
            else:
                settings.openai_model = model

        if settings.llm_provider == "openai" and not settings.openai_api_key:
            print_cli_error(
                console,
                "OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                fixes=[
                    ".env íŒŒì¼ ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜ì— OPENAI_API_KEY=... ê°’ì„ ì¶”ê°€í•˜ì„¸ìš”.",
                    "--profile dev ê°™ì´ Ollama ê¸°ë°˜ í”„ë¡œí•„ì„ ì‚¬ìš©í•´ ë¡œì»¬ ëª¨ë¸ì„ ì‹¤í–‰í•˜ì„¸ìš”.",
                ],
            )
            raise typer.Exit(1)

        if settings.llm_provider == "ollama":
            display_model = f"ollama/{settings.ollama_model}"
        elif settings.llm_provider == "vllm":
            display_model = f"vllm/{settings.vllm_model}"
        else:
            display_model = settings.openai_model

        console.print("\n[bold]EvalVault[/bold] - RAG Evaluation")
        console.print(f"Dataset: [cyan]{dataset}[/cyan]")
        console.print(f"Metrics: [cyan]{', '.join(metric_list)}[/cyan]")
        console.print(f"Provider: [cyan]{settings.llm_provider}[/cyan]")
        console.print(f"Model: [cyan]{display_model}[/cyan]")
        if profile_name:
            console.print(f"Profile: [cyan]{profile_name}[/cyan]")
        console.print()
        _log_timestamp(console, verbose, f"ì‹¤í–‰ ì‹œì‘ (mode={preset.name})")

        phoenix_trace_metadata: dict[str, Any] = {
            "dataset.path": str(dataset),
            "metrics": metric_list,
            "run_mode": preset.name,
        }
        if threshold_profile:
            phoenix_trace_metadata["threshold.profile"] = str(threshold_profile).strip().lower()

        # Load dataset or configure streaming metadata
        if stream:
            stream_started_at = datetime.now()
            _log_timestamp(
                console,
                verbose,
                f"ìŠ¤íŠ¸ë¦¬ë° í…œí”Œë¦¿ ìƒì„± ì‹œì‘ (chunk_size={stream_chunk_size})",
            )
            ds = _build_streaming_dataset_template(dataset)
            _log_duration(console, verbose, "ìŠ¤íŠ¸ë¦¬ë° í…œí”Œë¦¿ ìƒì„± ì™„ë£Œ", stream_started_at)
            console.print(
                f"[dim]Streaming evaluation enabled (chunk size={stream_chunk_size}).[/dim]"
            )
            phoenix_trace_metadata["dataset.stream"] = True
            phoenix_trace_metadata["dataset.template_version"] = ds.version
        else:
            dataset_load_started_at = datetime.now()
            _log_timestamp(console, verbose, f"ë°ì´í„°ì…‹ ë¡œë”© ì‹œì‘: {dataset}")
            with progress_spinner(console, "ğŸ“‚ ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...") as update_progress:
                try:
                    loader = get_loader(dataset)
                    ds = loader.load(dataset)
                    update_progress(f"âœ… {len(ds)}ê°œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ë¡œë“œ ì™„ë£Œ")
                    _log_duration(console, verbose, "ë°ì´í„°ì…‹ ë¡œë”© ì™„ë£Œ", dataset_load_started_at)
                    phoenix_trace_metadata["dataset.test_cases"] = len(ds)
                    if ds.metadata:
                        for key, value in ds.metadata.items():
                            phoenix_trace_metadata[f"dataset.meta.{key}"] = str(value)
                except Exception as exc:  # pragma: no cover - user feedback path
                    _log_duration(console, verbose, "ë°ì´í„°ì…‹ ë¡œë”© ì‹¤íŒ¨", dataset_load_started_at)
                    print_cli_error(
                        console,
                        "ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
                        details=str(exc),
                        fixes=[
                            "íŒŒì¼ ê²½ë¡œì™€ í™•ì¥ì(csv/json/xlsx)ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
                            "ë°ì´í„°ì…‹ ìŠ¤í‚¤ë§ˆê°€ ë¬¸ì„œì™€ ë™ì¼í•œì§€ ê²€ì¦í•˜ì„¸ìš”.",
                        ],
                    )
                    raise typer.Exit(1) from exc

        if memory_domain:
            ds.metadata["domain"] = memory_domain
            phoenix_trace_metadata["dataset.meta.domain"] = memory_domain

        retriever_instance: RetrieverPort | None = None
        retriever_doc_ids: list[str] | None = None
        if retriever:
            _log_timestamp(console, verbose, f"Retriever ì¤€ë¹„ ì‹œì‘ (mode={retriever})")
            validate_choice(
                retriever,
                ("bm25", "dense", "hybrid", "graphrag"),
                console,
                value_label="retriever",
            )
            if stream:
                print_cli_warning(
                    console,
                    "Streaming ëª¨ë“œì—ì„œëŠ” retriever ì ìš©ì„ ê±´ë„ˆëœë‹ˆë‹¤.",
                    tips=["--streamì„ ë„ê±°ë‚˜ streamingìš© retriever ì§€ì›ì„ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”."],
                )
            elif not retriever_docs:
                print_cli_warning(
                    console,
                    "Retrieverë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ë¬¸ì„œ íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤.",
                    tips=["--retriever-docs <documents.json> ì˜µì…˜ì„ í•¨ê»˜ ì§€ì •í•˜ì„¸ìš”."],
                )
            elif retriever == "graphrag" and not kg:
                print_cli_warning(
                    console,
                    "GraphRAG retrieverë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ KG íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤.",
                    tips=["--kg <knowledge_graph.json> ì˜µì…˜ì„ í•¨ê»˜ ì§€ì •í•˜ì„¸ìš”."],
                )
            else:
                retriever_docs_started_at = datetime.now()
                try:
                    documents, doc_ids = load_retriever_documents(retriever_docs)
                    retriever_doc_ids = doc_ids
                    _log_duration(
                        console,
                        verbose,
                        f"Retriever ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ (count={len(documents)})",
                        retriever_docs_started_at,
                    )
                except Exception as exc:
                    _log_duration(
                        console,
                        verbose,
                        "Retriever ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨",
                        retriever_docs_started_at,
                    )
                    print_cli_error(
                        console,
                        "Retriever ë¬¸ì„œë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
                        details=str(exc),
                        fixes=["JSON/JSONL/TXT í˜•ì‹ì„ í™•ì¸í•˜ì„¸ìš”."],
                    )
                    raise typer.Exit(1) from exc

                retriever_init_started_at = datetime.now()
                try:
                    if retriever == "graphrag":
                        from evalvault.adapters.outbound.kg.graph_rag_retriever import (
                            GraphRAGRetriever,
                        )

                        try:
                            kg_graph = load_knowledge_graph(kg)
                        except Exception as exc:
                            print_cli_error(
                                console,
                                "Knowledge Graph íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
                                details=str(exc),
                                fixes=["KG JSON ìŠ¤í‚¤ë§ˆì™€ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”."],
                            )
                            raise typer.Exit(1) from exc

                        bm25_retriever = None
                        try:
                            from evalvault.adapters.outbound.nlp.korean import KoreanNLPToolkit

                            toolkit = KoreanNLPToolkit()
                            bm25_retriever = toolkit.build_retriever(
                                documents,
                                use_hybrid=False,
                                verbose=verbose,
                            )
                        except Exception as exc:  # pragma: no cover - optional dependency
                            print_cli_warning(
                                console,
                                "GraphRAGìš© BM25 retriever ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
                                tips=[str(exc)],
                            )

                        dense_retriever = None
                        try:
                            dense_retriever = _build_dense_retriever(
                                documents=documents,
                                settings=settings,
                                profile_name=profile_name,
                            )
                        except Exception as exc:  # pragma: no cover - optional dependency
                            print_cli_warning(
                                console,
                                "GraphRAGìš© Dense retriever ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
                                tips=[str(exc)],
                            )

                        kg_doc_ids = {
                            str(entity.source_document_id)
                            for entity in kg_graph.get_all_entities()
                            if entity.source_document_id
                        }
                        if kg_doc_ids and not (kg_doc_ids & set(doc_ids)):
                            preview = ", ".join(sorted(kg_doc_ids)[:3])
                            print_cli_warning(
                                console,
                                "KGì˜ doc_idê°€ ë¬¸ì„œ doc_idì™€ ë§¤ì¹­ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
                                tips=[
                                    "ë¬¸ì„œ íŒŒì¼ì˜ doc_idë¥¼ KG source_document_idì™€ ë™ì¼í•˜ê²Œ ì§€ì •í•˜ì„¸ìš”.",
                                    f"ì˜ˆì‹œ KG doc_id: {preview}",
                                ],
                            )

                        retriever_instance = GraphRAGRetriever(
                            kg_graph,
                            bm25_retriever=bm25_retriever,
                            dense_retriever=dense_retriever,
                            documents=documents,
                            document_ids=doc_ids,
                        )
                    elif retriever == "dense":
                        retriever_instance = _build_dense_retriever(
                            documents=documents,
                            settings=settings,
                            profile_name=profile_name,
                        )
                    else:
                        from evalvault.adapters.outbound.nlp.korean import KoreanNLPToolkit

                        toolkit = KoreanNLPToolkit()
                        retriever_instance = toolkit.build_retriever(
                            documents,
                            use_hybrid=retriever == "hybrid",
                            verbose=verbose,
                        )
                    if retriever_instance:
                        _log_duration(
                            console,
                            verbose,
                            "Retriever ì´ˆê¸°í™” ì™„ë£Œ",
                            retriever_init_started_at,
                        )
                    else:
                        _log_duration(
                            console,
                            verbose,
                            "Retriever ì´ˆê¸°í™” ì‹¤íŒ¨",
                            retriever_init_started_at,
                        )
                except Exception as exc:  # pragma: no cover - dependency/IO issues
                    _log_duration(
                        console,
                        verbose,
                        "Retriever ì´ˆê¸°í™” ì‹¤íŒ¨",
                        retriever_init_started_at,
                    )
                    print_cli_warning(
                        console,
                        "Retriever ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
                        tips=[str(exc)],
                    )
                    retriever_instance = None

                if retriever_instance:
                    phoenix_trace_metadata["retriever.mode"] = retriever
                    phoenix_trace_metadata["retriever.docs"] = str(retriever_docs)
                    if retriever == "graphrag" and kg:
                        phoenix_trace_metadata["retriever.kg"] = str(kg)

        try:
            resolved_thresholds = _resolve_thresholds(
                metric_list,
                ds,
                profile=threshold_profile,
            )
        except ValueError as exc:
            print_cli_error(
                console,
                "Threshold profile ê°’ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.",
                details=str(exc),
                fixes=["--threshold-profile summary|qa ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”."],
            )
            raise typer.Exit(2) from exc

        phoenix_dataset_name = phoenix_dataset
        if phoenix_experiment and not phoenix_dataset_name:
            phoenix_dataset_name = f"{ds.name}:{ds.version}"

        phoenix_dataset_description_value = phoenix_dataset_description
        if phoenix_dataset_name and not phoenix_dataset_description_value:
            desc_source = ds.metadata.get("description") if isinstance(ds.metadata, dict) else None
            phoenix_dataset_description_value = desc_source or f"{ds.name} v{ds.version}"

        phoenix_sync_service: PhoenixSyncService | None = None
        phoenix_dataset_result: dict[str, Any] | None = None
        phoenix_experiment_result: dict[str, Any] | None = None

        if phoenix_dataset_name or phoenix_experiment:
            try:
                phoenix_sync_service = PhoenixSyncService(
                    endpoint=settings.phoenix_endpoint,
                    api_token=getattr(settings, "phoenix_api_token", None),
                )
            except PhoenixSyncError as exc:
                print_cli_warning(
                    console,
                    "Phoenix Sync ì„œë¹„ìŠ¤ë¥¼ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    tips=[str(exc)],
                )
                phoenix_sync_service = None

        effective_tracker = tracker
        if langfuse and tracker == "none" and not preset.default_tracker:
            effective_tracker = "langfuse"
            print_cli_warning(
                console,
                "--langfuse í”Œë˜ê·¸ëŠ” ê³§ ì œê±°ë©ë‹ˆë‹¤.",
                tips=["ëŒ€ì‹  --tracker langfuseë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."],
            )

        config_wants_phoenix = getattr(settings, "phoenix_enabled", False)
        if not isinstance(config_wants_phoenix, bool):
            config_wants_phoenix = False
        should_enable_phoenix = effective_tracker == "phoenix" or config_wants_phoenix
        if should_enable_phoenix:
            ensure_phoenix_instrumentation(settings, console=console, force=True)

        evaluator = RagasEvaluator()
        llm_adapter = get_llm_adapter(settings)

        memory_adapter: SQLiteDomainMemoryAdapter | None = None
        memory_evaluator: MemoryAwareEvaluator | None = None
        memory_domain_name = memory_domain or ds.metadata.get("domain") or "default"
        memory_required = domain_memory_requested
        reliability_snapshot: dict[str, float] | None = None

        if memory_required:
            phoenix_trace_metadata["domain_memory.enabled"] = True
            phoenix_trace_metadata["domain_memory.domain"] = memory_domain_name
            phoenix_trace_metadata["domain_memory.language"] = memory_language
            phoenix_trace_metadata["domain_memory.augment_context"] = memory_augment_context
        else:
            phoenix_trace_metadata["domain_memory.enabled"] = False

        if memory_required:
            memory_started_at = datetime.now()
            _log_timestamp(
                console,
                verbose,
                f"Domain Memory ì´ˆê¸°í™” ì‹œì‘ (domain={memory_domain_name}, lang={memory_language})",
            )
            try:
                memory_adapter = SQLiteDomainMemoryAdapter(memory_db)
                memory_evaluator = MemoryAwareEvaluator(
                    evaluator=evaluator,
                    memory_port=memory_adapter,
                    tracer=PhoenixTracerAdapter(),
                )
                console.print(
                    "[dim]Domain Memory enabled for "
                    f"'{memory_domain_name}' ({memory_language}).[/dim]"
                )
                if memory_adapter:
                    reliability = memory_adapter.get_aggregated_reliability(
                        domain=memory_domain_name,
                        language=memory_language,
                    )
                    reliability_snapshot = reliability
                    if reliability:
                        console.print(
                            "[dim]Reliability snapshot:[/dim] "
                            + ", ".join(f"{k}={v:.2f}" for k, v in reliability.items())
                        )
                        phoenix_trace_metadata["domain_memory.reliability"] = reliability
                _log_duration(console, verbose, "Domain Memory ì´ˆê¸°í™” ì™„ë£Œ", memory_started_at)
            except Exception as exc:  # pragma: no cover - best-effort memory hookup
                _log_duration(console, verbose, "Domain Memory ì´ˆê¸°í™” ì‹¤íŒ¨", memory_started_at)
                print_cli_warning(
                    console,
                    "Domain Memory ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
                    tips=[str(exc)],
                )
                memory_evaluator = None
                memory_adapter = None

        if memory_evaluator and memory_augment_context:
            memory_enrich_started_at = datetime.now()
            _log_timestamp(console, verbose, "Domain Memory ì»¨í…ìŠ¤íŠ¸ ë³´ê°• ì‹œì‘")
            enriched = enrich_dataset_with_memory(
                dataset=ds,
                memory_evaluator=memory_evaluator,
                domain=memory_domain_name,
                language=memory_language,
            )
            enriched_count = enriched or 0
            _log_duration(
                console,
                verbose,
                f"Domain Memory ì»¨í…ìŠ¤íŠ¸ ë³´ê°• ì™„ë£Œ (count={enriched_count})",
                memory_enrich_started_at,
            )
            if enriched:
                console.print(
                    f"[dim]Appended Domain Memory facts to {enriched} test case(s).[/dim]"
                )

        if resolved_thresholds:
            if ds.thresholds and not threshold_profile:
                console.print("[dim]Thresholds from dataset:[/dim]")
                thresholds_to_show = ds.thresholds
            else:
                console.print("[dim]Thresholds in use:[/dim]")
                thresholds_to_show = resolved_thresholds
            for metric, threshold in thresholds_to_show.items():
                console.print(f"  [dim]{metric}: {threshold}[/dim]")
            console.print()

        # Apply preset parallelization settings if not explicitly overridden
        final_parallel = parallel
        final_batch_size = batch_size
        if eval_preset_config:
            if not _option_was_provided(ctx, "parallel"):
                final_parallel = eval_preset_config.parallel
            if not _option_was_provided(ctx, "batch_size"):
                final_batch_size = eval_preset_config.batch_size
            if final_parallel != parallel or final_batch_size != batch_size:
                console.print(
                    f"[dim]Preset parallelization: parallel={final_parallel}, batch_size={final_batch_size}[/dim]"
                )

        if stream:
            status_msg = f"ğŸ“¡ Streaming evaluation (chunk_size={stream_chunk_size})"
        elif final_parallel:
            status_msg = f"âš¡ Parallel evaluation (batch_size={final_batch_size})"
        else:
            status_msg = "ğŸ¤– Evaluation in progress"
        evaluation_started_at = datetime.now()
        if stream:
            eval_mode_label = f"stream(chunk_size={stream_chunk_size})"
            _log_timestamp(
                console,
                verbose,
                f"í‰ê°€ ì‹œì‘ (mode={eval_mode_label}, metrics={', '.join(metric_list)})",
            )
        else:
            eval_mode_label = (
                f"parallel(batch_size={final_batch_size})" if final_parallel else "sequential"
            )
            _log_timestamp(
                console,
                verbose,
                "í‰ê°€ ì‹œì‘ "
                f"(mode={eval_mode_label}, cases={len(ds)}, metrics={', '.join(metric_list)})",
            )
        progress_context = (
            streaming_progress(console, description=status_msg)
            if stream
            else evaluation_progress(console, len(ds), description=status_msg)
        )
        with progress_context as update_progress:
            try:
                if stream:
                    result = asyncio.run(
                        _evaluate_streaming_run(
                            dataset_path=dataset,
                            dataset_template=ds,
                            metrics=metric_list,
                            thresholds=resolved_thresholds,
                            evaluator=evaluator,
                            llm=llm_adapter,
                            chunk_size=stream_chunk_size,
                            parallel=final_parallel,
                            batch_size=final_batch_size,
                            prompt_overrides=ragas_prompt_overrides or None,
                            on_progress=lambda c, t, msg: update_progress(c, t, msg),
                        )
                    )
                elif memory_evaluator and use_domain_memory:
                    update_progress(0, "ğŸ” Domain Memoryì™€ ë³‘ë ¬ë¡œ ì‹¤í–‰ ì¤‘...")
                    result = asyncio.run(
                        memory_evaluator.evaluate_with_memory(
                            dataset=ds,
                            metrics=metric_list,
                            llm=llm_adapter,
                            thresholds=resolved_thresholds,
                            parallel=final_parallel,
                            batch_size=final_batch_size,
                            domain=memory_domain_name,
                            language=memory_language,
                            retriever=retriever_instance,
                            retriever_top_k=retriever_top_k,
                            retriever_doc_ids=retriever_doc_ids,
                            prompt_overrides=ragas_prompt_overrides or None,
                            on_progress=lambda c, _t, msg: update_progress(c, msg),
                        )
                    )
                else:
                    result = asyncio.run(
                        evaluator.evaluate(
                            dataset=ds,
                            metrics=metric_list,
                            llm=llm_adapter,
                            thresholds=resolved_thresholds,
                            parallel=final_parallel,
                            batch_size=final_batch_size,
                            retriever=retriever_instance,
                            retriever_top_k=retriever_top_k,
                            retriever_doc_ids=retriever_doc_ids,
                            prompt_overrides=ragas_prompt_overrides or None,
                            on_progress=lambda c, _t, msg: update_progress(c, msg),
                        )
                    )
                _log_duration(console, verbose, "í‰ê°€ ì™„ë£Œ", evaluation_started_at)
            except Exception as exc:  # pragma: no cover - surfaced to CLI
                _log_duration(console, verbose, "í‰ê°€ ì‹¤íŒ¨", evaluation_started_at)
                print_cli_error(
                    console,
                    "í‰ê°€ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                    details=str(exc),
                    fixes=[
                        "LLM API í‚¤/ì¿¼í„° ìƒíƒœì™€ dataset ìŠ¤í‚¤ë§ˆë¥¼ í™•ì¸í•˜ì„¸ìš”.",
                        "ì¶”ê°€ ë¡œê·¸ëŠ” --verbose ì˜µì…˜ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                    ],
                )
                raise typer.Exit(1) from exc

        phoenix_trace_metadata["dataset.test_cases"] = result.total_test_cases

        result.tracker_metadata.setdefault("run_mode", preset.name)
        if prompt_inputs:
            prompt_bundle = build_prompt_bundle(
                run_id=result.run_id,
                prompt_set_name=prompt_set_name,
                prompt_set_description=prompt_set_description,
                prompt_inputs=prompt_inputs,
                metadata={
                    "run_id": result.run_id,
                    "dataset": result.dataset_name,
                    "model": result.model_name,
                    "metrics": metric_list,
                },
            )
            if prompt_bundle:
                result.tracker_metadata["prompt_set"] = build_prompt_summary(prompt_bundle)

        preprocess_summary = format_dataset_preprocess_summary(
            result.tracker_metadata.get("dataset_preprocess")
        )
        if preprocess_summary:
            console.print(f"[dim]{preprocess_summary}[/dim]")

        retriever_metadata: dict[str, dict[str, Any]] | None = result.retrieval_metadata or None
        if retriever_instance and retriever_metadata:
            console.print(
                f"[dim]Applied {retriever} retriever to "
                f"{len(retriever_metadata)} test case(s).[/dim]"
            )

        _display_results(result, console, verbose)

        if threshold_profile:
            result.tracker_metadata["threshold_profile"] = str(threshold_profile).strip().lower()

        if memory_adapter and memory_required:
            analyzer = MemoryBasedAnalysis(memory_port=memory_adapter)
            insights = analyzer.generate_insights(
                evaluation_run=result,
                domain=memory_domain_name,
                language=memory_language,
            )
            _display_memory_insights(insights, console)

        if phoenix_sync_service:
            phoenix_meta = result.tracker_metadata.setdefault("phoenix", {})
            phoenix_meta.setdefault("schema_version", 2)
            if phoenix_dataset_name:
                try:
                    dataset_info = phoenix_sync_service.upload_dataset(
                        dataset=ds,
                        dataset_name=phoenix_dataset_name,
                        description=phoenix_dataset_description_value,
                    )
                    phoenix_dataset_result = dataset_info.to_dict()
                    phoenix_meta["dataset"] = phoenix_dataset_result
                    phoenix_trace_metadata["phoenix.dataset_id"] = dataset_info.dataset_id
                    phoenix_meta["embedding_export"] = {
                        "dataset_id": dataset_info.dataset_id,
                        "cli": (
                            "uv run evalvault phoenix export-embeddings "
                            f"--dataset {dataset_info.dataset_id}"
                        ),
                        "endpoint": getattr(settings, "phoenix_endpoint", None),
                    }
                    console.print(
                        "[green]Uploaded dataset to Phoenix:[/green] "
                        f"{dataset_info.dataset_name} ({dataset_info.dataset_id})"
                    )
                    console.print(f"[dim]View datasets: {dataset_info.url}[/dim]")
                except PhoenixSyncError as exc:
                    print_cli_warning(
                        console,
                        "Phoenix Dataset ì—…ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
                        tips=[str(exc)],
                    )
            if phoenix_experiment:
                if not phoenix_dataset_result:
                    print_cli_warning(
                        console,
                        "Dataset ì—…ë¡œë“œì— ì‹¤íŒ¨í•´ Phoenix Experiment ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.",
                        tips=["`--phoenix-dataset` ì—…ë¡œë“œê°€ ì„±ê³µí•œ ë’¤ ì‹¤í—˜ì„ ìƒì„±í•˜ì„¸ìš”."],
                    )
                else:
                    experiment_name = (
                        phoenix_experiment or f"{result.model_name}-{result.run_id[:8]}"
                    )
                    experiment_description = (
                        phoenix_experiment_description
                        or f"EvalVault run {result.run_id} ({result.model_name})"
                    )
                    extra_meta = {
                        "domain_memory": {
                            "enabled": memory_required,
                            "domain": memory_domain_name,
                            "language": memory_language,
                        }
                    }
                    experiment_metadata = build_experiment_metadata(
                        run=result,
                        dataset=ds,
                        reliability_snapshot=reliability_snapshot,
                        extra=extra_meta,
                    )
                    try:
                        dataset_info_obj = PhoenixDatasetInfo(
                            dataset_id=phoenix_dataset_result["dataset_id"],
                            dataset_name=phoenix_dataset_result["dataset_name"],
                            dataset_version_id=phoenix_dataset_result["dataset_version_id"],
                            url=phoenix_dataset_result["url"],
                        )
                        exp_info = phoenix_sync_service.create_experiment_record(
                            dataset_info=dataset_info_obj,
                            experiment_name=experiment_name,
                            description=experiment_description,
                            metadata=experiment_metadata,
                        )
                        phoenix_experiment_result = exp_info.to_dict()
                        phoenix_meta["experiment"] = phoenix_experiment_result
                        console.print(
                            "[green]Created Phoenix experiment:[/green] "
                            f"{experiment_name} ({exp_info.experiment_id})"
                        )
                        console.print(f"[dim]View experiment: {exp_info.url}[/dim]")
                    except PhoenixSyncError as exc:
                        print_cli_warning(
                            console,
                            "Phoenix Experiment ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
                            tips=[str(exc)],
                        )

        if prompt_metadata_entries:
            phoenix_meta = result.tracker_metadata.setdefault("phoenix", {})
            phoenix_meta.setdefault("schema_version", 2)
            phoenix_meta["prompts"] = prompt_metadata_entries

        if stage_events or stage_store:
            stage_event_builder = StageEventBuilder()
            stage_event_payload = stage_event_builder.build_for_run(
                result,
                prompt_metadata=prompt_metadata_entries or None,
                retrieval_metadata=retriever_metadata,
            )
            if stage_events:
                stored = _write_stage_events_jsonl(stage_events, stage_event_payload)
                console.print(f"[green]Saved {stored} stage event(s).[/green]")
            if stage_store:
                if db_path:
                    storage = SQLiteStorageAdapter(db_path=db_path)
                    stored = storage.save_stage_events(stage_event_payload)
                    console.print(f"[green]Stored {stored} stage event(s).[/green]")
                else:
                    print_cli_warning(
                        console,
                        "Stage ì´ë²¤íŠ¸ë¥¼ ì €ì¥í•˜ë ¤ë©´ --db ê²½ë¡œê°€ í•„ìš”í•©ë‹ˆë‹¤.",
                        tips=["--db <sqlite_path> ì˜µì…˜ì„ í•¨ê»˜ ì§€ì •í•˜ì„¸ìš”."],
                    )

        if effective_tracker != "none":
            phoenix_opts = None
            if effective_tracker == "phoenix":
                phoenix_opts = {
                    "max_traces": phoenix_max_traces,
                    "metadata": phoenix_trace_metadata or None,
                }
            tracker_started_at = datetime.now()
            _log_timestamp(
                console,
                verbose,
                f"Tracker ë¡œê¹… ì‹œì‘ ({effective_tracker})",
            )
            _log_to_tracker(
                settings,
                result,
                console,
                effective_tracker,
                phoenix_options=phoenix_opts,
                log_phoenix_traces_fn=log_phoenix_traces,
            )
            _log_duration(console, verbose, "Tracker ë¡œê¹… ì™„ë£Œ", tracker_started_at)
        if db_path:
            db_started_at = datetime.now()
            _log_timestamp(console, verbose, f"DB ì €ì¥ ì‹œì‘ ({db_path})")
            _save_to_db(
                db_path,
                result,
                console,
                storage_cls=SQLiteStorageAdapter,
                prompt_bundle=prompt_bundle,
            )
            _log_duration(console, verbose, "DB ì €ì¥ ì™„ë£Œ", db_started_at)
        if output:
            output_started_at = datetime.now()
            _log_timestamp(console, verbose, f"ê²°ê³¼ ì €ì¥ ì‹œì‘ ({output})")
            _save_results(output, result, console)
            _log_duration(console, verbose, "ê²°ê³¼ ì €ì¥ ì™„ë£Œ", output_started_at)

    @app.command(
        name="run-simple",
        help="Shortcut for ì´ˆë³´ììš© ê°„í¸ ëª¨ë“œ. `evalvault run --mode simple`ê³¼ ë™ì¼í•©ë‹ˆë‹¤.",
    )
    def run_simple(  # noqa: PLR0913 - CLI arguments intentionally flat
        dataset: Path = typer.Argument(
            ...,
            help="Path to dataset file (CSV, Excel, or JSON).",
            exists=True,
            readable=True,
        ),
        summary: bool = typer.Option(
            False,
            "--summary",
            help=(
                "Enable summarization evaluation preset "
                "(summary_score, summary_faithfulness, entity_preservation)."
            ),
        ),
        metrics: str = typer.Option(
            "faithfulness,answer_relevancy",
            "--metrics",
            "-m",
            help="Comma-separated list of metrics to evaluate.",
        ),
        threshold_profile: str | None = typer.Option(
            None,
            "--threshold-profile",
            help="Apply a threshold profile (summary/qa) to matching metrics.",
        ),
        profile: str | None = profile_option(
            help_text="Model profile (dev, prod, openai). Overrides .env setting.",
        ),
        model: str | None = typer.Option(
            None,
            "--model",
            help="Model to use for evaluation (overrides profile).",
        ),
        output: Path | None = typer.Option(
            None,
            "--output",
            "-o",
            help="Output file for results (JSON format).",
        ),
        retriever: str | None = typer.Option(
            None,
            "--retriever",
            help="Retriever to fill empty contexts (bm25, dense, hybrid, graphrag).",
        ),
        retriever_docs: Path | None = typer.Option(
            None,
            "--retriever-docs",
            help="Documents file for retriever (.json/.jsonl/.txt).",
        ),
        kg: Path | None = typer.Option(
            None,
            "--kg",
            help="Knowledge graph JSON file for GraphRAG retriever.",
        ),
        retriever_top_k: int = typer.Option(
            5,
            "--retriever-top-k",
            help="Top-K documents to retrieve (default: 5).",
        ),
        stage_events: Path | None = typer.Option(
            None,
            "--stage-events",
            help="Write stage events as JSONL for later ingestion.",
        ),
        stage_store: bool = typer.Option(
            False,
            "--stage-store/--no-stage-store",
            help="Store stage events in the SQLite database (requires --db).",
        ),
        tracker: str = typer.Option(
            "none",
            "--tracker",
            "-t",
            help="Tracker to log results: 'langfuse', 'mlflow', 'phoenix', or 'none'.",
        ),
        langfuse: bool = typer.Option(
            False,
            "--langfuse",
            "-l",
            help="[Deprecated] Use --tracker langfuse instead.",
            hidden=True,
        ),
        phoenix_max_traces: int | None = typer.Option(
            None,
            "--phoenix-max-traces",
            help="Max per-test-case traces to send to Phoenix (default: send all).",
        ),
        phoenix_dataset: str | None = typer.Option(
            None,
            "--phoenix-dataset",
            help="Upload the dataset/test cases to Phoenix under this name.",
        ),
        phoenix_dataset_description: str | None = typer.Option(
            None,
            "--phoenix-dataset-description",
            help="Description stored on the Phoenix dataset (default: dataset metadata).",
        ),
        phoenix_experiment: str | None = typer.Option(
            None,
            "--phoenix-experiment",
            help="Create a Phoenix experiment record for this run (requires dataset upload).",
        ),
        phoenix_experiment_description: str | None = typer.Option(
            None,
            "--phoenix-experiment-description",
            help="Description stored on the Phoenix experiment.",
        ),
        prompt_manifest: Path | None = typer.Option(
            Path("agent/prompts/prompt_manifest.json"),
            "--prompt-manifest",
            help="Path to Phoenix prompt manifest JSON.",
        ),
        prompt_files: str | None = typer.Option(
            None,
            "--prompt-files",
            help="Comma-separated prompt files to capture in Phoenix metadata.",
        ),
        prompt_set_name: str | None = typer.Option(
            None,
            "--prompt-set-name",
            help="Name for the prompt set snapshot stored in the DB.",
        ),
        prompt_set_description: str | None = typer.Option(
            None,
            "--prompt-set-description",
            help="Description for the prompt set snapshot.",
        ),
        system_prompt: str | None = typer.Option(
            None,
            "--system-prompt",
            help="System prompt text for the target LLM (stored for comparison).",
        ),
        system_prompt_file: Path | None = typer.Option(
            None,
            "--system-prompt-file",
            help="Path to a system prompt file to store alongside this run.",
        ),
        system_prompt_name: str | None = typer.Option(
            None,
            "--system-prompt-name",
            help="Optional name for the system prompt snapshot.",
        ),
        ragas_prompts: Path | None = typer.Option(
            None,
            "--ragas-prompts",
            help="YAML file with Ragas metric prompt overrides.",
        ),
        db_path: Path | None = db_option(
            help_text="Path to SQLite database file for storing results.",
        ),
        use_domain_memory: bool = typer.Option(
            False,
            "--use-domain-memory",
            help="Leverage Domain Memory for threshold adjustment and insights.",
        ),
        memory_domain: str | None = typer.Option(
            None,
            "--memory-domain",
            help="Domain name for Domain Memory (defaults to dataset metadata).",
        ),
        memory_language: str = typer.Option(
            "ko",
            "--memory-language",
            help="Language code for Domain Memory lookups (default: ko).",
        ),
        memory_db: Path = memory_db_option(
            help_text="Path to Domain Memory database (default: data/db/evalvault_memory.db).",
        ),
        memory_augment_context: bool = typer.Option(
            False,
            "--augment-context",
            help="Append retrieved factual memories to each test case context.",
        ),
        verbose: bool = typer.Option(
            False,
            "--verbose",
            help="Show detailed output.",
        ),
        parallel: bool = typer.Option(
            False,
            "--parallel",
            help="Enable parallel evaluation for faster processing.",
        ),
        batch_size: int = typer.Option(
            5,
            "--batch-size",
            "-b",
            help="Batch size for parallel evaluation (default: 5).",
        ),
        stream: bool = typer.Option(
            False,
            "--stream",
            help="Enable streaming evaluation for large datasets (process file in chunks).",
        ),
        stream_chunk_size: int = typer.Option(
            200,
            "--stream-chunk-size",
            help="Chunk size when streaming evaluation is enabled (default: 200).",
        ),
    ) -> None:
        """Alias for simple mode presets."""
        try:
            ctx = click.get_current_context()
        except RuntimeError:
            ctx = None
        if ctx:
            ctx.meta["run_mode_alias"] = "run-simple"
        try:
            run(
                dataset=dataset,
                evaluation_preset=None,
                summary=summary,
                metrics=metrics,
                threshold_profile=threshold_profile,
                profile=profile,
                model=model,
                output=output,
                retriever=retriever,
                retriever_docs=retriever_docs,
                kg=kg,
                retriever_top_k=retriever_top_k,
                stage_events=stage_events,
                stage_store=stage_store,
                tracker=tracker,
                langfuse=langfuse,
                phoenix_max_traces=phoenix_max_traces,
                phoenix_dataset=phoenix_dataset,
                phoenix_dataset_description=phoenix_dataset_description,
                phoenix_experiment=phoenix_experiment,
                phoenix_experiment_description=phoenix_experiment_description,
                prompt_manifest=prompt_manifest,
                prompt_files=prompt_files,
                prompt_set_name=prompt_set_name,
                prompt_set_description=prompt_set_description,
                system_prompt=system_prompt,
                system_prompt_file=system_prompt_file,
                system_prompt_name=system_prompt_name,
                ragas_prompts=ragas_prompts,
                db_path=db_path,
                use_domain_memory=use_domain_memory,
                memory_domain=memory_domain,
                memory_language=memory_language,
                memory_db=memory_db,
                memory_augment_context=memory_augment_context,
                verbose=verbose,
                parallel=parallel,
                batch_size=batch_size,
                stream=stream,
                stream_chunk_size=stream_chunk_size,
                mode="simple",
            )
        finally:
            if ctx:
                ctx.meta.pop("run_mode_alias", None)

    @app.command(
        name="run-full",
        help="ì „ë¬¸ê°€ìš© ì „ì²´ ëª¨ë“œë¥¼ ë°”ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤. `evalvault run --mode full` ë³„ì¹­.",
    )
    def run_full(  # noqa: PLR0913 - CLI arguments intentionally flat
        dataset: Path = typer.Argument(
            ...,
            help="Path to dataset file (CSV, Excel, or JSON).",
            exists=True,
            readable=True,
        ),
        summary: bool = typer.Option(
            False,
            "--summary",
            help=(
                "Enable summarization evaluation preset "
                "(summary_score, summary_faithfulness, entity_preservation)."
            ),
        ),
        metrics: str = typer.Option(
            "faithfulness,answer_relevancy",
            "--metrics",
            "-m",
            help="Comma-separated list of metrics to evaluate.",
        ),
        threshold_profile: str | None = typer.Option(
            None,
            "--threshold-profile",
            help="Apply a threshold profile (summary/qa) to matching metrics.",
        ),
        profile: str | None = profile_option(
            help_text="Model profile (dev, prod, openai). Overrides .env setting.",
        ),
        model: str | None = typer.Option(
            None,
            "--model",
            help="Model to use for evaluation (overrides profile).",
        ),
        output: Path | None = typer.Option(
            None,
            "--output",
            "-o",
            help="Output file for results (JSON format).",
        ),
        retriever: str | None = typer.Option(
            None,
            "--retriever",
            help="Retriever to fill empty contexts (bm25, dense, hybrid, graphrag).",
        ),
        retriever_docs: Path | None = typer.Option(
            None,
            "--retriever-docs",
            help="Documents file for retriever (.json/.jsonl/.txt).",
        ),
        kg: Path | None = typer.Option(
            None,
            "--kg",
            help="Knowledge graph JSON file for GraphRAG retriever.",
        ),
        retriever_top_k: int = typer.Option(
            5,
            "--retriever-top-k",
            help="Top-K documents to retrieve (default: 5).",
        ),
        stage_events: Path | None = typer.Option(
            None,
            "--stage-events",
            help="Write stage events as JSONL for later ingestion.",
        ),
        stage_store: bool = typer.Option(
            False,
            "--stage-store/--no-stage-store",
            help="Store stage events in the SQLite database (requires --db).",
        ),
        tracker: str = typer.Option(
            "none",
            "--tracker",
            "-t",
            help="Tracker to log results: 'langfuse', 'mlflow', 'phoenix', or 'none'.",
        ),
        langfuse: bool = typer.Option(
            False,
            "--langfuse",
            "-l",
            help="[Deprecated] Use --tracker langfuse instead.",
            hidden=True,
        ),
        phoenix_max_traces: int | None = typer.Option(
            None,
            "--phoenix-max-traces",
            help="Max per-test-case traces to send to Phoenix (default: send all).",
        ),
        phoenix_dataset: str | None = typer.Option(
            None,
            "--phoenix-dataset",
            help="Upload the dataset/test cases to Phoenix under this name.",
        ),
        phoenix_dataset_description: str | None = typer.Option(
            None,
            "--phoenix-dataset-description",
            help="Description stored on the Phoenix dataset (default: dataset metadata).",
        ),
        phoenix_experiment: str | None = typer.Option(
            None,
            "--phoenix-experiment",
            help="Create a Phoenix experiment record for this run (requires dataset upload).",
        ),
        phoenix_experiment_description: str | None = typer.Option(
            None,
            "--phoenix-experiment-description",
            help="Description stored on the Phoenix experiment.",
        ),
        prompt_manifest: Path | None = typer.Option(
            Path("agent/prompts/prompt_manifest.json"),
            "--prompt-manifest",
            help="Path to Phoenix prompt manifest JSON.",
        ),
        prompt_files: str | None = typer.Option(
            None,
            "--prompt-files",
            help="Comma-separated prompt files to capture in Phoenix metadata.",
        ),
        prompt_set_name: str | None = typer.Option(
            None,
            "--prompt-set-name",
            help="Name for the prompt set snapshot stored in the DB.",
        ),
        prompt_set_description: str | None = typer.Option(
            None,
            "--prompt-set-description",
            help="Description for the prompt set snapshot.",
        ),
        system_prompt: str | None = typer.Option(
            None,
            "--system-prompt",
            help="System prompt text for the target LLM (stored for comparison).",
        ),
        system_prompt_file: Path | None = typer.Option(
            None,
            "--system-prompt-file",
            help="Path to a system prompt file to store alongside this run.",
        ),
        system_prompt_name: str | None = typer.Option(
            None,
            "--system-prompt-name",
            help="Optional name for the system prompt snapshot.",
        ),
        ragas_prompts: Path | None = typer.Option(
            None,
            "--ragas-prompts",
            help="YAML file with Ragas metric prompt overrides.",
        ),
        db_path: Path | None = db_option(
            help_text="Path to SQLite database file for storing results.",
        ),
        use_domain_memory: bool = typer.Option(
            False,
            "--use-domain-memory",
            help="Leverage Domain Memory for threshold adjustment and insights.",
        ),
        memory_domain: str | None = typer.Option(
            None,
            "--memory-domain",
            help="Domain name for Domain Memory (defaults to dataset metadata).",
        ),
        memory_language: str = typer.Option(
            "ko",
            "--memory-language",
            help="Language code for Domain Memory lookups (default: ko).",
        ),
        memory_db: Path = memory_db_option(
            help_text="Path to Domain Memory database (default: data/db/evalvault_memory.db).",
        ),
        memory_augment_context: bool = typer.Option(
            False,
            "--augment-context",
            help="Append retrieved factual memories to each test case context.",
        ),
        verbose: bool = typer.Option(
            False,
            "--verbose",
            help="Show detailed output.",
        ),
        parallel: bool = typer.Option(
            False,
            "--parallel",
            help="Enable parallel evaluation for faster processing.",
        ),
        batch_size: int = typer.Option(
            5,
            "--batch-size",
            "-b",
            help="Batch size for parallel evaluation (default: 5).",
        ),
        stream: bool = typer.Option(
            False,
            "--stream",
            help="Enable streaming evaluation for large datasets (process file in chunks).",
        ),
        stream_chunk_size: int = typer.Option(
            200,
            "--stream-chunk-size",
            help="Chunk size when streaming evaluation is enabled (default: 200).",
        ),
    ) -> None:
        """Alias for full mode presets."""
        try:
            ctx = click.get_current_context()
        except RuntimeError:
            ctx = None
        if ctx:
            ctx.meta["run_mode_alias"] = "run-full"
        try:
            run(
                dataset=dataset,
                evaluation_preset=None,
                summary=summary,
                metrics=metrics,
                threshold_profile=threshold_profile,
                profile=profile,
                model=model,
                output=output,
                retriever=retriever,
                retriever_docs=retriever_docs,
                kg=kg,
                retriever_top_k=retriever_top_k,
                stage_events=stage_events,
                stage_store=stage_store,
                tracker=tracker,
                langfuse=langfuse,
                phoenix_max_traces=phoenix_max_traces,
                phoenix_dataset=phoenix_dataset,
                phoenix_dataset_description=phoenix_dataset_description,
                phoenix_experiment=phoenix_experiment,
                phoenix_experiment_description=phoenix_experiment_description,
                prompt_manifest=prompt_manifest,
                prompt_files=prompt_files,
                prompt_set_name=prompt_set_name,
                prompt_set_description=prompt_set_description,
                system_prompt=system_prompt,
                system_prompt_file=system_prompt_file,
                system_prompt_name=system_prompt_name,
                ragas_prompts=ragas_prompts,
                db_path=db_path,
                use_domain_memory=use_domain_memory,
                memory_domain=memory_domain,
                memory_language=memory_language,
                memory_db=memory_db,
                memory_augment_context=memory_augment_context,
                verbose=verbose,
                parallel=parallel,
                batch_size=batch_size,
                stream=stream,
                stream_chunk_size=stream_chunk_size,
                mode="full",
            )
        finally:
            if ctx:
                ctx.meta.pop("run_mode_alias", None)


__all__ = [
    "register_run_commands",
    "enrich_dataset_with_memory",
    "apply_retriever_to_dataset",
    "load_retriever_documents",
    "log_phoenix_traces",
]
