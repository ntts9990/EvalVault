"""EvalVault Web UI - Streamlit Application."""

from __future__ import annotations

import sys
from pathlib import Path

# Streamlit ì•± ì‹¤í–‰ ì‹œ src ê²½ë¡œ ì¶”ê°€
src_path = Path(__file__).parent.parent.parent.parent.parent
if str(src_path) not in sys.path:
    sys.path.insert(0, str(src_path))


def create_app():
    """Streamlit ì•± ìƒì„± ë° ì„¤ì •."""
    import streamlit as st

    from evalvault.adapters.inbound.web.adapter import create_adapter
    from evalvault.adapters.inbound.web.pages import (
        render_history_page,
        render_reports_page,
    )
    from evalvault.adapters.inbound.web.session import init_session

    # í˜ì´ì§€ ì„¤ì •
    st.set_page_config(
        page_title="EvalVault",
        page_icon="ğŸ“Š",
        layout="wide",
        initial_sidebar_state="expanded",
    )

    # ì„¸ì…˜ ì´ˆê¸°í™”
    session = init_session()

    # ì–´ëŒ‘í„° ì´ˆê¸°í™” (ì„¸ì…˜ì— ìºì‹œ)
    if "adapter" not in st.session_state:
        st.session_state.adapter = create_adapter()

    adapter = st.session_state.adapter

    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.title("ğŸ“Š EvalVault")
        st.caption("RAG Evaluation System")

        st.divider()

        # ë„¤ë¹„ê²Œì´ì…˜
        page = st.radio(
            "Navigation",
            options=["ğŸ  Home", "ğŸ“Š Evaluate", "ğŸ“‹ History", "ğŸ”§ Improve", "ğŸ“„ Reports"],
            label_visibility="collapsed",
        )

        st.divider()

        # ì„¤ì • ì„¹ì…˜
        with st.expander("âš™ï¸ Settings", expanded=False):
            st.caption("Model Configuration")
            model = st.selectbox(
                "Default Model",
                options=[
                    "gpt-5-nano (OpenAI)",
                    "gemma3:1b (Ollama, dev)",
                    "gpt-oss-safeguard:20b (Ollama, prod)",
                ],
                index=0,
            )
            session.selected_model = model

        # ë²„ì „ ì •ë³´
        st.caption("v1.3.0 | Powered by Ragas + Langfuse")

    # ë©”ì¸ ì»¨í…ì¸ 
    if page == "ğŸ  Home":
        render_home_page(adapter, session)
    elif page == "ğŸ“Š Evaluate":
        render_evaluate_page(adapter, session)
    elif page == "ğŸ“‹ History":
        render_history_page(adapter, session)
    elif page == "ğŸ”§ Improve":
        render_improvement_page(adapter, session)
    elif page == "ğŸ“„ Reports":
        render_reports_page(adapter, session)


def render_home_page(adapter, session):
    """í™ˆ í˜ì´ì§€ ë Œë”ë§."""
    import streamlit as st

    from evalvault.adapters.inbound.web.components import (
        DashboardStats,
        MetricSummaryCard,
        RecentRunsList,
        create_pass_rate_chart,
        create_trend_chart,
    )

    st.header("Welcome to EvalVault")
    st.markdown(
        """
        EvalVaultëŠ” RAG (Retrieval-Augmented Generation) ì‹œìŠ¤í…œì„ í‰ê°€í•˜ê³ 
        ë¶„ì„í•˜ê¸° ìœ„í•œ ë„êµ¬ì…ë‹ˆë‹¤.
        """
    )

    # í‰ê°€ ë°ì´í„° ì¡°íšŒ
    runs = adapter.list_runs(limit=20)

    # ëŒ€ì‹œë³´ë“œ í†µê³„ ê³„ì‚°
    stats = DashboardStats.from_runs(runs)

    # í†µê³„ ì¹´ë“œ ì„¹ì…˜
    st.subheader("Overview")
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        card = MetricSummaryCard(
            title="Total Runs",
            value=stats.total_runs,
            format_type="number",
        )
        st.metric(label=card.title, value=card.formatted_value)

    with col2:
        card = MetricSummaryCard(
            title="Test Cases",
            value=stats.total_test_cases,
            format_type="number",
        )
        st.metric(label=card.title, value=card.formatted_value)

    with col3:
        card = MetricSummaryCard(
            title="Avg Pass Rate",
            value=stats.avg_pass_rate,
            format_type="percent",
        )
        st.metric(label=card.title, value=card.formatted_value)

    with col4:
        card = MetricSummaryCard(
            title="Total Cost",
            value=stats.total_cost,
            format_type="currency",
        )
        st.metric(label=card.title, value=card.formatted_value)

    # ì°¨íŠ¸ ì„¹ì…˜
    st.divider()
    chart_col1, chart_col2 = st.columns(2)

    with chart_col1:
        pass_rate_fig = create_pass_rate_chart(runs[:10])
        st.plotly_chart(pass_rate_fig, width="stretch", key="home_pass_rate_chart")

    with chart_col2:
        trend_fig = create_trend_chart(runs)
        st.plotly_chart(trend_fig, width="stretch", key="home_trend_chart")

    # ì§€ì› ë©”íŠ¸ë¦­ ì„¹ì…˜
    st.divider()
    with st.expander("ğŸ“Š ì§€ì› ë©”íŠ¸ë¦­", expanded=False):
        metrics = adapter.get_available_metrics()
        descriptions = adapter.get_metric_descriptions()

        cols = st.columns(3)
        for i, metric in enumerate(metrics):
            with cols[i % 3]:
                st.markdown(
                    f"""
                    <div style="
                        padding: 0.75rem;
                        border-radius: 0.5rem;
                        border: 1px solid #334155;
                        margin-bottom: 0.5rem;
                    ">
                        <strong>{metric}</strong><br>
                        <small style="color: #94A3B8;">{descriptions.get(metric, "")}</small>
                    </div>
                    """,
                    unsafe_allow_html=True,
                )

    # í’ˆì§ˆ ê²Œì´íŠ¸ ë° ê°œì„  ì œì•ˆ ì„¹ì…˜
    st.divider()
    st.subheader("í’ˆì§ˆ í˜„í™© ë° ê°œì„  ì œì•ˆ")

    if runs:
        # ê°€ì¥ ìµœê·¼ ì‹¤í–‰ì˜ í’ˆì§ˆ ê²Œì´íŠ¸ í‘œì‹œ
        latest_run = runs[0]
        try:
            gate_report = adapter.check_quality_gate(latest_run.run_id)

            gate_col1, gate_col2 = st.columns([1, 2])

            with gate_col1:
                # í’ˆì§ˆ ê²Œì´íŠ¸ ìƒíƒœ
                if gate_report.overall_passed:
                    st.success("âœ… í’ˆì§ˆ ê²Œì´íŠ¸ PASS")
                else:
                    st.error("âŒ í’ˆì§ˆ ê²Œì´íŠ¸ FAIL")

                st.caption(f"ìµœê·¼ í‰ê°€: {latest_run.run_id[:12]}...")

            with gate_col2:
                # ë©”íŠ¸ë¦­ë³„ ìƒíƒœ (ì‹¤íŒ¨ ë©”íŠ¸ë¦­ ê°•ì¡°)
                failed_metrics = [r for r in gate_report.results if not r.passed]
                passed_metrics = [r for r in gate_report.results if r.passed]

                if failed_metrics:
                    st.markdown("**ê°œì„  í•„ìš” ë©”íŠ¸ë¦­:**")
                    for result in failed_metrics[:3]:  # ìƒìœ„ 3ê°œë§Œ
                        gap_pct = abs(result.gap) * 100
                        st.markdown(
                            f"- ğŸ”´ **{result.metric}**: {result.score:.2f} / {result.threshold:.2f} "
                            f"(ê°­: -{gap_pct:.1f}%)"
                        )

                if passed_metrics:
                    with st.expander(f"âœ… í†µê³¼ ë©”íŠ¸ë¦­ ({len(passed_metrics)}ê°œ)"):
                        for result in passed_metrics:
                            st.markdown(
                                f"- {result.metric}: {result.score:.2f} / {result.threshold:.2f}"
                            )

            # ë¹ ë¥¸ ê°œì„  ì œì•ˆ ë§í¬
            if failed_metrics:
                st.markdown("---")
                col1, col2 = st.columns([3, 1])
                with col1:
                    st.info(
                        f"ğŸ’¡ {len(failed_metrics)}ê°œ ë©”íŠ¸ë¦­ì´ ì„ê³„ê°’ ë¯¸ë‹¬ì…ë‹ˆë‹¤. "
                        "ê°œì„  ê°€ì´ë“œë¥¼ í™•ì¸í•˜ì„¸ìš”."
                    )
                with col2:
                    if st.button("ğŸ”§ ê°œì„  ê°€ì´ë“œ", key="home_improve_btn"):
                        session.current_run_id = latest_run.run_id

        except Exception as e:
            st.warning(f"í’ˆì§ˆ ê²Œì´íŠ¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
    else:
        st.info("ì•„ì§ í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # ìµœê·¼ í‰ê°€ ëª©ë¡
    st.divider()
    st.subheader("ìµœê·¼ í‰ê°€")

    recent_list = RecentRunsList(runs=runs, max_items=5)

    if not recent_list.is_empty:
        for run in recent_list.displayed_runs:
            col1, col2, col3, col4, col5 = st.columns([3, 1, 1, 1, 1])
            with col1:
                emoji = recent_list.get_pass_rate_emoji(run.pass_rate)
                st.text(f"{emoji} {run.dataset_name}")
                extra_bits: list[str] = []
                if run.phoenix_precision is not None:
                    extra_bits.append(f"P@K {run.phoenix_precision:.2f}")
                if run.phoenix_drift is not None:
                    extra_bits.append(f"Drift {run.phoenix_drift:.2f}")
                if extra_bits:
                    st.caption(" | ".join(extra_bits))
                if run.phoenix_experiment_url:
                    st.caption(f"[Phoenix Experiment]({run.phoenix_experiment_url})")
            with col2:
                st.text(run.model_name)
            with col3:
                pass_rate_pct = run.pass_rate * 100
                if pass_rate_pct >= 70:
                    st.success(f"{pass_rate_pct:.1f}%")
                elif pass_rate_pct >= 50:
                    st.warning(f"{pass_rate_pct:.1f}%")
                else:
                    st.error(f"{pass_rate_pct:.1f}%")
            with col4:
                st.text(run.started_at.strftime("%m/%d"))
            with col5:
                if st.button("ğŸ”§", key=f"improve_{run.run_id}", help="ê°œì„  ê°€ì´ë“œ"):
                    session.current_run_id = run.run_id

        if recent_list.has_more:
            st.caption(f"+{recent_list.remaining_count} more runs...")
    else:
        st.info("ì•„ì§ í‰ê°€ ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤. ì²« í‰ê°€ë¥¼ ì‹¤í–‰í•´ë³´ì„¸ìš”!")


def render_evaluate_page(adapter, session):
    """í‰ê°€ ì‹¤í–‰ í˜ì´ì§€ ë Œë”ë§."""
    import streamlit as st

    from evalvault.adapters.inbound.web.components import (
        FileUploadHandler,
        MetricSelector,
    )

    st.header("ğŸ“Š Evaluate")
    st.markdown("ë°ì´í„°ì…‹ì„ ì—…ë¡œë“œí•˜ê³  RAG í‰ê°€ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.")
    st.markdown(
        """
        <style>
        .mode-pill {
            display: inline-block;
            padding: 0.2rem 0.6rem;
            border-radius: 999px;
            font-size: 0.85rem;
            font-weight: 600;
            color: white;
        }
        .mode-pill.simple { background: #0ea5e9; }
        .mode-pill.full { background: #7c3aed; }
        </style>
        """,
        unsafe_allow_html=True,
    )

    st.subheader("0. ì‹¤í–‰ ëª¨ë“œ ì„ íƒ")
    mode_label = st.radio(
        "ëª¨ë“œ",
        options=["Simple", "Full"],
        horizontal=True,
        index=0 if session.selected_run_mode == "simple" else 1,
        help="Simpleì€ ê¸°ë³¸ ë©”íŠ¸ë¦­/íŠ¸ë˜ì»¤ë¥¼ ê³ ì •í•˜ê³  Fullì€ ëª¨ë“  ê³ ê¸‰ ì˜µì…˜ì„ ë…¸ì¶œí•©ë‹ˆë‹¤.",
    )
    session.selected_run_mode = "simple" if mode_label == "Simple" else "full"
    simple_mode_active = session.selected_run_mode == "simple"
    pill_class = "simple" if simple_mode_active else "full"
    st.markdown(
        f"<span class='mode-pill {pill_class}'>Mode Â· {mode_label}</span>",
        unsafe_allow_html=True,
    )
    if simple_mode_active:
        st.info("ì‹¬í”Œ ëª¨ë“œëŠ” faithfulness/answer_relevancy + Phoenix trackerë¥¼ ê³ ì •í•©ë‹ˆë‹¤.")
    else:
        st.caption(
            "ì „ì²´ ëª¨ë“œ: Domain MemoryÂ·PromptÂ·Phoenix dataset/experiment ì˜µì…˜ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        )

    # ì´ˆê¸°í™”
    upload_handler = FileUploadHandler()
    metric_selector = MetricSelector()

    # íŒŒì¼ ì—…ë¡œë“œ ì„¹ì…˜
    st.subheader("1. ë°ì´í„°ì…‹ ì—…ë¡œë“œ")

    uploaded_file = st.file_uploader(
        "ë°ì´í„°ì…‹ ì—…ë¡œë“œ",
        type=["csv", "json", "xlsx"],
        help="CSV, JSON, ë˜ëŠ” Excel í˜•ì‹ì˜ ë°ì´í„°ì…‹ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.",
    )

    validation_result = None
    if uploaded_file:
        # íŒŒì¼ ê²€ì¦
        content = uploaded_file.read()
        uploaded_file.seek(0)  # ë‹¤ì‹œ ì½ì„ ìˆ˜ ìˆë„ë¡ ë¦¬ì…‹

        validation_result = upload_handler.validate_file(uploaded_file.name, content)

        if validation_result.is_valid:
            st.success(
                f"âœ… {uploaded_file.name} ({validation_result.row_count} rows, "
                f"{validation_result.file_type.upper()})"
            )
            if validation_result.dataset_name:
                st.caption(f"Dataset: {validation_result.dataset_name}")
            # ì„ê³„ê°’ ì •ë³´ í‘œì‹œ
            if validation_result.thresholds:
                threshold_str = ", ".join(
                    f"{k}: {v:.2f}" for k, v in validation_result.thresholds.items()
                )
                st.caption(f"ğŸ“ ì„ê³„ê°’: {threshold_str}")
            else:
                st.caption("ğŸ“ ì„ê³„ê°’: ê¸°ë³¸ê°’ 0.7 ì ìš© (JSONì— thresholds ë¯¸ì§€ì •)")
        else:
            st.error(f"âŒ {validation_result.error_message}")

    # ë©”íŠ¸ë¦­ ì„ íƒ ì„¹ì…˜
    st.divider()
    st.subheader("2. ë©”íŠ¸ë¦­ ì„ íƒ")

    # ì¹´í…Œê³ ë¦¬ë³„ ê·¸ë£¹í™”
    categories = metric_selector.get_metrics_by_category()

    selected_metrics = []
    for category, metrics in categories.items():
        with st.expander(f"ğŸ“ {category.title()}", expanded=category == "generation"):
            cols = st.columns(2)
            for i, metric in enumerate(metrics):
                with cols[i % 2]:
                    icon = metric_selector.get_icon(metric)
                    desc = metric_selector.get_description(metric)
                    default_selected = metric in metric_selector.get_default_metrics()
                    checkbox_disabled = simple_mode_active
                    checked = st.checkbox(
                        f"{icon} {metric}",
                        value=default_selected,
                        help=desc,
                        disabled=checkbox_disabled,
                        key=f"metric_{metric}",
                    )
                    if checked:
                        selected_metrics.append(metric)

    if simple_mode_active:
        selected_metrics = metric_selector.get_default_metrics()

    session.selected_metrics = selected_metrics

    # ì„ íƒëœ ë©”íŠ¸ë¦­ í‘œì‹œ
    if selected_metrics:
        st.caption(f"Selected: {', '.join(selected_metrics)}")

    # ê³ ê¸‰ ì˜µì…˜
    st.divider()
    with st.expander("âš™ï¸ ê³ ê¸‰ ì˜µì…˜"):
        col1, col2, col3 = st.columns(3)
        with col1:
            session.selected_model = st.selectbox(
                "ëª¨ë¸",
                options=[
                    "gpt-5-nano (OpenAI)",
                    "gemma3:1b (Ollama, dev)",
                    "gpt-oss-safeguard:20b (Ollama, prod)",
                ],
                index=0,
            )
        with col2:
            session.langfuse_enabled = st.checkbox("Langfuse íŠ¸ë˜í‚¹", value=False)
        with col3:
            session.parallel_processing = st.checkbox("ë³‘ë ¬ ì²˜ë¦¬", value=True)

        # ì„ê³„ê°’ ì•ˆë‚´ (ë°ì´í„°ì…‹ì—ì„œ ë¡œë“œë¨)
        st.caption(
            "ğŸ’¡ ë©”íŠ¸ë¦­ ì„ê³„ê°’ì€ ë°ì´í„°ì…‹ JSONì˜ `thresholds`ì—ì„œ ë¡œë“œë©ë‹ˆë‹¤. "
            "ë¯¸ì§€ì • ì‹œ ê¸°ë³¸ê°’ 0.7 ì ìš©."
        )

    # ì‹¤í–‰ ë²„íŠ¼
    st.divider()
    can_run = (
        validation_result is not None
        and validation_result.is_valid
        and len(selected_metrics) > 0
        and not session.is_evaluating
    )

    col1, col2 = st.columns([3, 1])
    with col1:
        if st.button("ğŸš€ í‰ê°€ ì‹¤í–‰", type="primary", disabled=not can_run):
            # LLM ì„¤ì • í™•ì¸
            if adapter._llm_adapter is None:
                st.error("LLMì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
            else:
                try:
                    # í‰ê°€ ì‹œì‘ ìƒíƒœ ì„¤ì •
                    session.is_evaluating = True

                    # íŒŒì¼ ë‚´ìš© ì½ê¸°
                    file_content = uploaded_file.getvalue()

                    # st.statusë¥¼ ì‚¬ìš©í•˜ì—¬ ì§„í–‰ ìƒíƒœ í‘œì‹œ
                    with st.status("ğŸ”„ í‰ê°€ ì§„í–‰ ì¤‘...", expanded=True) as status:
                        # Dataset ìƒì„±
                        status.write("ğŸ“‚ ë°ì´í„°ì…‹ íŒŒì‹± ì¤‘...")
                        dataset = adapter.create_dataset_from_upload(
                            uploaded_file.name,
                            file_content,
                        )
                        status.write(f"âœ… ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(dataset.test_cases)}ê°œ ì¼€ì´ìŠ¤")

                        # ë°ì´í„°ì…‹ì—ì„œ Threshold ë¡œë“œ (ë¯¸ì§€ì • ì‹œ ê¸°ë³¸ê°’ 0.7)
                        thresholds = dataset.thresholds or {}
                        if thresholds:
                            status.write(f"ğŸ“ ì„ê³„ê°’ ë¡œë“œ: {thresholds}")
                        else:
                            status.write("ğŸ“ ì„ê³„ê°’: ê¸°ë³¸ê°’ 0.7 ì ìš©")

                        # ë©”íŠ¸ë¦­ ì •ë³´ í‘œì‹œ
                        status.write(f"ğŸ“Š í‰ê°€ ë©”íŠ¸ë¦­: {', '.join(selected_metrics)}")
                        status.write("â³ LLM API í˜¸ì¶œ ì¤‘... (1-2ë¶„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)")

                        # í‰ê°€ ì‹¤í–‰
                        import time

                        start_time = time.time()
                        parallel_mode = session.parallel_processing
                        mode_str = "ë³‘ë ¬" if parallel_mode else "ìˆœì°¨"
                        status.write(f"âš¡ ì‹¤í–‰ ëª¨ë“œ: {mode_str} ì²˜ë¦¬")

                        result = adapter.run_evaluation_with_dataset(
                            dataset=dataset,
                            metrics=selected_metrics,
                            thresholds=thresholds,
                            parallel=parallel_mode,
                            batch_size=5,
                            run_mode=session.selected_run_mode,
                        )
                        elapsed = time.time() - start_time

                        # ì™„ë£Œ ìƒíƒœë¡œ ì—…ë°ì´íŠ¸
                        status.update(label="âœ… í‰ê°€ ì™„ë£Œ!", state="complete", expanded=False)
                        status.write(f"â±ï¸ ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ")

                    # ê²°ê³¼ í‘œì‹œ
                    st.success(f"âœ… í‰ê°€ ì™„ë£Œ! (Run ID: `{result.run_id}`)")

                    # ìš”ì•½ ë©”íŠ¸ë¦­
                    result_cols = st.columns(4)
                    with result_cols[0]:
                        st.metric("í†µê³¼ìœ¨", f"{result.pass_rate:.1%}")
                    with result_cols[1]:
                        st.metric("í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤", result.total_test_cases)
                    with result_cols[2]:
                        passed = result.passed_test_cases
                        st.metric("í†µê³¼", f"{passed}/{result.total_test_cases}")
                    with result_cols[3]:
                        duration = result.duration_seconds or 0
                        st.metric("ì†Œìš” ì‹œê°„", f"{duration:.1f}s")

                    # ë©”íŠ¸ë¦­ë³„ ì ìˆ˜
                    st.subheader("ğŸ“Š ë©”íŠ¸ë¦­ë³„ ê²°ê³¼")
                    metric_results = []
                    for metric in result.metrics_evaluated:
                        score = result.get_avg_score(metric)
                        threshold = thresholds.get(metric, 0.7)
                        passed = score >= threshold if score else False
                        metric_results.append(
                            {
                                "ë©”íŠ¸ë¦­": metric,
                                "ì ìˆ˜": f"{score:.3f}" if score else "N/A",
                                "ì„ê³„ê°’": f"{threshold:.2f}",
                                "ê²°ê³¼": "âœ… Pass" if passed else "âŒ Fail",
                            }
                        )

                    st.dataframe(metric_results, width="stretch")

                    # ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
                    session.current_run_id = result.run_id

                    # History í˜ì´ì§€ ì´ë™ ì•ˆë‚´
                    st.info(
                        f"ğŸ“‹ History í˜ì´ì§€ì—ì„œ ìƒì„¸ ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. "
                        f"(Mode: {session.selected_run_mode.capitalize()})"
                    )

                except Exception as e:
                    st.error(f"âŒ í‰ê°€ ì‹¤íŒ¨: {e}")
                    import traceback

                    st.code(traceback.format_exc())
                finally:
                    session.is_evaluating = False

    with col2:
        if session.is_evaluating:
            st.warning("ì‹¤í–‰ ì¤‘...")

    # ìƒíƒœ ë©”ì‹œì§€
    if not uploaded_file:
        st.info("ğŸ’¡ ë¨¼ì € ë°ì´í„°ì…‹ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")
    elif not validation_result or not validation_result.is_valid:
        st.warning("âš ï¸ ìœ íš¨í•œ ë°ì´í„°ì…‹ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")
    elif not selected_metrics:
        st.warning("âš ï¸ ìµœì†Œ í•˜ë‚˜ì˜ ë©”íŠ¸ë¦­ì„ ì„ íƒí•˜ì„¸ìš”.")
    elif adapter._llm_adapter is None:
        st.warning("âš ï¸ LLMì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .envì— OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")


def render_improvement_page(adapter, session):
    """ê°œì„  ê°€ì´ë“œ í˜ì´ì§€ ë Œë”ë§."""
    import streamlit as st

    from evalvault.adapters.inbound.web.components import RunSelector, render_model_selector

    # LLM ë¶„ì„ ë‹¤ì´ì–¼ë¡œê·¸
    @st.dialog("ğŸ”§ LLM ê°œì„  ê°€ì´ë“œ ìƒì„±", width="large")
    def llm_improvement_dialog(run_id: str, run_name: str):
        """LLM ê°œì„  ê°€ì´ë“œ ìƒì„± ëª¨ë‹¬."""
        st.markdown(f"**ëŒ€ìƒ í‰ê°€:** {run_name}")
        st.divider()

        # ëª¨ë¸ ì„ íƒ
        st.subheader("ë¶„ì„ ëª¨ë¸ ì„ íƒ")
        selected_model = render_model_selector(
            st,
            key="dialog_improve_model",
            label="LLM ëª¨ë¸",
            help_text="ê°œì„  ê°€ì´ë“œ ìƒì„±ì— ì‚¬ìš©í•  LLM ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”.",
        )

        col1, col2 = st.columns([3, 1])
        with col1:
            generate_clicked = st.button(
                "ğŸš€ ë¶„ì„ ì‹œì‘",
                type="primary",
                use_container_width=True,
            )
        with col2:
            if st.button("ì·¨ì†Œ", use_container_width=True):
                st.rerun()

        if generate_clicked:
            model_id = selected_model.id if selected_model else None
            model_name = selected_model.display_name if selected_model else "ê¸°ë³¸ ëª¨ë¸"

            with st.status("ğŸ”§ LLM ê°œì„  ê°€ì´ë“œ ìƒì„± ì¤‘...", expanded=True) as status:
                try:
                    status.write(f"ğŸ“Š ëª¨ë¸: **{model_name}**")
                    status.write("ğŸ§  LLM ë¶„ì„ ì‹œì‘...")
                    status.write("â³ ì•½ 1-2ë¶„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤...")

                    # LLM ê°œì„  ê°€ì´ë“œ ìƒì„±
                    report = adapter.get_improvement_guide(
                        run_id,
                        include_llm=True,
                        model_id=model_id,
                    )

                    status.update(
                        label="âœ… LLM ê°œì„  ê°€ì´ë“œ ìƒì„± ì™„ë£Œ!",
                        state="complete",
                        expanded=False,
                    )

                    # ì„¸ì…˜ì— ì €ì¥
                    session.improvement_report = report
                    st.session_state.last_improve_options = {
                        "run_id": run_id,
                        "include_llm": True,
                        "model_id": model_id,
                    }

                    st.success("âœ… ê°œì„  ê°€ì´ë“œ ìƒì„± ì™„ë£Œ!")
                    st.info("ë‹¤ì´ì–¼ë¡œê·¸ë¥¼ ë‹«ìœ¼ë©´ ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

                    if st.button("ğŸ“„ ê²°ê³¼ í™•ì¸", type="primary", use_container_width=True):
                        st.rerun()

                except Exception as e:  # noqa: BLE001
                    status.update(label="âŒ ìƒì„± ì‹¤íŒ¨", state="error")
                    st.error(f"âŒ ê°œì„  ê°€ì´ë“œ ìƒì„± ì‹¤íŒ¨: {e}")
                    import traceback

                    with st.expander("ì˜¤ë¥˜ ìƒì„¸"):
                        st.code(traceback.format_exc())

    st.header("ğŸ”§ ê°œì„  ê°€ì´ë“œ")
    st.markdown("í‰ê°€ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ RAG ì‹œìŠ¤í…œ ê°œì„  ë°©ì•ˆì„ ì œì•ˆí•©ë‹ˆë‹¤.")

    # í‰ê°€ ê²°ê³¼ ì¡°íšŒ
    runs = adapter.list_runs(limit=50)

    if not runs:
        st.info("ë¶„ì„í•  í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € í‰ê°€ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return

    # ì‹¤í–‰ ì„ íƒ ì„¹ì…˜
    st.subheader("1. í‰ê°€ ì„ íƒ")
    selector = RunSelector(runs=runs)
    options = selector.get_options()

    selected_option = st.selectbox(
        "ë¶„ì„í•  í‰ê°€ ì‹¤í–‰ ì„ íƒ",
        options=options,
        format_func=lambda x: x,
        help="ê°œì„  ê°€ì´ë“œë¥¼ ìƒì„±í•  í‰ê°€ ì‹¤í–‰ì„ ì„ íƒí•˜ì„¸ìš”.",
    )

    # ì„ íƒëœ ì‹¤í–‰ ID ì¶”ì¶œ
    selected_run_id = selected_option.split(" | ")[0] if selected_option else None
    selected_run = selector.get_by_id(selected_run_id) if selected_run_id else None

    if not selected_run:
        return

    # ì„ íƒëœ í‰ê°€ ì •ë³´ ë° í’ˆì§ˆ ê²Œì´íŠ¸
    st.divider()
    st.subheader("2. í’ˆì§ˆ í˜„í™©")

    # í’ˆì§ˆ ê²Œì´íŠ¸ ì²´í¬
    try:
        gate_report = adapter.check_quality_gate(selected_run_id)

        # ì „ì²´ ìƒíƒœ í‘œì‹œ
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ì „ì²´ í†µê³¼ìœ¨", f"{selected_run.pass_rate:.1%}")
        with col2:
            if gate_report.overall_passed:
                st.success("âœ… í’ˆì§ˆ ê²Œì´íŠ¸ PASS")
            else:
                st.error("âŒ í’ˆì§ˆ ê²Œì´íŠ¸ FAIL")
        with col3:
            st.metric("í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤", selected_run.total_test_cases)

        # ë©”íŠ¸ë¦­ë³„ í˜„í™©
        st.markdown("**ë©”íŠ¸ë¦­ë³„ í˜„í™©**")
        for result in gate_report.results:
            col1, col2, col3, col4 = st.columns([3, 1, 1, 1])
            with col1:
                # í”„ë¡œê·¸ë ˆìŠ¤ ë°”
                st.progress(result.score, text=result.metric)
            with col2:
                st.text(f"{result.score:.2f}")
            with col3:
                st.text(f"/ {result.threshold:.2f}")
            with col4:
                if result.passed:
                    st.success("âœ…")
                else:
                    st.error("âŒ")

    except Exception as e:
        st.warning(f"í’ˆì§ˆ ê²Œì´íŠ¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")

    # ê°œì„  ê°€ì´ë“œ ìƒì„± ì˜µì…˜
    st.divider()
    st.subheader("3. ê°œì„  ê°€ì´ë“œ ìƒì„±")

    analysis_type = st.radio(
        "ë¶„ì„ ìœ í˜•",
        options=["llm_analysis", "basic"],
        format_func=lambda x: {
            "llm_analysis": "ğŸ¤– LLM ë¶„ì„ (ê¶Œì¥) - AI ê¸°ë°˜ ì‹¬ì¸µ ë¶„ì„",
            "basic": "ğŸ“ ê¸°ë³¸ ë¶„ì„ - ê·œì¹™ ê¸°ë°˜ íŒ¨í„´ ë¶„ì„",
        }.get(x, x),
        horizontal=False,
        help="LLM ë¶„ì„ì€ ë” ìƒì„¸í•œ ê°œì„  ì œì•ˆì„ ì œê³µí•©ë‹ˆë‹¤.",
    )

    # LLM ë¶„ì„ ì„ íƒ ì‹œ
    if analysis_type == "llm_analysis":
        st.info(
            "ğŸ’¡ **LLM ë¶„ì„**ì€ AIë¥¼ í™œìš©í•˜ì—¬ ì‹¤íŒ¨ ì›ì¸ì„ ì‹¬ì¸µ ë¶„ì„í•˜ê³  "
            "êµ¬ì²´ì ì¸ ê°œì„  ë°©ì•ˆì„ ì œì•ˆí•©ë‹ˆë‹¤. (LLM API í˜¸ì¶œë¡œ ì¸í•´ 1-2ë¶„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)"
        )

        if adapter._llm_adapter is None:
            st.error("LLMì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        elif selected_run is None:
            st.warning("ë¨¼ì € í‰ê°€ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
        else:
            if st.button(
                "ğŸ¤– LLM ë¶„ì„ ì‹œì‘",
                type="primary",
                help="í´ë¦­í•˜ë©´ ëª¨ë‹¬ ì°½ì—ì„œ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.",
            ):
                llm_improvement_dialog(
                    selected_run.run_id,
                    f"{selected_run.dataset_name} ({selected_run.run_id[:8]}...)",
                )

    # ê¸°ë³¸ ë¶„ì„ ì„ íƒ ì‹œ
    else:
        if st.button("ğŸ” ê¸°ë³¸ ë¶„ì„ ì‹œì‘", type="primary"):
            with st.spinner("ê°œì„  ê°€ì´ë“œ ìƒì„± ì¤‘..."):
                try:
                    report = adapter.get_improvement_guide(
                        selected_run_id,
                        include_llm=False,
                        model_id=None,
                    )

                    # ì„¸ì…˜ì— ì €ì¥
                    session.improvement_report = report
                    st.session_state.last_improve_options = {
                        "run_id": selected_run_id,
                        "include_llm": False,
                        "model_id": None,
                    }
                    st.success("âœ… ê¸°ë³¸ ë¶„ì„ ì™„ë£Œ!")

                except Exception as e:
                    st.error(f"ê°œì„  ê°€ì´ë“œ ìƒì„± ì‹¤íŒ¨: {e}")
                    return

    # ì˜µì…˜ ë³€ê²½ ì‹œ ì´ì „ ê²°ê³¼ ë¬´íš¨í™”
    if "last_improve_options" not in st.session_state:
        st.session_state.last_improve_options = {
            "run_id": None,
            "include_llm": False,
            "model_id": None,
        }

    options_changed = st.session_state.last_improve_options["run_id"] != selected_run_id

    if options_changed and hasattr(session, "improvement_report") and session.improvement_report:
        # í‰ê°€ê°€ ë³€ê²½ë˜ë©´ ì´ì „ ê²°ê³¼ ë¬´íš¨í™”
        session.improvement_report = None

    # ê°œì„  ê°€ì´ë“œ í‘œì‹œ
    if hasattr(session, "improvement_report") and session.improvement_report:
        report = session.improvement_report

        st.divider()
        st.subheader("4. ê°œì„  ê°€ì´ë“œ")

        # ìš”ì•½
        st.markdown(
            f"""
        **ë¶„ì„ ìš”ì•½**
        - ë¶„ì„ ëŒ€ìƒ: {report.run_id}
        - í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {report.total_test_cases}ê°œ
        - ì‹¤íŒ¨ ì¼€ì´ìŠ¤: {report.failed_test_cases}ê°œ
        - í†µê³¼ìœ¨: {report.pass_rate:.1%}
        """
        )

        # ê°€ì´ë“œ ëª©ë¡
        if report.guides:
            for i, guide in enumerate(report.guides, 1):
                priority_colors = {
                    "P0_CRITICAL": "ğŸ”´",
                    "P1_HIGH": "ğŸŸ ",
                    "P2_MEDIUM": "ğŸŸ¡",
                    "P3_LOW": "ğŸŸ¢",
                }
                priority_icon = priority_colors.get(guide.priority.name, "âšª")

                with st.expander(
                    f"{priority_icon} {i}. {guide.component.value.title()} ê°œì„  "
                    f"(ì˜ˆìƒ +{guide.total_expected_improvement:.0%})",
                    expanded=i == 1,
                ):
                    # ëŒ€ìƒ ë©”íŠ¸ë¦­
                    st.markdown(f"**ëŒ€ìƒ ë©”íŠ¸ë¦­**: {', '.join(guide.target_metrics)}")

                    # ì¦ê±° ë°ì´í„°
                    if guide.evidence:
                        st.markdown("**ì¦ê±° ë°ì´í„°**")
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("ì‹¤íŒ¨ ì¼€ì´ìŠ¤", guide.evidence.total_failures)
                        with col2:
                            if guide.evidence.avg_score_failures:
                                st.metric(
                                    "ì‹¤íŒ¨ í‰ê·  ì ìˆ˜",
                                    f"{guide.evidence.avg_score_failures:.2f}",
                                )
                        with col3:
                            if guide.evidence.avg_score_passes:
                                st.metric(
                                    "í†µê³¼ í‰ê·  ì ìˆ˜",
                                    f"{guide.evidence.avg_score_passes:.2f}",
                                )

                    # ê°œì„  ì•¡ì…˜
                    st.markdown("**ê¶Œì¥ ì•¡ì…˜**")
                    for j, action in enumerate(guide.actions, 1):
                        effort_icons = {"low": "ğŸŸ¢", "medium": "ğŸŸ¡", "high": "ğŸ”´"}
                        effort_icon = effort_icons.get(action.effort.value, "âšª")

                        st.markdown(
                            f"{j}. **{action.title}** {effort_icon} "
                            f"(ì˜ˆìƒ +{action.expected_improvement:.0%})"
                        )
                        st.caption(action.description)

                        if action.implementation_hint:
                            st.code(action.implementation_hint, language="python")

                    # ê²€ì¦ ë°©ë²•
                    if guide.verification_command:
                        st.markdown("**ê²€ì¦ ë°©ë²•**")
                        st.code(guide.verification_command, language="bash")
        else:
            st.info("íƒì§€ëœ ê°œì„  íŒ¨í„´ì´ ì—†ìŠµë‹ˆë‹¤. í˜„ì¬ ì‹œìŠ¤í…œì´ ì–‘í˜¸í•œ ìƒíƒœì…ë‹ˆë‹¤.")

        # ë§ˆí¬ë‹¤ìš´ ë‹¤ìš´ë¡œë“œ
        st.divider()
        if hasattr(report, "to_markdown"):
            st.download_button(
                "ğŸ“¥ ë§ˆí¬ë‹¤ìš´ ë‹¤ìš´ë¡œë“œ",
                data=report.to_markdown(),
                file_name=f"improvement_guide_{report.run_id}.md",
                mime="text/markdown",
            )


def main():
    """Streamlit ì•± ì§„ì…ì ."""
    create_app()


if __name__ == "__main__":
    main()
