"""EvalVault Web UI - Streamlit Application."""

from __future__ import annotations

import sys
from pathlib import Path

# Streamlit ì•± ì‹¤í–‰ ì‹œ src ê²½ë¡œ ì¶”ê°€
src_path = Path(__file__).parent.parent.parent.parent.parent
if str(src_path) not in sys.path:
    sys.path.insert(0, str(src_path))


def create_app():
    """Streamlit ì•± ìƒì„± ë° ì„¤ì •."""
    import streamlit as st

    from evalvault.adapters.inbound.web.adapter import create_adapter
    from evalvault.adapters.inbound.web.session import init_session

    # í˜ì´ì§€ ì„¤ì •
    st.set_page_config(
        page_title="EvalVault",
        page_icon="ğŸ“Š",
        layout="wide",
        initial_sidebar_state="expanded",
    )

    # ì„¸ì…˜ ì´ˆê¸°í™”
    session = init_session()

    # ì–´ëŒ‘í„° ì´ˆê¸°í™” (ì„¸ì…˜ì— ìºì‹œ)
    if "adapter" not in st.session_state:
        st.session_state.adapter = create_adapter()

    adapter = st.session_state.adapter

    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.title("ğŸ“Š EvalVault")
        st.caption("RAG Evaluation System")

        st.divider()

        # ë„¤ë¹„ê²Œì´ì…˜
        page = st.radio(
            "Navigation",
            options=["ğŸ  Home", "ğŸ“Š Evaluate", "ğŸ“‹ History", "ğŸ”§ Improve", "ğŸ“„ Reports"],
            label_visibility="collapsed",
        )

        st.divider()

        # ì„¤ì • ì„¹ì…˜
        with st.expander("âš™ï¸ Settings", expanded=False):
            st.caption("Model Configuration")
            model = st.selectbox(
                "Default Model",
                options=["gpt-5-nano", "gpt-4", "gpt-4o", "claude-3-5-sonnet"],
                index=0,
            )
            session.selected_model = model

        # ë²„ì „ ì •ë³´
        st.caption("v1.3.0 | Powered by Ragas + Langfuse")

    # ë©”ì¸ ì»¨í…ì¸ 
    if page == "ğŸ  Home":
        render_home_page(adapter, session)
    elif page == "ğŸ“Š Evaluate":
        render_evaluate_page(adapter, session)
    elif page == "ğŸ“‹ History":
        render_history_page(adapter, session)
    elif page == "ğŸ”§ Improve":
        render_improvement_page(adapter, session)
    elif page == "ğŸ“„ Reports":
        render_reports_page(adapter, session)


def render_home_page(adapter, session):
    """í™ˆ í˜ì´ì§€ ë Œë”ë§."""
    import streamlit as st

    from evalvault.adapters.inbound.web.components import (
        DashboardStats,
        MetricSummaryCard,
        RecentRunsList,
        create_pass_rate_chart,
        create_trend_chart,
    )

    st.header("Welcome to EvalVault")
    st.markdown(
        """
        EvalVaultëŠ” RAG (Retrieval-Augmented Generation) ì‹œìŠ¤í…œì„ í‰ê°€í•˜ê³ 
        ë¶„ì„í•˜ê¸° ìœ„í•œ ë„êµ¬ì…ë‹ˆë‹¤.
        """
    )

    # í‰ê°€ ë°ì´í„° ì¡°íšŒ
    runs = adapter.list_runs(limit=20)

    # ëŒ€ì‹œë³´ë“œ í†µê³„ ê³„ì‚°
    stats = DashboardStats.from_runs(runs)

    # í†µê³„ ì¹´ë“œ ì„¹ì…˜
    st.subheader("Overview")
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        card = MetricSummaryCard(
            title="Total Runs",
            value=stats.total_runs,
            format_type="number",
        )
        st.metric(label=card.title, value=card.formatted_value)

    with col2:
        card = MetricSummaryCard(
            title="Test Cases",
            value=stats.total_test_cases,
            format_type="number",
        )
        st.metric(label=card.title, value=card.formatted_value)

    with col3:
        card = MetricSummaryCard(
            title="Avg Pass Rate",
            value=stats.avg_pass_rate,
            format_type="percent",
        )
        st.metric(label=card.title, value=card.formatted_value)

    with col4:
        card = MetricSummaryCard(
            title="Total Cost",
            value=stats.total_cost,
            format_type="currency",
        )
        st.metric(label=card.title, value=card.formatted_value)

    # ì°¨íŠ¸ ì„¹ì…˜
    st.divider()
    chart_col1, chart_col2 = st.columns(2)

    with chart_col1:
        pass_rate_fig = create_pass_rate_chart(runs[:10])
        st.plotly_chart(pass_rate_fig, use_container_width=True, key="home_pass_rate_chart")

    with chart_col2:
        trend_fig = create_trend_chart(runs)
        st.plotly_chart(trend_fig, use_container_width=True, key="home_trend_chart")

    # ì§€ì› ë©”íŠ¸ë¦­ ì„¹ì…˜
    st.divider()
    with st.expander("ğŸ“Š ì§€ì› ë©”íŠ¸ë¦­", expanded=False):
        metrics = adapter.get_available_metrics()
        descriptions = adapter.get_metric_descriptions()

        cols = st.columns(3)
        for i, metric in enumerate(metrics):
            with cols[i % 3]:
                st.markdown(
                    f"""
                    <div style="
                        padding: 0.75rem;
                        border-radius: 0.5rem;
                        border: 1px solid #334155;
                        margin-bottom: 0.5rem;
                    ">
                        <strong>{metric}</strong><br>
                        <small style="color: #94A3B8;">{descriptions.get(metric, "")}</small>
                    </div>
                    """,
                    unsafe_allow_html=True,
                )

    # í’ˆì§ˆ ê²Œì´íŠ¸ ë° ê°œì„  ì œì•ˆ ì„¹ì…˜
    st.divider()
    st.subheader("í’ˆì§ˆ í˜„í™© ë° ê°œì„  ì œì•ˆ")

    if runs:
        # ê°€ì¥ ìµœê·¼ ì‹¤í–‰ì˜ í’ˆì§ˆ ê²Œì´íŠ¸ í‘œì‹œ
        latest_run = runs[0]
        try:
            gate_report = adapter.check_quality_gate(latest_run.run_id)

            gate_col1, gate_col2 = st.columns([1, 2])

            with gate_col1:
                # í’ˆì§ˆ ê²Œì´íŠ¸ ìƒíƒœ
                if gate_report.overall_passed:
                    st.success("âœ… í’ˆì§ˆ ê²Œì´íŠ¸ PASS")
                else:
                    st.error("âŒ í’ˆì§ˆ ê²Œì´íŠ¸ FAIL")

                st.caption(f"ìµœê·¼ í‰ê°€: {latest_run.run_id[:12]}...")

            with gate_col2:
                # ë©”íŠ¸ë¦­ë³„ ìƒíƒœ (ì‹¤íŒ¨ ë©”íŠ¸ë¦­ ê°•ì¡°)
                failed_metrics = [r for r in gate_report.results if not r.passed]
                passed_metrics = [r for r in gate_report.results if r.passed]

                if failed_metrics:
                    st.markdown("**ê°œì„  í•„ìš” ë©”íŠ¸ë¦­:**")
                    for result in failed_metrics[:3]:  # ìƒìœ„ 3ê°œë§Œ
                        gap_pct = abs(result.gap) * 100
                        st.markdown(
                            f"- ğŸ”´ **{result.metric}**: {result.score:.2f} / {result.threshold:.2f} "
                            f"(ê°­: -{gap_pct:.1f}%)"
                        )

                if passed_metrics:
                    with st.expander(f"âœ… í†µê³¼ ë©”íŠ¸ë¦­ ({len(passed_metrics)}ê°œ)"):
                        for result in passed_metrics:
                            st.markdown(
                                f"- {result.metric}: {result.score:.2f} / {result.threshold:.2f}"
                            )

            # ë¹ ë¥¸ ê°œì„  ì œì•ˆ ë§í¬
            if failed_metrics:
                st.markdown("---")
                col1, col2 = st.columns([3, 1])
                with col1:
                    st.info(
                        f"ğŸ’¡ {len(failed_metrics)}ê°œ ë©”íŠ¸ë¦­ì´ ì„ê³„ê°’ ë¯¸ë‹¬ì…ë‹ˆë‹¤. "
                        "ê°œì„  ê°€ì´ë“œë¥¼ í™•ì¸í•˜ì„¸ìš”."
                    )
                with col2:
                    if st.button("ğŸ”§ ê°œì„  ê°€ì´ë“œ", key="home_improve_btn"):
                        session.current_run_id = latest_run.run_id

        except Exception as e:
            st.warning(f"í’ˆì§ˆ ê²Œì´íŠ¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
    else:
        st.info("ì•„ì§ í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # ìµœê·¼ í‰ê°€ ëª©ë¡
    st.divider()
    st.subheader("ìµœê·¼ í‰ê°€")

    recent_list = RecentRunsList(runs=runs, max_items=5)

    if not recent_list.is_empty:
        for run in recent_list.displayed_runs:
            col1, col2, col3, col4, col5 = st.columns([3, 1, 1, 1, 1])
            with col1:
                emoji = recent_list.get_pass_rate_emoji(run.pass_rate)
                st.text(f"{emoji} {run.dataset_name}")
            with col2:
                st.text(run.model_name)
            with col3:
                pass_rate_pct = run.pass_rate * 100
                if pass_rate_pct >= 70:
                    st.success(f"{pass_rate_pct:.1f}%")
                elif pass_rate_pct >= 50:
                    st.warning(f"{pass_rate_pct:.1f}%")
                else:
                    st.error(f"{pass_rate_pct:.1f}%")
            with col4:
                st.text(run.started_at.strftime("%m/%d"))
            with col5:
                if st.button("ğŸ”§", key=f"improve_{run.run_id}", help="ê°œì„  ê°€ì´ë“œ"):
                    session.current_run_id = run.run_id

        if recent_list.has_more:
            st.caption(f"+{recent_list.remaining_count} more runs...")
    else:
        st.info("ì•„ì§ í‰ê°€ ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤. ì²« í‰ê°€ë¥¼ ì‹¤í–‰í•´ë³´ì„¸ìš”!")


def render_evaluate_page(adapter, session):
    """í‰ê°€ ì‹¤í–‰ í˜ì´ì§€ ë Œë”ë§."""
    import streamlit as st

    from evalvault.adapters.inbound.web.components import (
        FileUploadHandler,
        MetricSelector,
    )

    st.header("ğŸ“Š Evaluate")
    st.markdown("ë°ì´í„°ì…‹ì„ ì—…ë¡œë“œí•˜ê³  RAG í‰ê°€ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.")

    # ì´ˆê¸°í™”
    upload_handler = FileUploadHandler()
    metric_selector = MetricSelector()

    # íŒŒì¼ ì—…ë¡œë“œ ì„¹ì…˜
    st.subheader("1. ë°ì´í„°ì…‹ ì—…ë¡œë“œ")

    uploaded_file = st.file_uploader(
        "ë°ì´í„°ì…‹ ì—…ë¡œë“œ",
        type=["csv", "json", "xlsx"],
        help="CSV, JSON, ë˜ëŠ” Excel í˜•ì‹ì˜ ë°ì´í„°ì…‹ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.",
    )

    validation_result = None
    if uploaded_file:
        # íŒŒì¼ ê²€ì¦
        content = uploaded_file.read()
        uploaded_file.seek(0)  # ë‹¤ì‹œ ì½ì„ ìˆ˜ ìˆë„ë¡ ë¦¬ì…‹

        validation_result = upload_handler.validate_file(uploaded_file.name, content)

        if validation_result.is_valid:
            st.success(
                f"âœ… {uploaded_file.name} ({validation_result.row_count} rows, "
                f"{validation_result.file_type.upper()})"
            )
            if validation_result.dataset_name:
                st.caption(f"Dataset: {validation_result.dataset_name}")
        else:
            st.error(f"âŒ {validation_result.error_message}")

    # ë©”íŠ¸ë¦­ ì„ íƒ ì„¹ì…˜
    st.divider()
    st.subheader("2. ë©”íŠ¸ë¦­ ì„ íƒ")

    # ì¹´í…Œê³ ë¦¬ë³„ ê·¸ë£¹í™”
    categories = metric_selector.get_metrics_by_category()

    selected_metrics = []
    for category, metrics in categories.items():
        with st.expander(f"ğŸ“ {category.title()}", expanded=category == "generation"):
            cols = st.columns(2)
            for i, metric in enumerate(metrics):
                with cols[i % 2]:
                    icon = metric_selector.get_icon(metric)
                    desc = metric_selector.get_description(metric)
                    if st.checkbox(
                        f"{icon} {metric}",
                        value=metric in metric_selector.get_default_metrics(),
                        help=desc,
                        key=f"metric_{metric}",
                    ):
                        selected_metrics.append(metric)

    session.selected_metrics = selected_metrics

    # ì„ íƒëœ ë©”íŠ¸ë¦­ í‘œì‹œ
    if selected_metrics:
        st.caption(f"Selected: {', '.join(selected_metrics)}")

    # ê³ ê¸‰ ì˜µì…˜
    st.divider()
    with st.expander("âš™ï¸ ê³ ê¸‰ ì˜µì…˜"):
        col1, col2, col3 = st.columns(3)
        with col1:
            session.selected_model = st.selectbox(
                "ëª¨ë¸",
                options=["gpt-5-nano", "gpt-4", "gpt-4o", "claude-3-5-sonnet"],
                index=0,
            )
        with col2:
            session.langfuse_enabled = st.checkbox("Langfuse íŠ¸ë˜í‚¹", value=False)
        with col3:
            session.parallel_processing = st.checkbox("ë³‘ë ¬ ì²˜ë¦¬", value=True)

        # ì„ê³„ê°’ ì„¤ì •
        st.caption("ë©”íŠ¸ë¦­ ì„ê³„ê°’ (Pass/Fail ê¸°ì¤€)")
        threshold_cols = st.columns(len(selected_metrics) if selected_metrics else 1)
        for i, metric in enumerate(selected_metrics[:4]):  # ìµœëŒ€ 4ê°œë§Œ í‘œì‹œ
            with threshold_cols[i]:
                st.number_input(
                    metric,
                    min_value=0.0,
                    max_value=1.0,
                    value=0.7,
                    step=0.1,
                    key=f"threshold_{metric}",
                )

    # ì‹¤í–‰ ë²„íŠ¼
    st.divider()
    can_run = (
        validation_result is not None
        and validation_result.is_valid
        and len(selected_metrics) > 0
        and not session.is_evaluating
    )

    col1, col2 = st.columns([3, 1])
    with col1:
        if st.button("ğŸš€ í‰ê°€ ì‹¤í–‰", type="primary", disabled=not can_run):
            st.info("í‰ê°€ ì‹¤í–‰ ê¸°ëŠ¥ì€ ì•„ì§ êµ¬í˜„ ì¤‘ì…ë‹ˆë‹¤.")
            # TODO: ì‹¤ì œ í‰ê°€ ì‹¤í–‰ ë¡œì§
    with col2:
        if session.is_evaluating:
            st.warning("ì‹¤í–‰ ì¤‘...")

    # ìƒíƒœ ë©”ì‹œì§€
    if not uploaded_file:
        st.info("ğŸ’¡ ë¨¼ì € ë°ì´í„°ì…‹ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")
    elif not validation_result or not validation_result.is_valid:
        st.warning("âš ï¸ ìœ íš¨í•œ ë°ì´í„°ì…‹ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")
    elif not selected_metrics:
        st.warning("âš ï¸ ìµœì†Œ í•˜ë‚˜ì˜ ë©”íŠ¸ë¦­ì„ ì„ íƒí•˜ì„¸ìš”.")


def render_history_page(adapter, session):
    """ì´ë ¥ ì¡°íšŒ í˜ì´ì§€ ë Œë”ë§."""
    import streamlit as st

    from evalvault.adapters.inbound.web.components import (
        HistoryExporter,
        RunFilter,
        RunSearch,
        RunTable,
    )

    st.header("ğŸ“‹ History")
    st.markdown("ì´ì „ í‰ê°€ ê²°ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.")

    # ë°ì´í„° ë¡œë“œ
    all_runs = adapter.list_runs(limit=100)

    # ê²€ìƒ‰ ë° í•„í„° ì„¹ì…˜
    search_col, filter_col = st.columns([2, 1])

    with search_col:
        search_query = st.text_input(
            "ğŸ” ê²€ìƒ‰",
            placeholder="ë°ì´í„°ì…‹ ë˜ëŠ” ëª¨ë¸ ì´ë¦„ìœ¼ë¡œ ê²€ìƒ‰...",
            key="history_search",
        )

    with filter_col, st.popover("ğŸ”§ í•„í„°"):
        # ëª¨ë¸ í•„í„°
        model_options = ["All"] + sorted({r.model_name for r in all_runs})
        selected_model = st.selectbox("ëª¨ë¸", options=model_options, index=0)

        # í†µê³¼ìœ¨ í•„í„°
        min_pass_rate = st.slider("ìµœì†Œ í†µê³¼ìœ¨", 0.0, 1.0, 0.0, 0.1)

        # ë‚ ì§œ í•„í„° (UIë§Œ, ì¶”í›„ êµ¬í˜„)
        st.checkbox("ë‚ ì§œ ë²”ìœ„ í•„í„°", disabled=True, help="ì¶”í›„ êµ¬í˜„ ì˜ˆì •")

    # ê²€ìƒ‰ ì ìš©
    search = RunSearch(query=search_query)
    runs = search.search(all_runs)

    # í•„í„° ì ìš©
    run_filter = RunFilter(
        model_name=selected_model if selected_model != "All" else None,
        min_pass_rate=min_pass_rate if min_pass_rate > 0 else None,
    )
    runs = run_filter.apply(runs)

    # ê²°ê³¼ ìš”ì•½
    st.divider()
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Total Runs", len(runs))
    with col2:
        avg_rate = sum(r.pass_rate for r in runs) / len(runs) if runs else 0
        st.metric("Avg Pass Rate", f"{avg_rate * 100:.1f}%")
    with col3:
        total_cases = sum(r.total_test_cases for r in runs)
        st.metric("Total Test Cases", f"{total_cases:,}")

    # í…Œì´ë¸” ë° ì •ë ¬
    st.divider()

    if runs:
        # ì •ë ¬ ì˜µì…˜
        sort_col, export_col = st.columns([3, 1])
        with sort_col:
            sort_by = st.selectbox(
                "ì •ë ¬ ê¸°ì¤€",
                options=["date", "pass_rate", "dataset", "model"],
                format_func=lambda x: {
                    "date": "ğŸ“… ë‚ ì§œ",
                    "pass_rate": "ğŸ“Š í†µê³¼ìœ¨",
                    "dataset": "ğŸ“ ë°ì´í„°ì…‹",
                    "model": "ğŸ¤– ëª¨ë¸",
                }.get(x, x),
                index=0,
            )
        with export_col:
            exporter = HistoryExporter(runs=runs)
            st.download_button(
                "ğŸ“¥ CSV ë‹¤ìš´ë¡œë“œ",
                data=exporter.to_csv(),
                file_name="evaluation_history.csv",
                mime="text/csv",
            )

        # í…Œì´ë¸” ìƒì„±
        table = RunTable(runs=runs, page_size=10)
        table.sort_by(sort_by, ascending=sort_by == "dataset")

        # í…Œì´ë¸” í—¤ë”
        cols = st.columns([3, 2, 2, 1, 1, 1])
        cols[0].markdown("**Dataset**")
        cols[1].markdown("**Model**")
        cols[2].markdown("**Metrics**")
        cols[3].markdown("**Pass Rate**")
        cols[4].markdown("**Date**")
        cols[5].markdown("**Actions**")

        for run in table.get_current_page_runs():
            cols = st.columns([3, 2, 2, 1, 1, 1])
            cols[0].text(run.dataset_name)
            cols[1].text(run.model_name)
            cols[2].text(
                ", ".join(run.metrics_evaluated[:2])
                + ("..." if len(run.metrics_evaluated) > 2 else "")
            )

            pass_rate_pct = run.pass_rate * 100
            if pass_rate_pct >= 70:
                cols[3].success(f"{pass_rate_pct:.0f}%")
            elif pass_rate_pct >= 50:
                cols[3].warning(f"{pass_rate_pct:.0f}%")
            else:
                cols[3].error(f"{pass_rate_pct:.0f}%")

            cols[4].text(run.started_at.strftime("%m/%d"))
            if cols[5].button("ğŸ‘", key=f"view_{run.run_id}", help="ìƒì„¸ ë³´ê¸°"):
                session.current_run_id = run.run_id

        # í˜ì´ì§€ë„¤ì´ì…˜
        if table.total_pages > 1:
            st.divider()
            page_cols = st.columns([1, 3, 1])
            with page_cols[1]:
                st.caption(f"Page {table.page} of {table.total_pages}")
    else:
        st.info("í‰ê°€ ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤.")


def render_reports_page(adapter, session):
    """ë³´ê³ ì„œ í˜ì´ì§€ ë Œë”ë§."""
    import streamlit as st

    from evalvault.adapters.inbound.web.components import (
        ReportConfig,
        ReportDownloader,
        ReportGenerator,
        ReportPreview,
        ReportTemplate,
        RunSelector,
    )

    st.header("ğŸ“„ Reports")
    st.markdown("í‰ê°€ ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ê³  ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.")

    # í‰ê°€ ì„ íƒ
    runs = adapter.list_runs(limit=50)
    if not runs:
        st.info("ë³´ê³ ì„œë¥¼ ìƒì„±í•  í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ì‹¤í–‰ ì„ íƒ ì„¹ì…˜
    st.subheader("1. í‰ê°€ ì„ íƒ")
    selector = RunSelector(runs=runs)
    options = selector.get_options()

    selected_option = st.selectbox(
        "í‰ê°€ ì‹¤í–‰ ì„ íƒ",
        options=options,
        format_func=lambda x: x,
        help="ë³´ê³ ì„œë¥¼ ìƒì„±í•  í‰ê°€ ì‹¤í–‰ì„ ì„ íƒí•˜ì„¸ìš”.",
    )

    # ì„ íƒëœ ì‹¤í–‰ ID ì¶”ì¶œ
    selected_run_id = selected_option.split(" | ")[0] if selected_option else None
    selected_run = selector.get_by_id(selected_run_id) if selected_run_id else None

    if selected_run:
        # ì„ íƒëœ í‰ê°€ ì •ë³´ í‘œì‹œ
        info_col1, info_col2, info_col3, info_col4 = st.columns(4)
        with info_col1:
            st.metric("Dataset", selected_run.dataset_name)
        with info_col2:
            st.metric("Model", selected_run.model_name)
        with info_col3:
            st.metric("Pass Rate", f"{selected_run.pass_rate:.1%}")
        with info_col4:
            st.metric("Test Cases", selected_run.total_test_cases)

    # ë³´ê³ ì„œ ì˜µì…˜ ì„¹ì…˜
    st.divider()
    st.subheader("2. ë³´ê³ ì„œ ì„¤ì •")

    col1, col2 = st.columns(2)

    with col1:
        # í…œí”Œë¦¿ ì„ íƒ
        templates = ReportTemplate.list_templates()
        template_descriptions = {t: ReportTemplate.get_description(t) for t in templates}

        selected_template = st.selectbox(
            "í…œí”Œë¦¿",
            options=templates,
            format_func=lambda x: f"{x.title()} - {template_descriptions.get(x, '')}",
        )

        # ì¶œë ¥ í˜•ì‹
        output_format = st.radio(
            "ì¶œë ¥ í˜•ì‹",
            options=["markdown", "html"],
            format_func=lambda x: {"markdown": "ğŸ“ Markdown", "html": "ğŸŒ HTML"}.get(x, x),
            horizontal=True,
        )

    with col2:
        # í¬í•¨ ì˜µì…˜
        st.caption("í¬í•¨ í•­ëª©")
        include_summary = st.checkbox("ìš”ì•½", value=True)
        include_metrics_detail = st.checkbox("ë©”íŠ¸ë¦­ ìƒì„¸", value=True)
        include_charts = st.checkbox("ì°¨íŠ¸", value=True, disabled=True, help="HTML í˜•ì‹ì—ì„œë§Œ ì§€ì›")
        include_nlp = st.checkbox("NLP ë¶„ì„", value=False)
        include_causal = st.checkbox("ì¸ê³¼ ë¶„ì„", value=False)

    # ë³´ê³ ì„œ ìƒì„± ì„¹ì…˜
    st.divider()
    st.subheader("3. ë³´ê³ ì„œ ìƒì„±")

    # ì„¤ì • ìƒì„±
    config = ReportConfig(
        output_format=output_format,
        include_summary=include_summary,
        include_metrics_detail=include_metrics_detail,
        include_charts=include_charts and output_format == "html",
        include_nlp_analysis=include_nlp,
        include_causal_analysis=include_causal,
        template_name=selected_template,
    )

    gen_col1, gen_col2 = st.columns([1, 3])

    with gen_col1:
        generate_clicked = st.button(
            "ğŸ“ ë³´ê³ ì„œ ìƒì„±",
            type="primary",
            disabled=selected_run is None,
        )

    # ë³´ê³ ì„œ ìƒì„± ë° ë¯¸ë¦¬ë³´ê¸°
    if generate_clicked and selected_run:
        with st.spinner("ë³´ê³ ì„œ ìƒì„± ì¤‘..."):
            # ë©”íŠ¸ë¦­ ì ìˆ˜ (Mock - ì‹¤ì œë¡œëŠ” adapterì—ì„œ ì¡°íšŒ)
            metrics = dict.fromkeys(selected_run.metrics_evaluated, 0.8)

            # ë³´ê³ ì„œ ìƒì„±
            generator = ReportGenerator(config=config)
            report_result = generator.generate(run=selected_run, metrics=metrics)

            # ì„¸ì…˜ì— ì €ì¥
            session.generated_report = report_result

        st.success("âœ… ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ!")

    # ë¯¸ë¦¬ë³´ê¸° ë° ë‹¤ìš´ë¡œë“œ
    if hasattr(session, "generated_report") and session.generated_report:
        report_result = session.generated_report

        st.divider()
        st.subheader("4. ë³´ê³ ì„œ ë¯¸ë¦¬ë³´ê¸°")

        # í†µê³„ í‘œì‹œ
        preview = ReportPreview(result=report_result)
        stats = preview.get_stats()

        stat_col1, stat_col2, stat_col3 = st.columns(3)
        with stat_col1:
            st.caption(f"ğŸ“„ {stats['char_count']:,} ë¬¸ì")
        with stat_col2:
            st.caption(f"ğŸ“ {stats['line_count']} ì¤„")
        with stat_col3:
            st.caption(f"ğŸ“Š í˜•ì‹: {report_result.format.upper()}")

        # ë¯¸ë¦¬ë³´ê¸° ë‚´ìš©
        with st.expander("ğŸ“– ë¯¸ë¦¬ë³´ê¸°", expanded=True):
            if report_result.format == "html":
                st.components.v1.html(report_result.content, height=500, scrolling=True)
            else:
                st.markdown(preview.get_preview())

        # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
        st.divider()
        downloader = ReportDownloader(result=report_result)
        download_data = downloader.prepare_download()

        st.download_button(
            label=f"ğŸ“¥ {report_result.format.upper()} ë‹¤ìš´ë¡œë“œ",
            data=download_data["data"],
            file_name=download_data["filename"],
            mime=download_data["mime_type"],
            type="primary",
        )


def render_improvement_page(adapter, session):
    """ê°œì„  ê°€ì´ë“œ í˜ì´ì§€ ë Œë”ë§."""
    import streamlit as st

    from evalvault.adapters.inbound.web.components import RunSelector

    st.header("ğŸ”§ ê°œì„  ê°€ì´ë“œ")
    st.markdown("í‰ê°€ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ RAG ì‹œìŠ¤í…œ ê°œì„  ë°©ì•ˆì„ ì œì•ˆí•©ë‹ˆë‹¤.")

    # í‰ê°€ ê²°ê³¼ ì¡°íšŒ
    runs = adapter.list_runs(limit=50)

    if not runs:
        st.info("ë¶„ì„í•  í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € í‰ê°€ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return

    # ì‹¤í–‰ ì„ íƒ ì„¹ì…˜
    st.subheader("1. í‰ê°€ ì„ íƒ")
    selector = RunSelector(runs=runs)
    options = selector.get_options()

    selected_option = st.selectbox(
        "ë¶„ì„í•  í‰ê°€ ì‹¤í–‰ ì„ íƒ",
        options=options,
        format_func=lambda x: x,
        help="ê°œì„  ê°€ì´ë“œë¥¼ ìƒì„±í•  í‰ê°€ ì‹¤í–‰ì„ ì„ íƒí•˜ì„¸ìš”.",
    )

    # ì„ íƒëœ ì‹¤í–‰ ID ì¶”ì¶œ
    selected_run_id = selected_option.split(" | ")[0] if selected_option else None
    selected_run = selector.get_by_id(selected_run_id) if selected_run_id else None

    if not selected_run:
        return

    # ì„ íƒëœ í‰ê°€ ì •ë³´ ë° í’ˆì§ˆ ê²Œì´íŠ¸
    st.divider()
    st.subheader("2. í’ˆì§ˆ í˜„í™©")

    # í’ˆì§ˆ ê²Œì´íŠ¸ ì²´í¬
    try:
        gate_report = adapter.check_quality_gate(selected_run_id)

        # ì „ì²´ ìƒíƒœ í‘œì‹œ
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ì „ì²´ í†µê³¼ìœ¨", f"{selected_run.pass_rate:.1%}")
        with col2:
            if gate_report.overall_passed:
                st.success("âœ… í’ˆì§ˆ ê²Œì´íŠ¸ PASS")
            else:
                st.error("âŒ í’ˆì§ˆ ê²Œì´íŠ¸ FAIL")
        with col3:
            st.metric("í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤", selected_run.total_test_cases)

        # ë©”íŠ¸ë¦­ë³„ í˜„í™©
        st.markdown("**ë©”íŠ¸ë¦­ë³„ í˜„í™©**")
        for result in gate_report.results:
            col1, col2, col3, col4 = st.columns([3, 1, 1, 1])
            with col1:
                # í”„ë¡œê·¸ë ˆìŠ¤ ë°”
                st.progress(result.score, text=result.metric)
            with col2:
                st.text(f"{result.score:.2f}")
            with col3:
                st.text(f"/ {result.threshold:.2f}")
            with col4:
                if result.passed:
                    st.success("âœ…")
                else:
                    st.error("âŒ")

    except Exception as e:
        st.warning(f"í’ˆì§ˆ ê²Œì´íŠ¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")

    # ê°œì„  ê°€ì´ë“œ ìƒì„± ì˜µì…˜
    st.divider()
    st.subheader("3. ê°œì„  ê°€ì´ë“œ ìƒì„±")

    col1, col2 = st.columns([2, 1])
    with col1:
        include_llm = st.checkbox(
            "LLM ë¶„ì„ í¬í•¨",
            value=False,
            help="LLMì„ ì‚¬ìš©í•˜ì—¬ ë” ìƒì„¸í•œ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. (ì¶”ê°€ ë¹„ìš© ë°œìƒ)",
        )
    with col2:
        generate_clicked = st.button("ğŸ” ë¶„ì„ ì‹œì‘", type="primary")

    # ê°œì„  ê°€ì´ë“œ ìƒì„±
    if generate_clicked:
        with st.spinner("ê°œì„  ê°€ì´ë“œ ìƒì„± ì¤‘..."):
            try:
                report = adapter.get_improvement_guide(
                    selected_run_id,
                    include_llm=include_llm,
                )

                # ì„¸ì…˜ì— ì €ì¥
                session.improvement_report = report

            except Exception as e:
                st.error(f"ê°œì„  ê°€ì´ë“œ ìƒì„± ì‹¤íŒ¨: {e}")
                return

    # ê°œì„  ê°€ì´ë“œ í‘œì‹œ
    if hasattr(session, "improvement_report") and session.improvement_report:
        report = session.improvement_report

        st.divider()
        st.subheader("4. ê°œì„  ê°€ì´ë“œ")

        # ìš”ì•½
        st.markdown(f"""
        **ë¶„ì„ ìš”ì•½**
        - ë¶„ì„ ëŒ€ìƒ: {report.run_id}
        - í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {report.total_test_cases}ê°œ
        - ì‹¤íŒ¨ ì¼€ì´ìŠ¤: {report.failed_test_cases}ê°œ
        - í†µê³¼ìœ¨: {report.pass_rate:.1%}
        """)

        # ê°€ì´ë“œ ëª©ë¡
        if report.guides:
            for i, guide in enumerate(report.guides, 1):
                priority_colors = {
                    "P0_CRITICAL": "ğŸ”´",
                    "P1_HIGH": "ğŸŸ ",
                    "P2_MEDIUM": "ğŸŸ¡",
                    "P3_LOW": "ğŸŸ¢",
                }
                priority_icon = priority_colors.get(guide.priority.name, "âšª")

                with st.expander(
                    f"{priority_icon} {i}. {guide.component.value.title()} ê°œì„  "
                    f"(ì˜ˆìƒ +{guide.total_expected_improvement:.0%})",
                    expanded=i == 1,
                ):
                    # ëŒ€ìƒ ë©”íŠ¸ë¦­
                    st.markdown(f"**ëŒ€ìƒ ë©”íŠ¸ë¦­**: {', '.join(guide.target_metrics)}")

                    # ì¦ê±° ë°ì´í„°
                    if guide.evidence:
                        st.markdown("**ì¦ê±° ë°ì´í„°**")
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("ì‹¤íŒ¨ ì¼€ì´ìŠ¤", guide.evidence.total_failures)
                        with col2:
                            if guide.evidence.avg_score_failures:
                                st.metric(
                                    "ì‹¤íŒ¨ í‰ê·  ì ìˆ˜",
                                    f"{guide.evidence.avg_score_failures:.2f}",
                                )
                        with col3:
                            if guide.evidence.avg_score_passes:
                                st.metric(
                                    "í†µê³¼ í‰ê·  ì ìˆ˜",
                                    f"{guide.evidence.avg_score_passes:.2f}",
                                )

                    # ê°œì„  ì•¡ì…˜
                    st.markdown("**ê¶Œì¥ ì•¡ì…˜**")
                    for j, action in enumerate(guide.actions, 1):
                        effort_icons = {"low": "ğŸŸ¢", "medium": "ğŸŸ¡", "high": "ğŸ”´"}
                        effort_icon = effort_icons.get(action.effort.value, "âšª")

                        st.markdown(
                            f"{j}. **{action.title}** {effort_icon} "
                            f"(ì˜ˆìƒ +{action.expected_improvement:.0%})"
                        )
                        st.caption(action.description)

                        if action.implementation_hint:
                            st.code(action.implementation_hint, language="python")

                    # ê²€ì¦ ë°©ë²•
                    if guide.verification_command:
                        st.markdown("**ê²€ì¦ ë°©ë²•**")
                        st.code(guide.verification_command, language="bash")
        else:
            st.info("íƒì§€ëœ ê°œì„  íŒ¨í„´ì´ ì—†ìŠµë‹ˆë‹¤. í˜„ì¬ ì‹œìŠ¤í…œì´ ì–‘í˜¸í•œ ìƒíƒœì…ë‹ˆë‹¤.")

        # ë§ˆí¬ë‹¤ìš´ ë‹¤ìš´ë¡œë“œ
        st.divider()
        if hasattr(report, "to_markdown"):
            st.download_button(
                "ğŸ“¥ ë§ˆí¬ë‹¤ìš´ ë‹¤ìš´ë¡œë“œ",
                data=report.to_markdown(),
                file_name=f"improvement_guide_{report.run_id}.md",
                mime="text/markdown",
            )


def main():
    """Streamlit ì•± ì§„ì…ì ."""
    create_app()


if __name__ == "__main__":
    main()
