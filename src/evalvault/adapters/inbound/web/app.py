"""EvalVault Web UI - Streamlit Application."""

from __future__ import annotations

import sys
from pathlib import Path

# Streamlit ì•± ì‹¤í–‰ ì‹œ src ê²½ë¡œ ì¶”ê°€
src_path = Path(__file__).parent.parent.parent.parent.parent
if str(src_path) not in sys.path:
    sys.path.insert(0, str(src_path))


def create_app():
    """Streamlit ì•± ìƒì„± ë° ì„¤ì •."""
    import streamlit as st

    from evalvault.adapters.inbound.web.adapter import create_adapter
    from evalvault.adapters.inbound.web.session import init_session

    # í˜ì´ì§€ ì„¤ì •
    st.set_page_config(
        page_title="EvalVault",
        page_icon="ğŸ“Š",
        layout="wide",
        initial_sidebar_state="expanded",
    )

    # ì„¸ì…˜ ì´ˆê¸°í™”
    session = init_session()

    # ì–´ëŒ‘í„° ì´ˆê¸°í™” (ì„¸ì…˜ì— ìºì‹œ)
    if "adapter" not in st.session_state:
        st.session_state.adapter = create_adapter()

    adapter = st.session_state.adapter

    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.title("ğŸ“Š EvalVault")
        st.caption("RAG Evaluation System")

        st.divider()

        # ë„¤ë¹„ê²Œì´ì…˜
        page = st.radio(
            "Navigation",
            options=["ğŸ  Home", "ğŸ“Š Evaluate", "ğŸ“‹ History", "ğŸ“„ Reports"],
            label_visibility="collapsed",
        )

        st.divider()

        # ì„¤ì • ì„¹ì…˜
        with st.expander("âš™ï¸ Settings", expanded=False):
            st.caption("Model Configuration")
            model = st.selectbox(
                "Default Model",
                options=["gpt-5-nano", "gpt-4", "gpt-4o", "claude-3-5-sonnet"],
                index=0,
            )
            session.selected_model = model

        # ë²„ì „ ì •ë³´
        st.caption("v1.3.0 | Powered by Ragas + Langfuse")

    # ë©”ì¸ ì»¨í…ì¸ 
    if page == "ğŸ  Home":
        render_home_page(adapter, session)
    elif page == "ğŸ“Š Evaluate":
        render_evaluate_page(adapter, session)
    elif page == "ğŸ“‹ History":
        render_history_page(adapter, session)
    elif page == "ğŸ“„ Reports":
        render_reports_page(adapter, session)


def render_home_page(adapter, session):
    """í™ˆ í˜ì´ì§€ ë Œë”ë§."""
    import streamlit as st

    from evalvault.adapters.inbound.web.components import (
        DashboardStats,
        MetricSummaryCard,
        RecentRunsList,
        create_pass_rate_chart,
        create_trend_chart,
    )

    st.header("Welcome to EvalVault")
    st.markdown(
        """
        EvalVaultëŠ” RAG (Retrieval-Augmented Generation) ì‹œìŠ¤í…œì„ í‰ê°€í•˜ê³ 
        ë¶„ì„í•˜ê¸° ìœ„í•œ ë„êµ¬ì…ë‹ˆë‹¤.
        """
    )

    # í‰ê°€ ë°ì´í„° ì¡°íšŒ
    runs = adapter.list_runs(limit=20)

    # ëŒ€ì‹œë³´ë“œ í†µê³„ ê³„ì‚°
    stats = DashboardStats.from_runs(runs)

    # í†µê³„ ì¹´ë“œ ì„¹ì…˜
    st.subheader("Overview")
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        card = MetricSummaryCard(
            title="Total Runs",
            value=stats.total_runs,
            format_type="number",
        )
        st.metric(label=card.title, value=card.formatted_value)

    with col2:
        card = MetricSummaryCard(
            title="Test Cases",
            value=stats.total_test_cases,
            format_type="number",
        )
        st.metric(label=card.title, value=card.formatted_value)

    with col3:
        card = MetricSummaryCard(
            title="Avg Pass Rate",
            value=stats.avg_pass_rate,
            format_type="percent",
        )
        st.metric(label=card.title, value=card.formatted_value)

    with col4:
        card = MetricSummaryCard(
            title="Total Cost",
            value=stats.total_cost,
            format_type="currency",
        )
        st.metric(label=card.title, value=card.formatted_value)

    # ì°¨íŠ¸ ì„¹ì…˜
    st.divider()
    chart_col1, chart_col2 = st.columns(2)

    with chart_col1:
        pass_rate_fig = create_pass_rate_chart(runs[:10])
        st.plotly_chart(pass_rate_fig, use_container_width=True)

    with chart_col2:
        trend_fig = create_trend_chart(runs)
        st.plotly_chart(trend_fig, use_container_width=True)

    # ì§€ì› ë©”íŠ¸ë¦­ ì„¹ì…˜
    st.divider()
    with st.expander("ğŸ“Š ì§€ì› ë©”íŠ¸ë¦­", expanded=False):
        metrics = adapter.get_available_metrics()
        descriptions = adapter.get_metric_descriptions()

        cols = st.columns(3)
        for i, metric in enumerate(metrics):
            with cols[i % 3]:
                st.markdown(
                    f"""
                    <div style="
                        padding: 0.75rem;
                        border-radius: 0.5rem;
                        border: 1px solid #334155;
                        margin-bottom: 0.5rem;
                    ">
                        <strong>{metric}</strong><br>
                        <small style="color: #94A3B8;">{descriptions.get(metric, "")}</small>
                    </div>
                    """,
                    unsafe_allow_html=True,
                )

    # ìµœê·¼ í‰ê°€ ëª©ë¡
    st.divider()
    st.subheader("ìµœê·¼ í‰ê°€")

    recent_list = RecentRunsList(runs=runs, max_items=5)

    if not recent_list.is_empty:
        for run in recent_list.displayed_runs:
            col1, col2, col3, col4 = st.columns([3, 1, 1, 1])
            with col1:
                emoji = recent_list.get_pass_rate_emoji(run.pass_rate)
                st.text(f"{emoji} {run.dataset_name}")
            with col2:
                st.text(run.model_name)
            with col3:
                pass_rate_pct = run.pass_rate * 100
                if pass_rate_pct >= 70:
                    st.success(f"{pass_rate_pct:.1f}%")
                elif pass_rate_pct >= 50:
                    st.warning(f"{pass_rate_pct:.1f}%")
                else:
                    st.error(f"{pass_rate_pct:.1f}%")
            with col4:
                st.text(run.started_at.strftime("%m/%d"))

        if recent_list.has_more:
            st.caption(f"+{recent_list.remaining_count} more runs...")
    else:
        st.info("ì•„ì§ í‰ê°€ ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤. ì²« í‰ê°€ë¥¼ ì‹¤í–‰í•´ë³´ì„¸ìš”!")


def render_evaluate_page(adapter, session):
    """í‰ê°€ ì‹¤í–‰ í˜ì´ì§€ ë Œë”ë§."""
    import streamlit as st

    from evalvault.adapters.inbound.web.components import (
        FileUploadHandler,
        MetricSelector,
    )

    st.header("ğŸ“Š Evaluate")
    st.markdown("ë°ì´í„°ì…‹ì„ ì—…ë¡œë“œí•˜ê³  RAG í‰ê°€ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.")

    # ì´ˆê¸°í™”
    upload_handler = FileUploadHandler()
    metric_selector = MetricSelector()

    # íŒŒì¼ ì—…ë¡œë“œ ì„¹ì…˜
    st.subheader("1. ë°ì´í„°ì…‹ ì—…ë¡œë“œ")

    uploaded_file = st.file_uploader(
        "ë°ì´í„°ì…‹ ì—…ë¡œë“œ",
        type=["csv", "json", "xlsx"],
        help="CSV, JSON, ë˜ëŠ” Excel í˜•ì‹ì˜ ë°ì´í„°ì…‹ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.",
    )

    validation_result = None
    if uploaded_file:
        # íŒŒì¼ ê²€ì¦
        content = uploaded_file.read()
        uploaded_file.seek(0)  # ë‹¤ì‹œ ì½ì„ ìˆ˜ ìˆë„ë¡ ë¦¬ì…‹

        validation_result = upload_handler.validate_file(uploaded_file.name, content)

        if validation_result.is_valid:
            st.success(
                f"âœ… {uploaded_file.name} ({validation_result.row_count} rows, "
                f"{validation_result.file_type.upper()})"
            )
            if validation_result.dataset_name:
                st.caption(f"Dataset: {validation_result.dataset_name}")
        else:
            st.error(f"âŒ {validation_result.error_message}")

    # ë©”íŠ¸ë¦­ ì„ íƒ ì„¹ì…˜
    st.divider()
    st.subheader("2. ë©”íŠ¸ë¦­ ì„ íƒ")

    # ì¹´í…Œê³ ë¦¬ë³„ ê·¸ë£¹í™”
    categories = metric_selector.get_metrics_by_category()

    selected_metrics = []
    for category, metrics in categories.items():
        with st.expander(f"ğŸ“ {category.title()}", expanded=category == "generation"):
            cols = st.columns(2)
            for i, metric in enumerate(metrics):
                with cols[i % 2]:
                    icon = metric_selector.get_icon(metric)
                    desc = metric_selector.get_description(metric)
                    if st.checkbox(
                        f"{icon} {metric}",
                        value=metric in metric_selector.get_default_metrics(),
                        help=desc,
                        key=f"metric_{metric}",
                    ):
                        selected_metrics.append(metric)

    session.selected_metrics = selected_metrics

    # ì„ íƒëœ ë©”íŠ¸ë¦­ í‘œì‹œ
    if selected_metrics:
        st.caption(f"Selected: {', '.join(selected_metrics)}")

    # ê³ ê¸‰ ì˜µì…˜
    st.divider()
    with st.expander("âš™ï¸ ê³ ê¸‰ ì˜µì…˜"):
        col1, col2, col3 = st.columns(3)
        with col1:
            session.selected_model = st.selectbox(
                "ëª¨ë¸",
                options=["gpt-5-nano", "gpt-4", "gpt-4o", "claude-3-5-sonnet"],
                index=0,
            )
        with col2:
            session.langfuse_enabled = st.checkbox("Langfuse íŠ¸ë˜í‚¹", value=False)
        with col3:
            session.parallel_processing = st.checkbox("ë³‘ë ¬ ì²˜ë¦¬", value=True)

        # ì„ê³„ê°’ ì„¤ì •
        st.caption("ë©”íŠ¸ë¦­ ì„ê³„ê°’ (Pass/Fail ê¸°ì¤€)")
        threshold_cols = st.columns(len(selected_metrics) if selected_metrics else 1)
        for i, metric in enumerate(selected_metrics[:4]):  # ìµœëŒ€ 4ê°œë§Œ í‘œì‹œ
            with threshold_cols[i]:
                st.number_input(
                    metric,
                    min_value=0.0,
                    max_value=1.0,
                    value=0.7,
                    step=0.1,
                    key=f"threshold_{metric}",
                )

    # ì‹¤í–‰ ë²„íŠ¼
    st.divider()
    can_run = (
        validation_result is not None
        and validation_result.is_valid
        and len(selected_metrics) > 0
        and not session.is_evaluating
    )

    col1, col2 = st.columns([3, 1])
    with col1:
        if st.button("ğŸš€ í‰ê°€ ì‹¤í–‰", type="primary", disabled=not can_run):
            st.info("í‰ê°€ ì‹¤í–‰ ê¸°ëŠ¥ì€ ì•„ì§ êµ¬í˜„ ì¤‘ì…ë‹ˆë‹¤.")
            # TODO: ì‹¤ì œ í‰ê°€ ì‹¤í–‰ ë¡œì§
    with col2:
        if session.is_evaluating:
            st.warning("ì‹¤í–‰ ì¤‘...")

    # ìƒíƒœ ë©”ì‹œì§€
    if not uploaded_file:
        st.info("ğŸ’¡ ë¨¼ì € ë°ì´í„°ì…‹ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")
    elif not validation_result or not validation_result.is_valid:
        st.warning("âš ï¸ ìœ íš¨í•œ ë°ì´í„°ì…‹ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")
    elif not selected_metrics:
        st.warning("âš ï¸ ìµœì†Œ í•˜ë‚˜ì˜ ë©”íŠ¸ë¦­ì„ ì„ íƒí•˜ì„¸ìš”.")


def render_history_page(adapter, session):
    """ì´ë ¥ ì¡°íšŒ í˜ì´ì§€ ë Œë”ë§."""
    import streamlit as st

    from evalvault.adapters.inbound.web.components import (
        HistoryExporter,
        RunFilter,
        RunSearch,
        RunTable,
    )

    st.header("ğŸ“‹ History")
    st.markdown("ì´ì „ í‰ê°€ ê²°ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.")

    # ë°ì´í„° ë¡œë“œ
    all_runs = adapter.list_runs(limit=100)

    # ê²€ìƒ‰ ë° í•„í„° ì„¹ì…˜
    search_col, filter_col = st.columns([2, 1])

    with search_col:
        search_query = st.text_input(
            "ğŸ” ê²€ìƒ‰",
            placeholder="ë°ì´í„°ì…‹ ë˜ëŠ” ëª¨ë¸ ì´ë¦„ìœ¼ë¡œ ê²€ìƒ‰...",
            key="history_search",
        )

    with filter_col, st.popover("ğŸ”§ í•„í„°"):
        # ëª¨ë¸ í•„í„°
        model_options = ["All"] + sorted({r.model_name for r in all_runs})
        selected_model = st.selectbox("ëª¨ë¸", options=model_options, index=0)

        # í†µê³¼ìœ¨ í•„í„°
        min_pass_rate = st.slider("ìµœì†Œ í†µê³¼ìœ¨", 0.0, 1.0, 0.0, 0.1)

        # ë‚ ì§œ í•„í„° (UIë§Œ, ì¶”í›„ êµ¬í˜„)
        st.checkbox("ë‚ ì§œ ë²”ìœ„ í•„í„°", disabled=True, help="ì¶”í›„ êµ¬í˜„ ì˜ˆì •")

    # ê²€ìƒ‰ ì ìš©
    search = RunSearch(query=search_query)
    runs = search.search(all_runs)

    # í•„í„° ì ìš©
    run_filter = RunFilter(
        model_name=selected_model if selected_model != "All" else None,
        min_pass_rate=min_pass_rate if min_pass_rate > 0 else None,
    )
    runs = run_filter.apply(runs)

    # ê²°ê³¼ ìš”ì•½
    st.divider()
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Total Runs", len(runs))
    with col2:
        avg_rate = sum(r.pass_rate for r in runs) / len(runs) if runs else 0
        st.metric("Avg Pass Rate", f"{avg_rate * 100:.1f}%")
    with col3:
        total_cases = sum(r.total_test_cases for r in runs)
        st.metric("Total Test Cases", f"{total_cases:,}")

    # í…Œì´ë¸” ë° ì •ë ¬
    st.divider()

    if runs:
        # ì •ë ¬ ì˜µì…˜
        sort_col, export_col = st.columns([3, 1])
        with sort_col:
            sort_by = st.selectbox(
                "ì •ë ¬ ê¸°ì¤€",
                options=["date", "pass_rate", "dataset", "model"],
                format_func=lambda x: {
                    "date": "ğŸ“… ë‚ ì§œ",
                    "pass_rate": "ğŸ“Š í†µê³¼ìœ¨",
                    "dataset": "ğŸ“ ë°ì´í„°ì…‹",
                    "model": "ğŸ¤– ëª¨ë¸",
                }.get(x, x),
                index=0,
            )
        with export_col:
            exporter = HistoryExporter(runs=runs)
            st.download_button(
                "ğŸ“¥ CSV ë‹¤ìš´ë¡œë“œ",
                data=exporter.to_csv(),
                file_name="evaluation_history.csv",
                mime="text/csv",
            )

        # í…Œì´ë¸” ìƒì„±
        table = RunTable(runs=runs, page_size=10)
        table.sort_by(sort_by, ascending=sort_by == "dataset")

        # í…Œì´ë¸” í—¤ë”
        cols = st.columns([3, 2, 2, 1, 1, 1])
        cols[0].markdown("**Dataset**")
        cols[1].markdown("**Model**")
        cols[2].markdown("**Metrics**")
        cols[3].markdown("**Pass Rate**")
        cols[4].markdown("**Date**")
        cols[5].markdown("**Actions**")

        for run in table.get_current_page_runs():
            cols = st.columns([3, 2, 2, 1, 1, 1])
            cols[0].text(run.dataset_name)
            cols[1].text(run.model_name)
            cols[2].text(
                ", ".join(run.metrics_evaluated[:2])
                + ("..." if len(run.metrics_evaluated) > 2 else "")
            )

            pass_rate_pct = run.pass_rate * 100
            if pass_rate_pct >= 70:
                cols[3].success(f"{pass_rate_pct:.0f}%")
            elif pass_rate_pct >= 50:
                cols[3].warning(f"{pass_rate_pct:.0f}%")
            else:
                cols[3].error(f"{pass_rate_pct:.0f}%")

            cols[4].text(run.started_at.strftime("%m/%d"))
            if cols[5].button("ğŸ‘", key=f"view_{run.run_id}", help="ìƒì„¸ ë³´ê¸°"):
                session.current_run_id = run.run_id

        # í˜ì´ì§€ë„¤ì´ì…˜
        if table.total_pages > 1:
            st.divider()
            page_cols = st.columns([1, 3, 1])
            with page_cols[1]:
                st.caption(f"Page {table.page} of {table.total_pages}")
    else:
        st.info("í‰ê°€ ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤.")


def render_reports_page(adapter, session):
    """ë³´ê³ ì„œ í˜ì´ì§€ ë Œë”ë§."""
    import streamlit as st

    st.header("ğŸ“„ Reports")
    st.markdown("í‰ê°€ ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ê³  ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.")

    # í‰ê°€ ì„ íƒ
    runs = adapter.list_runs(limit=20)
    if not runs:
        st.info("ë³´ê³ ì„œë¥¼ ìƒì„±í•  í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    run_options = {
        f"{r.dataset_name} ({r.started_at.strftime('%Y-%m-%d')})": r.run_id for r in runs
    }
    selected = st.selectbox("í‰ê°€ ì„ íƒ", options=list(run_options.keys()))
    session.selected_report_run_id = run_options.get(selected)

    # ë³´ê³ ì„œ ì˜µì…˜
    st.subheader("ë³´ê³ ì„œ ì˜µì…˜")
    col1, col2 = st.columns(2)
    with col1:
        session.report_format = st.radio("ì¶œë ¥ í˜•ì‹", options=["Markdown", "HTML"], horizontal=True)
    with col2:
        session.include_nlp = st.checkbox("NLP ë¶„ì„ í¬í•¨", value=True)
        session.include_causal = st.checkbox("ì¸ê³¼ ë¶„ì„ í¬í•¨", value=True)

    # ìƒì„± ë²„íŠ¼
    st.divider()
    if st.button("ğŸ“ ë³´ê³ ì„œ ìƒì„±", type="primary"):
        st.info("ë³´ê³ ì„œ ìƒì„± ê¸°ëŠ¥ì€ ì•„ì§ êµ¬í˜„ ì¤‘ì…ë‹ˆë‹¤.")
        # TODO: ì‹¤ì œ ë³´ê³ ì„œ ìƒì„± ë¡œì§


def main():
    """Streamlit ì•± ì§„ì…ì ."""
    create_app()


if __name__ == "__main__":
    main()
