# AI 기반 LLM 평가 및 분석 자동화: EvalVault 도입 사례와 실무 가이드

## 1. 한 줄 요약과 세션 목적

- 한 줄 요약: EvalVault는 평가 결과를 `run_id`로 묶어 “평가 → 분석 → 비교 → 개선”을
  자동 루프로 만들고, 회귀를 CI에서 자동 감지해 PR에 결과를 남긴다.
- 세션 목적: 사례를 ‘구현 방법’ 관점으로 전환해, 누구나 같은 흐름으로 재현할 수 있게 한다.

---

## 2. 핵심 메시지 (반드시 전달)

1) AI로 이 업무가 이렇게나 줄었어요. 안 하면 손해!
   - Before: 점수만 보고 판단, 원인 추적 어려움, 재현 불가.
   - After: 결과+근거+비교+게이트가 자동화되어 사람이 판단만 하면 됨.

2) 이 업무는 이런 프로세스로 자동화했어요.
   - 평가 → 분석 → 비교 → 개선 → 게이트(자동 차단)
   - 각 단계는 툴이 “사람이 하던 반복 업무”를 대신한다.

3) 실제 구현 과정과 방법
   - 기술 나열이 아니라 “왜 이 단계가 필요한지”와 “툴의 역할” 중심 설명.

4) Lesson Learned
   - 점수만 보면 놓친다 → 근거(아티팩트)가 필요하다.
   - 재현 조건이 없으면 개선도 없다.
   - 자동 게이트가 없으면 결국 실수한다.

---

## 2.1 바로 써보는 3분 체험 (비전문가용)

목표: “실행했다” 경험을 3분 안에 만든다.

```bash
uv sync --extra dev
cp .env.example .env

uv run evalvault run --mode simple tests/fixtures/e2e/insurance_qa_korean.json \
  --metrics faithfulness,answer_relevancy \
  --profile dev \
  --db data/db/evalvault.db \
  --auto-analyze
```

이후 확인 포인트:
- 리포트: `reports/analysis/analysis_<RUN_ID>.md`
- 근거: `reports/analysis/artifacts/analysis_<RUN_ID>/index.json`
- “점수만”이 아니라 “왜 그렇게 나왔는지”가 보이면 성공

---

## 2.2 누가/언제 쓰는가 (한 줄 설명)

- PM: 기능 변경이 “진짜 개선”인지 숫자로 확인해야 할 때
- CS/QA: 고객 이슈가 재현되는지 빠르게 확인해야 할 때
- ML/데이터: 프롬프트·모델 변경 후 회귀를 막아야 할 때
- 팀 리드: 의사결정을 결과가 아닌 “근거 포함 리포트”로 바꾸고 싶을 때

---

## 2.3 실패/리스크 사례와 예방

- 실패 사례: 점수만 보고 개선했다고 판단 → 실제 배포 후 회귀 발생
- 실패 사례: 데이터 매핑 오류로 평가 전체가 오염 → 잘못된 개선 방향
- EvalVault 예방: run_id + 아티팩트로 원인을 추적하고, gate/ci-gate로 회귀를 자동 감지한다

---

## 2.4 용어 미니 사전 (비전문가용)

- run_id: “이번 평가 전체를 묶는 고유한 실행 번호”
- metric: “좋고 나쁨을 판단하는 기준(점수)”
- dataset: “평가에 쓰는 질문/정답/문서 묶음”
- artifact: “왜 그 점수가 나왔는지 보여주는 증거 파일”
- regression gate: “예전보다 나빠지면 자동으로 막는 안전장치”

---

## 2.5 ROI 미니 계산법

간단 계산식:
`월 평가 횟수 × 평가당 소요 시간 × 시급`

예시:
`40회 × 2시간 × 5만원 = 월 400만원`

자동화가 70%만 줄여도 월 280만원 수준의 절감이 가능하다.

---

## 2.6 직접 만들어보고 싶은 사람을 위한 이유

- 사내 보안/데이터 정책에 맞게 내부 전용 평가 루프를 만들 수 있다.
- 팀 상황에 맞는 지표와 임계값을 설계해 “내 업무 기준”을 고정할 수 있다.
- 외부 툴 비용을 줄이고, 분석 근거를 회사 자산으로 남길 수 있다.

---

## 2.7 작은 시작 로드맵 (1~3주)

1주차: 샘플 데이터셋으로 실행/리포트 읽기
2주차: 내 데이터셋 매핑 + 기본 메트릭 적용
3주차: 비교/회귀 게이트를 CI 또는 PR에 붙이기

---

## 2.8 현실적인 한계/주의

- 정답 라벨이 없으면 점수 해석이 흔들릴 수 있다.
- 지표가 높아도 실제 사용자 만족도와 다를 수 있다.
- 데이터 매핑이 틀리면 전체 결과가 오염되므로, 초기 검증이 필수다.

---

## 3. 배경: 자동화 도입 전의 업무 환경과 도전 과제

자동화된 평가 체계가 없던 시기, 기업은 모델 개선을 위해 막대한 인적 비용을 지불했고,
“왜 점수가 달라졌는가”를 설명할 방법이 부재했다. LLM이 비즈니스 핵심 워크플로에
들어오면서, 답변의 신뢰성을 입증하지 못하면 개선 자체가 의미를 잃는다.

So What?
- 수동 평가의 한계는 단순한 지연이 아니라 “신뢰할 수 없는 모델 개선 루프”를 만든다.
- 모델의 약점을 빠르게 찾지 못하면, 개선 시도는 사실상 도박과 다름없다.

---

## 4. Before / After: 가시적 변화

### Before
- 평가 결과가 팀마다 흩어져 있고, 누가 무엇을 바꿨는지 추적이 어렵다.
- 점수만 보고 “좋아졌다/나빠졌다” 판단한다.
- 재현이 어렵고, 비교 결과의 근거를 설명하기 힘들다.

### After
- `run_id` 하나로 평가/분석/리포트/아티팩트가 묶인다.
- 점수 + 근거(아티팩트)로 원인을 빠르게 좁힌다.
- 비교/회귀 판단이 자동화되어 사람이 매번 판단하지 않는다.

---

## 5. 전체 프로세스 지도: 업무 흐름과 도구 배치

아래는 발표 슬라이드에 넣을 “업무 흐름 한 장 요약”이다.

```
[데이터셋]
   ↓ (run)
[평가] → run_id 생성
   ↓ (auto-analyze)
[분석] → 리포트 + 아티팩트(index.json)
   ↓ (compare)
[비교] → 개선/회귀 판단
   ↓ (gate/regress)
[자동 차단 or 개선 실행]
```

툴의 역할(비전문가용 표현):
- EvalVault는 “결과를 모으고, 이유를 설명하고, 회귀를 막는 자동 파이프라인” 역할을 한다.
- 사람은 “결정”만 하고, 수집·정리·비교는 툴이 대신한다.

---

## 6. 구현 흐름 상세: 왜 이 단계가 필요한가

### 6.1 데이터 준비(Data Prep)
- 역할: 노이즈 데이터를 제거하고 평가 입력의 안정성을 확보한다.
- 이유: 입력 품질이 흔들리면 어떤 점수도 신뢰할 수 없기 때문이다.
- 구현 포인트: DatasetPreprocessor를 통해 공백 변종, N/A, TODO 등 노이즈를 정제한다.

### 6.2 평가 실행(Run)
- 역할: 데이터셋을 실행해 `run_id`를 생성하고, 메트릭을 계산한다.
- 이유: 모든 결과를 `run_id`로 묶어야 재현 가능하기 때문이다.
- 구현 포인트: 평가 결과는 `run_id`와 함께 저장된다.

### 6.3 분석(Analyze)
- 역할: 점수의 원인을 설명하는 리포트와 근거 아티팩트를 생성한다.
- 이유: 점수만으로는 “왜 낮은지”를 알 수 없기 때문이다.
- 구현 포인트: `reports/analysis/analysis_<RUN_ID>.md`와
  `reports/analysis/artifacts/analysis_<RUN_ID>/index.json`이 핵심 근거 파일이다.

### 6.4 비교(Compare)
- 역할: 변경 전/후의 차이를 자동으로 요약한다.
- 이유: 개선인지 회귀인지 빠르게 판단해야 하기 때문이다.

### 6.5 품질 게이트(Gate/Regress)
- 역할: CI/PR 단계에서 회귀를 자동 감지한다.
- 이유: 사람이 매번 판단하면 결국 실수한다.

---

## 7. 아키텍처 설계: 왜 헥사고날 구조가 필요한가

핵심 개념: 도메인(정책)과 어댑터(연결)를 분리한다.
- 도메인: 평가/분석/비교의 규칙
- 어댑터: CLI, API, 스토리지, LLM 공급자

역할 중심으로 보면:
- DatasetPreprocessor: “도메인 가드” 역할(노이즈 차단)
- Ports & Adapters: “벤더 종속성 방어막” 역할(LLM/DB 교체 가능)
- Metric Registry: “지능형 채점관” 역할(메트릭 일관성 유지)

---

## 8. 데이터와 메트릭 운영: 기준을 흔들리지 않게

핵심 메시지:
- 데이터셋, 메트릭, 임계값이 흔들리면 자동화가 의미 없어짐.
- gate로 쓰는 메트릭은 threshold 소유자를 명확히 해야 한다.

실무 포인트:
- signal_group으로 메트릭을 묶어 중복 측정을 줄인다.
- threshold profile로 목적별 추천 임계값을 일괄 적용한다.

---

## 9. 운영(Operations): 운영에서 깨지는 지점과 고정 규칙

운영 3대 불변식:
1) 프로필/시크릿 분리
2) DB 경로 고정
3) `run_id`로 교차 확인

자주 발생하는 문제:
- CLI와 UI가 서로 다른 DB를 봐서 결과가 안 보임
- 시크릿이 섞여 보안 사고 발생
- 트레이싱 옵션을 무조건 켜서 비용 증가

---

## 10. 보안(Security): 안전하게 쓰는 원칙

핵심 규칙:
- `.env`는 커밋 금지
- API 토큰이 설정되면 인증 강제
- MCP는 도구/파일 경로를 동시에 제한

왜 필요한가:
- 평가 데이터는 민감할 수 있고, 트레이스/아티팩트에 원문이 포함될 수 있기 때문이다.

---

## 11. 품질 & 테스트(Quality & Testing): 자동 게이트로 신뢰 확보

핵심 메시지:
- 사람 판단은 언젠가 실수한다.
- 회귀 게이트가 CI에서 자동으로 실패 처리하고 PR에 결과를 남겨야 한다.

툴의 역할:
- `regress`/`ci-gate`를 통해 결과를 표준 포맷으로 출력하고 CI가 바로 판단한다.

---

## 12. UX & Product: 비전문가도 이해하는 사용자 여정

핵심 메시지:
- UX 중심축은 `run_id`다.
- UI는 탐색/공유, CLI는 자동화/정밀 제어에 강하다.

사용자 여정(Primary Journeys):
- 여정 A: 실행 → 분석 → 리포트 → 근거 확인
- 여정 B: 비교 → 판단 → 개선
- 여정 C: 게이트 → PR 자동 차단

---

## 13. Roadmap: 우선순위와 성공 기준

우선순위:
- P0: 안정성/재현성
- P1: 회귀 게이트 자동화
- P2: 멀티턴 평가
- P3: GraphRAG 실험 프레임워크
- P4: UI 캘리브레이션

핵심 원칙:
- 재현성(P0) 없이는 이후 개선도 의미가 없다.
- 회귀 게이트(P1)가 있어야 사람 판단 비용을 줄인다.

---

## 14. Competitive Positioning: 왜 이 방식이 경쟁력인가

경쟁 범주 비교:
- Observability-first: 트레이스/로그 중심
- Eval-first: 메트릭/벤치마크 중심
- CI-first: PR 게이팅 중심

EvalVault의 차별점:
- `run_id` + DB + artifacts + gate를 하나의 루프로 연결
- “평가 → 분석 → 추적 → 개선”이 끊기지 않는다

---

## 15. Lessons Learned (동기부여)

- 점수만 보면 문제를 놓친다 → 근거(아티팩트)가 필요하다.
- 재현 가능한 조건이 없으면 개선도 없다.
- 자동 게이트가 없으면 결국 실수한다.

동기부여 한 문장:
- 처음에는 복잡해 보여도, 이 루프를 한 번 만들면 이후 개선 비용이 크게 줄어든다.

---

## 16. 발표 스크립트 작성 팁(실무용)

### 16.1 프레이밍 팁(비전문가 대상)
- 기술보다 “업무 흐름/결과”를 먼저 보여준다.
- Before → After → Bridge 구조로 고통, 해결, 변화의 순서를 유지한다.
- 용어는 비즈니스 언어로 번역한다(기술 명칭 대신 역할/효과 표현).
- 정량 지표(시간/비용/정확도)와 정성 효과(업무 집중도/번아웃 감소)를 함께 제시한다.
- 실패 대응/거버넌스를 간단히 설명해 신뢰를 만든다.

### 16.2 도표/다이어그램 팁
- 흐름 방향은 한쪽(좌→우 또는 상→하)으로 고정한다.
- 박스는 1개 작업만 담고, 라벨은 동사형(검토, 승인, 기록)으로 쓴다.
- 복잡한 도표는 5~7개 노드로 제한하고, 예외 경로는 최소화한다.
- 역할이 다른 경우 스윔레인으로 책임 구분을 보여준다.

---

## 17. 20분 발표 스크립트(버전)

> 진행 시간 20분 기준. 괄호는 권장 소요 시간.

### 0) 오프닝(1분)
안녕하세요. 오늘은 “EvalVault가 왜 필요했고, 어떻게 자동화를 만들었는지”를 비전문가 기준으로
20분 안에 이해하는 세션입니다. 핵심은 ‘점수’가 아니라 ‘근거’와 ‘재현’입니다.

### 1) 문제 정의: 평가의 늪(2분)
AI 모델을 바꾸면 “정말 좋아졌는지” 증명해야 합니다. 그런데 현실은 점수만 보고 판단합니다.
점수만 있으면 원인을 설명할 수 없고, 재현도 불가능합니다. 이것이 평가의 늪입니다.

### 2) 한 줄 요약(1분)
EvalVault는 결과를 `run_id`로 묶어 평가 → 분석 → 비교 → 개선을 자동화하고,
CI에서 회귀를 감지해 PR에 결과를 남깁니다.

### 3) Before / After(2분)
Before: 결과가 흩어져 있고, 누가 무엇을 바꿨는지 추적이 어렵습니다.
After: `run_id` 하나로 리포트와 아티팩트가 묶이며, 비교/회귀 판단이 자동화됩니다.

### 4) 3분 체험 시나리오(2분)
지금 바로 해볼 수 있는 최소 실행입니다.
실행 후 `reports/analysis/analysis_<RUN_ID>.md`와
`reports/analysis/artifacts/analysis_<RUN_ID>/index.json`을 확인합니다.
“왜 이런 점수인지”가 보이면 성공입니다.

### 5) 전체 프로세스 지도(2분)
데이터셋 → 평가(run_id) → 분석(리포트+아티팩트) → 비교 → 게이트/회귀 감지.
사람은 판단만 하고, 수집과 정리는 툴이 대신합니다.

### 6) 구현 흐름의 의미(3분)
데이터 준비: 노이즈 제거로 입력 흔들림 차단.
평가 실행: 모든 결과를 run_id로 묶어 재현 가능하게.
분석: 점수의 원인을 설명하는 근거 생성.
비교: 개선인지 회귀인지 빠르게 판단.
게이트: CI에서 자동 감지 및 실패 처리.

### 7) 왜 헥사고날 구조인가(2분)
도메인(정책)과 어댑터(연결)를 분리해서 교체 가능성을 확보합니다.
모델/DB/트레이싱이 바뀌어도 평가 로직은 흔들리지 않습니다.

### 8) 데이터와 메트릭 운영(2분)
signal_group으로 중복 평가를 피하고, threshold profile로 기준을 일괄 적용합니다.
중요한 건 “팀 기준”을 고정하는 것입니다.

### 9) 운영/보안 핵심(2분)
프로필·시크릿 분리, DB 경로 고정, run_id 교차 확인.
`.env`는 커밋 금지, 트레이스에는 민감 정보가 들어갈 수 있습니다.

### 10) 마무리(1분)
이 루프를 한 번 만들면 이후 개선 비용이 크게 줄어듭니다.
EvalVault는 “점수”가 아니라 “근거와 재현”을 자동화합니다.
