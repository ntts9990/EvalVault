# Web UI ì‹¤ì œ ë¶„ì„ ë° ê²°ê³¼ í™•ì¸ ê¸°ëŠ¥ êµ¬í˜„ ê³„íš

## ê°œìš”

EvalVault Web UIì—ì„œ ì‹¤ì œ RAG í‰ê°€ë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆë„ë¡ êµ¬í˜„í•˜ëŠ” ê³„íšì…ë‹ˆë‹¤.

## í˜„ì¬ ìƒíƒœ ë¶„ì„

### ë¬¸ì œì 

| êµ¬ë¶„ | í˜„ì¬ ìƒíƒœ | ìœ„ì¹˜ |
|------|----------|------|
| ì˜ì¡´ì„± ì£¼ì… | `create_adapter()`ê°€ ë¹ˆ `WebUIAdapter()` ë°˜í™˜ | `adapter.py:437-443` |
| í‰ê°€ ì‹¤í–‰ | "í‰ê°€ ì‹¤í–‰ ê¸°ëŠ¥ì€ ì•„ì§ êµ¬í˜„ ì¤‘ì…ë‹ˆë‹¤." ë©”ì‹œì§€ | `app.py:384-386` |
| ë¦¬í¬íŠ¸ ì ìˆ˜ | mock ë©”íŠ¸ë¦­ ì ìˆ˜(0.8) ì‚¬ìš© | `app.py` Reports í˜ì´ì§€ |

### ì •ìƒ ë™ì‘ ì¤‘

- Home, History, Improve í˜ì´ì§€ UI
- íŒŒì¼ ì—…ë¡œë“œ ê²€ì¦ (CSV/JSON/Excel)
- ë©”íŠ¸ë¦­ ì„ íƒ UI
- CLIë¥¼ í†µí•œ í‰ê°€ (`evalvault run`)

## êµ¬í˜„ ê³„íš

### Step 1: `create_adapter()` ì˜ì¡´ì„± ì£¼ì… êµ¬í˜„

**íŒŒì¼:** `src/evalvault/adapters/inbound/web/adapter.py`

**í˜„ì¬ ì½”ë“œ (line 437-443):**
```python
def create_adapter() -> WebUIAdapter:
    """WebUIAdapter ì¸ìŠ¤í„´ìŠ¤ ìƒì„± íŒ©í† ë¦¬."""
    # TODO: ì‹¤ì œ ì„¤ì •ì—ì„œ ì €ì¥ì†Œì™€ ì„œë¹„ìŠ¤ ë¡œë“œ
    return WebUIAdapter()
```

**ë³€ê²½ ë‚´ìš©:**
- Settings ë¡œë“œ (`get_settings()`)
- SQLiteStorageAdapter ìƒì„±
- LLM adapter ìƒì„± (API í‚¤ ì—†ìœ¼ë©´ graceful ì²˜ë¦¬)
- RagasEvaluator ìƒì„±
- ëª¨ë“  ì˜ì¡´ì„±ì„ WebUIAdapterì— ì£¼ì…

---

### Step 2: íŒŒì¼ ì—…ë¡œë“œ â†’ Dataset ë³€í™˜ ë©”ì„œë“œ ì¶”ê°€

**íŒŒì¼:** `src/evalvault/adapters/inbound/web/adapter.py`

**ìƒˆ ë©”ì„œë“œ:**
```python
def create_dataset_from_upload(
    self,
    filename: str,
    content: bytes,
) -> Dataset:
    """ì—…ë¡œë“œëœ íŒŒì¼ì—ì„œ Dataset ìƒì„±.

    Args:
        filename: ì›ë³¸ íŒŒì¼ëª… (í™•ì¥ìë¡œ í˜•ì‹ íŒë‹¨)
        content: íŒŒì¼ ë‚´ìš© (bytes)

    Returns:
        Dataset ì¸ìŠ¤í„´ìŠ¤
    """
```

**ì§€ì› í˜•ì‹:**
- JSON: ì§ì ‘ íŒŒì‹±í•˜ì—¬ Dataset ìƒì„±
- CSV: csv.DictReaderë¡œ íŒŒì‹±
- Excel: ì„ì‹œ íŒŒì¼ ì €ì¥ í›„ ê¸°ì¡´ loader ì‚¬ìš©

---

### Step 3: Datasetìœ¼ë¡œ ì§ì ‘ í‰ê°€í•˜ëŠ” ë©”ì„œë“œ ì¶”ê°€

**íŒŒì¼:** `src/evalvault/adapters/inbound/web/adapter.py`

**ìƒˆ ë©”ì„œë“œ:**
```python
def run_evaluation_with_dataset(
    self,
    dataset: Dataset,
    metrics: list[str],
    thresholds: dict[str, float] | None = None,
    on_progress: Callable[[EvalProgress], None] | None = None,
) -> EvaluationRun:
    """ë°ì´í„°ì…‹ ê°ì²´ë¡œ ì§ì ‘ í‰ê°€ ì‹¤í–‰.

    Args:
        dataset: í‰ê°€í•  ë°ì´í„°ì…‹
        metrics: í‰ê°€ ë©”íŠ¸ë¦­ ëª©ë¡
        thresholds: ë©”íŠ¸ë¦­ë³„ ì„ê³„ê°’ (ì„ íƒ)
        on_progress: ì§„í–‰ ìƒí™© ì½œë°± (ì„ íƒ)

    Returns:
        EvaluationRun ê²°ê³¼
    """
```

**ë™ì‘:**
1. evaluatorê°€ ì—†ìœ¼ë©´ RuntimeError ë°œìƒ
2. asyncio.run()ìœ¼ë¡œ evaluator.evaluate() í˜¸ì¶œ
3. ê²°ê³¼ë¥¼ storageì— ì €ì¥
4. EvaluationRun ë°˜í™˜

---

### Step 4: Evaluate í˜ì´ì§€ í‰ê°€ ì‹¤í–‰ êµ¬í˜„

**íŒŒì¼:** `src/evalvault/adapters/inbound/web/app.py`

**í˜„ì¬ ì½”ë“œ (line 384-386):**
```python
if st.button("ğŸš€ í‰ê°€ ì‹¤í–‰", type="primary", disabled=not can_run):
    st.info("í‰ê°€ ì‹¤í–‰ ê¸°ëŠ¥ì€ ì•„ì§ êµ¬í˜„ ì¤‘ì…ë‹ˆë‹¤.")
    # TODO: ì‹¤ì œ í‰ê°€ ì‹¤í–‰ ë¡œì§
```

**ë³€ê²½ ë‚´ìš©:**
1. LLM adapter ì„¤ì • í™•ì¸ (ì—†ìœ¼ë©´ ì—ëŸ¬ ë©”ì‹œì§€)
2. `adapter.create_dataset_from_upload()` í˜¸ì¶œ
3. ì„ íƒëœ ë©”íŠ¸ë¦­ê³¼ threshold ìˆ˜ì§‘
4. `adapter.run_evaluation_with_dataset()` í˜¸ì¶œ
5. ê²°ê³¼ í‘œì‹œ (í†µê³¼ìœ¨, í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìˆ˜, ì†Œìš” ì‹œê°„)
6. ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸

---

### Step 5: Reports í˜ì´ì§€ ì‹¤ì œ ë©”íŠ¸ë¦­ ì ìˆ˜ ì‚¬ìš©

**íŒŒì¼:** `src/evalvault/adapters/inbound/web/app.py`

**í˜„ì¬ ì½”ë“œ:**
```python
# ë©”íŠ¸ë¦­ ì ìˆ˜ (Mock - ì‹¤ì œë¡œëŠ” adapterì—ì„œ ì¡°íšŒ)
metrics = dict.fromkeys(selected_run.metrics_evaluated, 0.8)
```

**ë³€ê²½ ë‚´ìš©:**
```python
run_details = adapter.get_run_details(selected_run.run_id)
metrics = {
    m: run_details.get_avg_score(m) or 0.0
    for m in run_details.metrics_evaluated
}
```

---

## ìˆ˜ì • íŒŒì¼ ìš”ì•½

| íŒŒì¼ | ë³€ê²½ ë‚´ìš© |
|------|----------|
| `src/evalvault/adapters/inbound/web/adapter.py` | `create_adapter()` êµ¬í˜„, `create_dataset_from_upload()` ì¶”ê°€, `run_evaluation_with_dataset()` ì¶”ê°€ |
| `src/evalvault/adapters/inbound/web/app.py` | Evaluate í˜ì´ì§€ í‰ê°€ ì‹¤í–‰ ë¡œì§, Reports í˜ì´ì§€ ì‹¤ì œ ì ìˆ˜ ì¡°íšŒ |

## ì°¸ì¡° íŒŒì¼

| íŒŒì¼ | ì°¸ì¡° ë‚´ìš© |
|------|----------|
| `src/evalvault/adapters/inbound/cli.py` | CLI ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´ |
| `src/evalvault/domain/services/evaluator.py` | RagasEvaluator.evaluate() API |
| `src/evalvault/domain/entities/dataset.py` | Dataset, TestCase ì—”í‹°í‹° |
| `src/evalvault/domain/entities/result.py` | EvaluationRun ì—”í‹°í‹° |

## í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤

1. **Streamlit ì•± ì‹œì‘**
   ```bash
   uv run streamlit run src/evalvault/adapters/inbound/web/app.py
   ```

2. **Evaluate í˜ì´ì§€ í…ŒìŠ¤íŠ¸**
   - ìƒ˜í”Œ CSV/JSON íŒŒì¼ ì—…ë¡œë“œ
   - ë©”íŠ¸ë¦­ ì„ íƒ (faithfulness, answer_relevancy)
   - í‰ê°€ ì‹¤í–‰ ë²„íŠ¼ í´ë¦­
   - ê²°ê³¼ í™•ì¸ (í†µê³¼ìœ¨, í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìˆ˜)

3. **History í˜ì´ì§€ í…ŒìŠ¤íŠ¸**
   - ì €ì¥ëœ í‰ê°€ ê²°ê³¼ ì¡°íšŒ
   - ê²€ìƒ‰ ë° í•„í„° ê¸°ëŠ¥ í™•ì¸

4. **Reports í˜ì´ì§€ í…ŒìŠ¤íŠ¸**
   - ì‹¤ì œ ì ìˆ˜ë¡œ ë¦¬í¬íŠ¸ ìƒì„±
   - Markdown/HTML ë‹¤ìš´ë¡œë“œ

## ì˜ˆìƒ ê²°ê³¼

êµ¬í˜„ ì™„ë£Œ í›„:
- Web UIì—ì„œ íŒŒì¼ ì—…ë¡œë“œ â†’ ë©”íŠ¸ë¦­ ì„ íƒ â†’ í‰ê°€ ì‹¤í–‰ â†’ ê²°ê³¼ í™•ì¸ ì „ì²´ í”Œë¡œìš° ë™ì‘
- History í˜ì´ì§€ì—ì„œ ì´ì „ í‰ê°€ ê²°ê³¼ ì¡°íšŒ ê°€ëŠ¥
- Reports í˜ì´ì§€ì—ì„œ ì‹¤ì œ ì ìˆ˜ ê¸°ë°˜ ë¦¬í¬íŠ¸ ìƒì„± ê°€ëŠ¥
