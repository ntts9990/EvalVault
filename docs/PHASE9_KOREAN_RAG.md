# Phase 9: Korean RAG Optimization

> **Status**: Planning
> **Priority**: ğŸ”¥ High
> **Goal**: í•œêµ­ì–´ RAG ì‹œìŠ¤í…œ ì„±ëŠ¥ì„ ì‹¤ì§ˆì ìœ¼ë¡œ í–¥ìƒì‹œí‚¤ëŠ” ë„êµ¬ì™€ ê°€ì´ë“œ ì œê³µ

---

## ëª©í‘œ

1. **í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ í†µí•©**: Kiwi ê¸°ë°˜ í† í°í™”
2. **í•œêµ­ì–´ íŠ¹í™” í‚¤ì›Œë“œ ì¶”ì¶œ**: ì¡°ì‚¬/ì–´ë¯¸ ì œê±°, ì˜ë¯¸ ë‹¨ìœ„ ì¶”ì¶œ
3. **í•œêµ­ì–´ ê²€ìƒ‰ ìµœì í™”**: BM25 + í˜•íƒœì†Œ ë¶„ì„
4. **í•œêµ­ì–´ RAG í‰ê°€ ê°œì„ **: í•œêµ­ì–´ íŠ¹ì„± ë°˜ì˜ ë©”íŠ¸ë¦­
5. **ë²¤ì¹˜ë§ˆí¬ ë° ê°€ì´ë“œ**: ì„±ëŠ¥ ë¹„êµ ë°ì´í„° ë° ìµœì í™” ê°€ì´ë“œ

---

## ê¸°ìˆ  ìŠ¤íƒ ì„ ì •

### í˜•íƒœì†Œ ë¶„ì„ê¸° ë¹„êµ

| ë¶„ì„ê¸° | ì–¸ì–´ | ì„¤ì¹˜ ìš©ì´ì„± | ì„±ëŠ¥ | ì„ íƒ |
|--------|------|-------------|------|------|
| **Kiwi** | Pure Python | âœ… pip install | ë¹ ë¦„, ì •í™• | âœ… **ì„ íƒ** |
| Mecab-ko | C++ (Python wrapper) | âŒ ë³„ë„ ì„¤ì¹˜ í•„ìš” | ë§¤ìš° ë¹ ë¦„ | âŒ |
| Komoran | Java | âŒ JVM í•„ìš” | ë³´í†µ | âŒ |
| Okt (KoNLPy) | Java | âŒ JVM í•„ìš” | ë³´í†µ | âŒ |
| soynlp | Pure Python | âœ… pip install | ë¹„ì§€ë„í•™ìŠµ | ğŸ”„ ë³´ì¡° |

**ê²°ì •**: **Kiwi** (kiwipiepy)
- Pure Python, pip installë§Œìœ¼ë¡œ ì„¤ì¹˜
- ë¹ ë¥¸ ì†ë„ (100ë§Œ ë¬¸ì/ì´ˆ)
- ë†’ì€ ì •í™•ë„ (ì„¸ì¢… ì½”í¼ìŠ¤ ê¸°ì¤€ 97%+)
- ì‚¬ìš©ì ì‚¬ì „ ì§€ì›

---

### í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¹„êµ (2024-2025)

> ì°¸ê³ : [BGE-M3 Korean](https://huggingface.co/upskyy/bge-m3-korean), [dragonkue/BGE-m3-ko](https://huggingface.co/dragonkue/BGE-m3-ko)

| ëª¨ë¸ | ì°¨ì› | Max Tokens | íŠ¹ì§• | ì„ íƒ |
|------|------|------------|------|------|
| **upskyy/bge-m3-korean** | 1024 | 8192 | BGE-M3 í•œêµ­ì–´ íŒŒì¸íŠœë‹, Dense+Sparse+ColBERT | âœ… **1ìˆœìœ„** |
| **dragonkue/BGE-m3-ko** | 1024 | 8192 | 568M params, í•œêµ­ì–´ ë²¤ì¹˜ë§ˆí¬ ìš°ìˆ˜ | âœ… **2ìˆœìœ„** |
| BAAI/bge-m3 | 1024 | 8192 | 100+ ì–¸ì–´, Dense+Sparse+Multi-vec | ğŸ”„ Fallback |
| intfloat/multilingual-e5-large | 1024 | 512 | ë‹¤êµ­ì–´, ì•ˆì •ì  | ğŸ”„ ëŒ€ì•ˆ |
| jhgan/ko-sroberta-multitask | 768 | 512 | í•œêµ­ì–´ íŠ¹í™”, ì‘ì€ í¬ê¸° | ğŸ”„ ê²½ëŸ‰ |

**ê²°ì •**: **upskyy/bge-m3-korean** (1ìˆœìœ„)
- í•œêµ­ì–´ì— íŠ¹í™”ëœ íŒŒì¸íŠœë‹
- 8192 í† í° ì§€ì› (ê¸´ ë¬¸ì„œ ì²˜ë¦¬ ê°€ëŠ¥)
- Dense + Sparse + ColBERT 3ê°€ì§€ ê²€ìƒ‰ ëª¨ë“œ ì§€ì›
- ì˜ì–´-í•œêµ­ì–´ ìœ ì‚¬ë„ 0.78-0.94 ë‹¬ì„±

---

### SPLADE for Korean: íš¨ê³¼ ë¶„ì„

> ì°¸ê³ : [Naver SPLADE GitHub](https://github.com/naver/splade), [Korean SPLADE ì—°êµ¬](https://arxiv.org/html/2511.22263v1)

#### SPLADE í•œêµ­ì–´ ì ìš© ê°€ëŠ¥ì„±

| í•­ëª© | í‰ê°€ | ìƒì„¸ |
|------|------|------|
| **íš¨ê³¼** | âœ… ë†’ìŒ | 2.6ì–µ í•œêµ­ì–´ query-document í˜ì–´ë¡œ ê²€ì¦ë¨ |
| **BM25 ëŒ€ë¹„** | âœ… ìš°ìˆ˜ | ë³µì¡í•œ ì¿¼ë¦¬ì—ì„œ íŠ¹íˆ ì¢‹ì€ ì„±ëŠ¥ |
| **í•µì‹¬ ì¡°ê±´** | âš ï¸ ì–´íœ˜ ì„ íƒ | í•œêµ­ì–´ vocabì´ ì˜ ë§ëŠ” ëª¨ë¸ í•„ìˆ˜ |

#### í•œêµ­ì–´ SPLADE ê¶Œì¥ ë°±ë³¸

```
âœ… ê¶Œì¥ (í•œêµ­ì–´ vocab ìš°ìˆ˜):
- klue/roberta-base
- skt/A.X-Encoder-base
- monologg/koelectra-base-v3

âŒ ë¹„ê¶Œì¥ (vocab ë¶ˆì¼ì¹˜):
- jhu-clsp/mmBERT-base â†’ í•œêµ­ì–´ í‘œí˜„ ë¶•ê´´ (all-zero)
- ì˜ì–´ ì¤‘ì‹¬ ëª¨ë¸ â†’ í† í° ê³¼ë¶„ì ˆí™”
```

#### í•µì‹¬ ì¸ì‚¬ì´íŠ¸

> "Before asking 'Which backbone should I use?', ask 'Can this model's vocabulary properly express the language in my data?'"
> â€” [HuggingFace Blog: Vocabulary in Sparse Retrieval](https://huggingface.co/blog/yjoonjang/vocabulary-is-the-most-important-element-in-splade)

í•œêµ­ì–´ì—ì„œ ì–´íœ˜ê°€ ë§ì§€ ì•ŠëŠ” í† í¬ë‚˜ì´ì €ëŠ”:
- í† í° ê³¼ë¶„ì ˆí™” (ë³´í—˜ â†’ â–ë³´, ##í—˜)
- í¬ê·€ ì„œë¸Œì›Œë“œ ë§¤í•‘
- í¬ì†Œì„± ì••ë ¥ í•˜ì—ì„œ all-zero ì¶œë ¥ í•™ìŠµ

---

### í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì „ëµ (ê¶Œì¥)

BGE-M3 ëª¨ë¸ì€ **Dense + Sparse + ColBERT**ë¥¼ ë™ì‹œ ì§€ì›í•˜ë¯€ë¡œ, í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì´ ìµœì :

```python
from FlagEmbedding import BGEM3FlagModel

model = BGEM3FlagModel('upskyy/bge-m3-korean', use_fp16=True)

# Dense, Sparse, ColBERT ì„ë² ë”© ë™ì‹œ ìƒì„±
embeddings = model.encode(
    sentences,
    return_dense=True,
    return_sparse=True,
    return_colbert_vecs=True
)

# í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤ì½”ì–´ ê³„ì‚°
dense_score = dense_similarity(query_embed, doc_embed)
sparse_score = sparse_dot_product(query_sparse, doc_sparse)  # SPLADE ìŠ¤íƒ€ì¼
colbert_score = colbert_score(query_vecs, doc_vecs)

# ê°€ì¤‘ì¹˜ ì¡°í•© (Reciprocal Rank Fusion)
final_score = rrf(dense_rank, sparse_rank, colbert_rank)
```

#### ê²€ìƒ‰ ëª¨ë“œë³„ íŠ¹ì„±

| ëª¨ë“œ | ì¥ì  | ë‹¨ì  | ë³´í—˜ ë„ë©”ì¸ ì í•©ì„± |
|------|------|------|------------------|
| Dense | ì˜ë¯¸ ìœ ì‚¬ë„ | ì •í™•í•œ ìš©ì–´ ë§¤ì¹­ ì•½í•¨ | ğŸ”„ ë³´í†µ |
| Sparse (SPLADE) | ì •í™•í•œ ìš©ì–´ ë§¤ì¹­ | ì˜ë¯¸ í™•ì¥ í•œê³„ | âœ… ë†’ìŒ (ë³´í—˜ ìš©ì–´) |
| ColBERT | í† í° ë ˆë²¨ ë§¤ì¹­ | ê³„ì‚° ë¹„ìš© | âœ… ë†’ìŒ |
| **Hybrid** | ëª¨ë“  ì¥ì  í†µí•© | ë³µì¡ë„ | âœ…âœ… ìµœì  |

---

## êµ¬í˜„ ê³„íš

### Phase 9.1: Korean NLP Foundation (Week 1)

> **ëª©í‘œ**: Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° í†µí•© ë° ê¸°ë³¸ í•œêµ­ì–´ ì²˜ë¦¬ ì¸í”„ë¼

#### ìƒˆ íŒŒì¼ êµ¬ì¡°

```
src/evalvault/
â”œâ”€â”€ adapters/outbound/nlp/
â”‚   â””â”€â”€ korean/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ kiwi_tokenizer.py      # Kiwi ê¸°ë°˜ í† í¬ë‚˜ì´ì €
â”‚       â”œâ”€â”€ korean_stopwords.py    # í•œêµ­ì–´ ë¶ˆìš©ì–´ ì‚¬ì „
â”‚       â””â”€â”€ korean_utils.py        # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
â”œâ”€â”€ ports/outbound/
â”‚   â””â”€â”€ korean_nlp_port.py         # í•œêµ­ì–´ NLP í¬íŠ¸
```

#### KiwiTokenizer ì„¤ê³„

```python
from kiwipiepy import Kiwi

class KiwiTokenizer:
    """Kiwi ê¸°ë°˜ í•œêµ­ì–´ í† í¬ë‚˜ì´ì €.

    í˜•íƒœì†Œ ë¶„ì„ì„ í†µí•´ ì˜ë¯¸ìˆëŠ” í† í°ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """

    def __init__(
        self,
        remove_particles: bool = True,      # ì¡°ì‚¬ ì œê±°
        remove_endings: bool = True,        # ì–´ë¯¸ ì œê±°
        use_lemma: bool = True,             # ì›í˜• ì‚¬ìš©
        user_dict_path: str | None = None,  # ì‚¬ìš©ì ì‚¬ì „
    ):
        self.kiwi = Kiwi()
        self.remove_particles = remove_particles
        self.remove_endings = remove_endings
        self.use_lemma = use_lemma

        if user_dict_path:
            self._load_user_dict(user_dict_path)

    def tokenize(self, text: str) -> list[str]:
        """í…ìŠ¤íŠ¸ë¥¼ í˜•íƒœì†Œ ë¶„ì„í•˜ì—¬ í† í° ë¦¬ìŠ¤íŠ¸ ë°˜í™˜.

        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸

        Returns:
            í† í° ë¦¬ìŠ¤íŠ¸ (ë¶ˆìš©ì–´/ì¡°ì‚¬/ì–´ë¯¸ ì œê±°ë¨)
        """
        tokens = []
        for token in self.kiwi.tokenize(text):
            # ì¡°ì‚¬(J*), ì–´ë¯¸(E*), ê¸°í˜¸(S*) ì œì™¸
            if self.remove_particles and token.tag.startswith('J'):
                continue
            if self.remove_endings and token.tag.startswith('E'):
                continue
            if token.tag.startswith('S'):
                continue

            # ì›í˜• ì‚¬ìš© ë˜ëŠ” í‘œë©´í˜• ì‚¬ìš©
            form = token.lemma if self.use_lemma else token.form
            tokens.append(form)

        return tokens

    def extract_nouns(self, text: str) -> list[str]:
        """ëª…ì‚¬ë§Œ ì¶”ì¶œ."""
        nouns = []
        for token in self.kiwi.tokenize(text):
            if token.tag.startswith('N'):  # NNG, NNP, NNB, ...
                nouns.append(token.lemma)
        return nouns

    def extract_keywords(
        self,
        text: str,
        pos_tags: list[str] = ['NNG', 'NNP', 'VV', 'VA']
    ) -> list[str]:
        """í‚¤ì›Œë“œ í’ˆì‚¬ë§Œ ì¶”ì¶œ (ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬)."""
        keywords = []
        for token in self.kiwi.tokenize(text):
            if token.tag in pos_tags:
                keywords.append(token.lemma)
        return keywords
```

#### í•œêµ­ì–´ ë¶ˆìš©ì–´ ì‚¬ì „

```python
# korean_stopwords.py

KOREAN_STOPWORDS = {
    # ì¼ë°˜ ë¶ˆìš©ì–´
    'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë°', 'ë˜', 'ë•Œ', 'ë”', 'ì´', 'ê·¸', 'ì €',
    'ìˆë‹¤', 'í•˜ë‹¤', 'ë˜ë‹¤', 'ì•Šë‹¤', 'ì—†ë‹¤', 'ê°™ë‹¤',

    # ë³´í—˜ ë„ë©”ì¸ ë¶ˆìš©ì–´ (ë§¥ë½ì— ë”°ë¼ ì¡°ì •)
    'ê²½ìš°', 'í•´ë‹¹', 'ê´€ë ¨', 'ëŒ€í•œ', 'ìœ„í•œ', 'í†µí•´', 'ë”°ë¼',

    # ì ‘ì†ì‚¬/ë¶€ì‚¬
    'ê·¸ë¦¬ê³ ', 'ê·¸ëŸ¬ë‚˜', 'ë˜í•œ', 'ë”°ë¼ì„œ', 'ê·¸ë˜ì„œ', 'í•˜ì§€ë§Œ',
}

# í’ˆì‚¬ ê¸°ë°˜ ë¶ˆìš©ì–´ íƒœê·¸
STOPWORD_POS_TAGS = {
    'JKS', 'JKC', 'JKG', 'JKO', 'JKB', 'JKV', 'JKQ',  # ê²©ì¡°ì‚¬
    'JX', 'JC',  # ë³´ì¡°ì‚¬, ì ‘ì†ì¡°ì‚¬
    'EP', 'EF', 'EC', 'ETN', 'ETM',  # ì–´ë¯¸
    'SF', 'SP', 'SS', 'SE', 'SO',  # ê¸°í˜¸
}
```

#### í…ŒìŠ¤íŠ¸ ëª©í‘œ

- [ ] Kiwi ì„¤ì¹˜ ë° ê¸°ë³¸ ë™ì‘ í™•ì¸
- [ ] í† í°í™” ì •í™•ë„ í…ŒìŠ¤íŠ¸ (ë³´í—˜ ë„ë©”ì¸ í…ìŠ¤íŠ¸)
- [ ] ì‚¬ìš©ì ì‚¬ì „ ë¡œë“œ í…ŒìŠ¤íŠ¸
- [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ (ì²˜ë¦¬ ì†ë„)

---

### Phase 9.2: Korean Keyword Extraction (Week 1-2)

> **ëª©í‘œ**: í˜•íƒœì†Œ ë¶„ì„ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œë¡œ NLP ë¶„ì„ í’ˆì§ˆ í–¥ìƒ

#### NLPAnalysisAdapter ê°œì„ 

```python
# nlp_adapter.py ìˆ˜ì •

class NLPAnalysisAdapter:
    def __init__(
        self,
        llm: LLMPort | None = None,
        korean_tokenizer: KiwiTokenizer | None = None,  # ì¶”ê°€
    ):
        self.llm = llm
        self.korean_tokenizer = korean_tokenizer or KiwiTokenizer()

    def _extract_keywords_korean(
        self,
        texts: list[str],
        top_n: int = 20
    ) -> list[KeywordInfo]:
        """í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ."""
        # 1. í˜•íƒœì†Œ ë¶„ì„ìœ¼ë¡œ í† í° ì¶”ì¶œ
        all_tokens = []
        for text in texts:
            tokens = self.korean_tokenizer.extract_keywords(text)
            all_tokens.extend(tokens)

        # 2. TF-IDF ê³„ì‚° (í˜•íƒœì†Œ ê¸°ë°˜)
        from sklearn.feature_extraction.text import TfidfVectorizer

        # ê° ë¬¸ì„œë¥¼ í˜•íƒœì†Œ ë¶„ì„ í›„ ê³µë°± ì—°ê²°
        tokenized_docs = [
            ' '.join(self.korean_tokenizer.extract_keywords(text))
            for text in texts
        ]

        vectorizer = TfidfVectorizer(max_features=100)
        tfidf_matrix = vectorizer.fit_transform(tokenized_docs)

        # 3. í‚¤ì›Œë“œ ì ìˆ˜ ê³„ì‚°
        feature_names = vectorizer.get_feature_names_out()
        scores = tfidf_matrix.sum(axis=0).A1

        keywords = []
        for idx in scores.argsort()[::-1][:top_n]:
            keyword = feature_names[idx]
            tfidf_score = scores[idx]
            frequency = all_tokens.count(keyword)

            keywords.append(KeywordInfo(
                keyword=keyword,
                tfidf_score=tfidf_score,
                frequency=frequency,
            ))

        return keywords
```

#### ê°œì„  íš¨ê³¼ ì˜ˆì‹œ

```
Before (ê³µë°± ê¸°ë°˜):
  í‚¤ì›Œë“œ: ['ë³´í—˜ë£Œê°€', 'ì–¼ë§ˆì¸ê°€ìš”', 'ë¬´ì—‡ì¸ê°€ìš”', 'ê°€ëŠ¥í•©ë‹ˆë‹¤', 'ìˆìŠµë‹ˆë‹¤']

After (í˜•íƒœì†Œ ë¶„ì„):
  í‚¤ì›Œë“œ: ['ë³´í—˜ë£Œ', 'ë³´ì¥', 'ê°€ì…', 'ë³´í—˜', 'ë‚©ì…', 'ì‚¬ë§', 'ë§Œê¸°', 'ì—°ê¸ˆ']
```

---

### Phase 9.3: Korean Chunking & Retrieval (Week 2)

> **ëª©í‘œ**: ì˜ë¯¸ ë‹¨ìœ„ ì²­í‚¹ ë° í•œêµ­ì–´ ê²€ìƒ‰ ìµœì í™”

#### KoreanDocumentChunker

```python
class KoreanDocumentChunker:
    """í•œêµ­ì–´ íŠ¹í™” ë¬¸ì„œ ì²­í‚¹.

    í˜•íƒœì†Œ ë¶„ì„ì„ í™œìš©í•˜ì—¬ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ì²­í‚¹í•©ë‹ˆë‹¤.
    """

    def __init__(
        self,
        tokenizer: KiwiTokenizer,
        chunk_size: int = 500,       # í† í° ìˆ˜ ê¸°ì¤€
        overlap_tokens: int = 50,    # í† í° ì˜¤ë²„ë©
        split_by: str = 'sentence',  # sentence | paragraph
    ):
        self.tokenizer = tokenizer
        self.kiwi = tokenizer.kiwi
        self.chunk_size = chunk_size
        self.overlap_tokens = overlap_tokens
        self.split_by = split_by

    def _split_sentences(self, text: str) -> list[str]:
        """Kiwiì˜ ë¬¸ì¥ ë¶„ë¦¬ ì‚¬ìš©."""
        return [sent.text for sent in self.kiwi.split_into_sents(text)]

    def chunk(self, document: str) -> list[str]:
        """ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¬¸ì„œ ì²­í‚¹."""
        sentences = self._split_sentences(document)

        chunks = []
        current_chunk = []
        current_token_count = 0

        for sentence in sentences:
            sentence_tokens = len(self.tokenizer.tokenize(sentence))

            if current_token_count + sentence_tokens <= self.chunk_size:
                current_chunk.append(sentence)
                current_token_count += sentence_tokens
            else:
                # í˜„ì¬ ì²­í¬ ì €ì¥
                if current_chunk:
                    chunks.append(' '.join(current_chunk))

                # ì˜¤ë²„ë© ì²˜ë¦¬
                overlap_sents = self._get_overlap_sentences(
                    current_chunk, self.overlap_tokens
                )
                current_chunk = overlap_sents + [sentence]
                current_token_count = sum(
                    len(self.tokenizer.tokenize(s)) for s in current_chunk
                )

        if current_chunk:
            chunks.append(' '.join(current_chunk))

        return chunks
```

#### Korean BM25 Retriever

```python
class KoreanBM25Retriever:
    """í˜•íƒœì†Œ ë¶„ì„ ê¸°ë°˜ BM25 ê²€ìƒ‰.

    í•œêµ­ì–´ í…ìŠ¤íŠ¸ì— ìµœì í™”ëœ BM25 ê²€ìƒ‰ì„ ì œê³µí•©ë‹ˆë‹¤.
    """

    def __init__(self, tokenizer: KiwiTokenizer):
        self.tokenizer = tokenizer
        self.bm25 = None
        self.documents = []

    def index(self, documents: list[str]) -> None:
        """ë¬¸ì„œ ì¸ë±ì‹±."""
        from rank_bm25 import BM25Okapi

        self.documents = documents
        tokenized_docs = [
            self.tokenizer.tokenize(doc) for doc in documents
        ]
        self.bm25 = BM25Okapi(tokenized_docs)

    def search(self, query: str, top_k: int = 5) -> list[tuple[str, float]]:
        """ì¿¼ë¦¬ë¡œ ê²€ìƒ‰."""
        tokenized_query = self.tokenizer.tokenize(query)
        scores = self.bm25.get_scores(tokenized_query)

        # ìƒìœ„ kê°œ ë°˜í™˜
        top_indices = scores.argsort()[::-1][:top_k]
        return [
            (self.documents[i], scores[i])
            for i in top_indices
        ]
```

---

### Phase 9.4: Korean RAG Evaluation (Week 3)

> **ëª©í‘œ**: í•œêµ­ì–´ íŠ¹ì„±ì„ ë°˜ì˜í•œ í‰ê°€ ë©”íŠ¸ë¦­ ê°œì„ 

#### í•œêµ­ì–´ Faithfulness ê°œì„ 

```python
class KoreanFaithfulnessChecker:
    """í•œêµ­ì–´ Faithfulness ê²€ì¦.

    í•œêµ­ì–´ì˜ êµì°©ì–´ íŠ¹ì„±ì„ ê³ ë ¤í•˜ì—¬ faithfulnessë¥¼ ê²€ì¦í•©ë‹ˆë‹¤.
    - ì¡°ì‚¬ ë³€í˜• ë¬´ì‹œ (ë³´í—˜ë£Œê°€/ë³´í—˜ë£Œë¥¼/ë³´í—˜ë£ŒëŠ” â†’ ë³´í—˜ë£Œ)
    - ì–´ë¯¸ ë³€í˜• ë¬´ì‹œ (ì§€ê¸‰ë©ë‹ˆë‹¤/ì§€ê¸‰ë˜ë©°/ì§€ê¸‰í•˜ê³  â†’ ì§€ê¸‰)
    - ë™ì˜ì–´/ìœ ì˜ì–´ ì²˜ë¦¬
    """

    def __init__(self, tokenizer: KiwiTokenizer):
        self.tokenizer = tokenizer

    def extract_claims(self, text: str) -> list[str]:
        """ë‹µë³€ì—ì„œ ì£¼ì¥(claim) ì¶”ì¶œ.

        í˜•íƒœì†Œ ë¶„ì„ìœ¼ë¡œ í•µì‹¬ ëª…ì‚¬êµ¬/ë™ì‚¬êµ¬ ì¶”ì¶œ.
        """
        claims = []
        # Kiwiì˜ ëª…ì‚¬êµ¬ ì¶”ì¶œ í™œìš©
        for sent in self.tokenizer.kiwi.split_into_sents(text):
            nouns = self.tokenizer.extract_nouns(sent.text)
            if nouns:
                claims.append(' '.join(nouns))
        return claims

    def verify_against_context(
        self,
        claims: list[str],
        context: str
    ) -> list[tuple[str, bool, float]]:
        """ì»¨í…ìŠ¤íŠ¸ ëŒ€ë¹„ ì£¼ì¥ ê²€ì¦."""
        context_tokens = set(self.tokenizer.tokenize(context))

        results = []
        for claim in claims:
            claim_tokens = set(self.tokenizer.tokenize(claim))

            # í† í° ê²¹ì¹¨ ê³„ì‚°
            overlap = len(claim_tokens & context_tokens)
            coverage = overlap / len(claim_tokens) if claim_tokens else 0

            is_faithful = coverage >= 0.5  # 50% ì´ìƒ ê²¹ì¹¨
            results.append((claim, is_faithful, coverage))

        return results
```

#### í•œêµ­ì–´ Semantic Similarity ê°œì„ 

```python
class KoreanSemanticSimilarity:
    """í•œêµ­ì–´ ì˜ë¯¸ ìœ ì‚¬ë„ ê³„ì‚°.

    í˜•íƒœì†Œ ê¸°ë°˜ ì „ì²˜ë¦¬ + ì„ë² ë”©ìœ¼ë¡œ ìœ ì‚¬ë„ ê³„ì‚°.
    """

    def __init__(
        self,
        tokenizer: KiwiTokenizer,
        embedding_model: str = 'text-embedding-3-small'
    ):
        self.tokenizer = tokenizer
        self.embedding_model = embedding_model

    def preprocess(self, text: str) -> str:
        """í˜•íƒœì†Œ ë¶„ì„ìœ¼ë¡œ ì „ì²˜ë¦¬.

        ì¡°ì‚¬/ì–´ë¯¸ ì œê±°í•˜ì—¬ í•µì‹¬ ì˜ë¯¸ë§Œ ì¶”ì¶œ.
        """
        tokens = self.tokenizer.extract_keywords(text)
        return ' '.join(tokens)

    def calculate_similarity(
        self,
        text1: str,
        text2: str,
        use_preprocessing: bool = True
    ) -> float:
        """ë‘ í…ìŠ¤íŠ¸ì˜ ì˜ë¯¸ ìœ ì‚¬ë„ ê³„ì‚°."""
        if use_preprocessing:
            text1 = self.preprocess(text1)
            text2 = self.preprocess(text2)

        # ì„ë² ë”© ê³„ì‚° ë° ì½”ì‚¬ì¸ ìœ ì‚¬ë„
        # (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” LLMPort í™œìš©)
        ...
```

---

### Phase 9.5: Benchmarks & Guidelines (Week 3-4)

> **ëª©í‘œ**: í•œêµ­ì–´ RAG ìµœì í™” íš¨ê³¼ ì¸¡ì • ë° ê°€ì´ë“œ ë¬¸ì„œí™”

#### ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹

```
examples/benchmarks/korean_rag/
â”œâ”€â”€ insurance_qa_100.json     # ë³´í—˜ QA 100ê°œ
â”œâ”€â”€ retrieval_test.json       # ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
â”œâ”€â”€ chunking_test.json        # ì²­í‚¹ í’ˆì§ˆ í…ŒìŠ¤íŠ¸
â””â”€â”€ faithfulness_test.json    # Faithfulness í…ŒìŠ¤íŠ¸
```

#### ì„±ëŠ¥ ë¹„êµ ë©”íŠ¸ë¦­

| í•­ëª© | Before (ê³µë°± ê¸°ë°˜) | After (í˜•íƒœì†Œ ê¸°ë°˜) | ê°œì„ ìœ¨ |
|------|-------------------|-------------------|-------|
| í‚¤ì›Œë“œ ì •í™•ë„ | ì¸¡ì • ì˜ˆì • | ì¸¡ì • ì˜ˆì • | - |
| ê²€ìƒ‰ Recall@5 | ì¸¡ì • ì˜ˆì • | ì¸¡ì • ì˜ˆì • | - |
| Faithfulness | ì¸¡ì • ì˜ˆì • | ì¸¡ì • ì˜ˆì • | - |
| ì²˜ë¦¬ ì†ë„ | baseline | ì¸¡ì • ì˜ˆì • | - |

#### ê°€ì´ë“œ ë¬¸ì„œ

```markdown
# í•œêµ­ì–´ RAG ìµœì í™” ê°€ì´ë“œ

## 1. í˜•íƒœì†Œ ë¶„ì„ í™œìš©
- Kiwi í† í¬ë‚˜ì´ì € ì„¤ì • ë°©ë²•
- ì‚¬ìš©ì ì‚¬ì „ ì¶”ê°€ (ë³´í—˜ ìš©ì–´)

## 2. ì²­í‚¹ ì „ëµ
- í† í° ê¸°ë°˜ vs ë¬¸ì ê¸°ë°˜
- ì˜¤ë²„ë© ì„¤ì • ê¶Œì¥ê°’

## 3. ê²€ìƒ‰ ìµœì í™”
- BM25 vs Dense Retrieval
- í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì„¤ì •

## 4. í‰ê°€ ì‹œ ì£¼ì˜ì‚¬í•­
- ì¡°ì‚¬/ì–´ë¯¸ ë³€í˜• ì²˜ë¦¬
- ë™ì˜ì–´ ì²˜ë¦¬
```

---

## ì˜ì¡´ì„± ì¶”ê°€

```toml
# pyproject.toml
[project.optional-dependencies]
korean = [
    # í˜•íƒœì†Œ ë¶„ì„
    "kiwipiepy>=0.18.0",              # í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ (Pure Python)

    # ì„ë² ë”© & ê²€ìƒ‰
    "FlagEmbedding>=1.2.0",           # BGE-M3 ëª¨ë¸ (Dense+Sparse+ColBERT)
    "rank-bm25>=0.2.2",               # BM25 ê²€ìƒ‰

    # Hugging Face
    "transformers>=4.40.0",           # ëª¨ë¸ ë¡œë”©
    "sentence-transformers>=2.7.0",   # ì„ë² ë”© ìœ í‹¸ë¦¬í‹°
]

korean-full = [
    # korean + ì¶”ê°€ ëª¨ë¸
    "evalvault[korean]",
    "torch>=2.0.0",                   # GPU ê°€ì†
    "faiss-cpu>=1.7.4",               # ë²¡í„° ê²€ìƒ‰ (CPU)
    # "faiss-gpu>=1.7.4",             # GPU ë²„ì „ (ì„ íƒ)
]
```

### ì„¤ì¹˜ ë°©ë²•

```bash
# ê¸°ë³¸ í•œêµ­ì–´ ì§€ì›
uv add evalvault[korean]

# ì „ì²´ í•œêµ­ì–´ ê¸°ëŠ¥ (GPU í¬í•¨)
uv add evalvault[korean-full]

# ë˜ëŠ” ê°œë³„ ì„¤ì¹˜
uv add kiwipiepy FlagEmbedding rank-bm25
```

### ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

```python
# ì²« ì‚¬ìš© ì‹œ ìë™ ë‹¤ìš´ë¡œë“œ (~2GB)
from FlagEmbedding import BGEM3FlagModel

model = BGEM3FlagModel(
    'upskyy/bge-m3-korean',
    use_fp16=True,  # GPU ë©”ëª¨ë¦¬ ì ˆì•½
    device='cuda'   # ë˜ëŠ” 'cpu'
)
```

---

## íƒ€ì„ë¼ì¸

| Week | Phase | ì£¼ìš” ì‘ì—… |
|------|-------|----------|
| 1 | 9.1 | Kiwi í†µí•©, KiwiTokenizer êµ¬í˜„ |
| 1-2 | 9.2 | í‚¤ì›Œë“œ ì¶”ì¶œ ê°œì„ , NLP ì–´ëŒ‘í„° ìˆ˜ì • |
| 2 | 9.3 | í•œêµ­ì–´ ì²­í‚¹, BM25 ê²€ìƒ‰ |
| 3 | 9.4 | í•œêµ­ì–´ í‰ê°€ ë©”íŠ¸ë¦­ |
| 3-4 | 9.5 | ë²¤ì¹˜ë§ˆí¬, ê°€ì´ë“œ ë¬¸ì„œí™” |

---

## ì„±ê³µ ì§€í‘œ

| ì§€í‘œ | í˜„ì¬ | ëª©í‘œ |
|------|------|------|
| í‚¤ì›Œë“œ ì¶”ì¶œ ì •í™•ë„ | ~60% (ì¶”ì •) | 85%+ |
| ê²€ìƒ‰ Recall@5 | ì¸¡ì • í•„ìš” | +15% ê°œì„  |
| Faithfulness ì •í™•ë„ | ì¸¡ì • í•„ìš” | +10% ê°œì„  |
| ì‚¬ìš©ì ì„¤ì • ìš©ì´ì„± | ìˆ˜ë™ | CLI ìë™í™” |

---

## ì°¸ê³  ìë£Œ

- [Kiwi ê³µì‹ ë¬¸ì„œ](https://github.com/bab2min/kiwipiepy)
- [í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸° ë¹„êµ](https://konlpy.org/ko/latest/morph/)
- [BM25 ì•Œê³ ë¦¬ì¦˜](https://en.wikipedia.org/wiki/Okapi_BM25)
