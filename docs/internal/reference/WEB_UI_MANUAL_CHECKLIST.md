# Web UI 수동 검증 체크리스트 (Analysis Lab 중심)

> 작성일: 2026-01-09
> 상태: v0.1 (초안)
> 목적: 실제 API/프론트 환경에서 실행·결과 표출을 수동으로 확인한다.

## 1) 준비

- `.env` 및 `config/models.yaml` 세팅 완료
- 데이터/런 정보가 있는 로컬 DB 또는 샘플 데이터 준비

## 2) 실행 환경

```bash
uv run evalvault serve-api --reload
```

```bash
cd frontend && npm run dev
```

## 3) 공통 사전 점검

- API 응답 정상 여부: 브라우저 개발자도구 네트워크 탭 확인
- Web UI 라우팅 정상 동작 여부
- 새로고침/라우팅 시 에러 없는지 확인

## 4) Analysis Lab 수동 체크리스트

### 4.1 실행 전

- [ ] `분석 실험실` 진입 시 API 연결 배지 표시
- [ ] Intent 목록/Run 목록/저장된 결과 리스트 표시
- [ ] Run 미선택 시 “샘플 데이터” 안내 문구 표시

### 4.2 실행 요청

- [ ] Intent 클릭 시 분석 실행 시작, 로딩 상태 표시
- [ ] Run 선택/미선택에 따라 요청 파라미터 차이 확인
- [ ] `LLM 보고서 사용` 토글 반영 확인
- [ ] 벤치마크 옵션 입력 시 파라미터 전송 확인

### 4.3 실행 결과 표출

- [ ] 결과 카드에 분석 유형/처리 시간/상태/보고서 상태 표시
- [ ] 보고서(결과 출력) 영역에 요약/리포트 표시
- [ ] 노드 상세 출력에서 상태/오류/출력이 분리 표시
- [ ] 저장 메타데이터 입력/저장 성공 확인

### 4.4 에러/대체 처리

- [ ] API 실패 시 “API 연결 실패” 배지/메시지 표시
- [ ] LLM 오류 시 “대체 보고서” 배너 표시
- [ ] 일부 노드 실패 시 경고 배너 표시

## 5) 기록/증적

- 요청/응답 JSON 스냅샷 저장
- 문제 발생 시 스크린샷 + Run ID/Result ID 기록
- 재현 가능하도록 날짜/버전/브랜치 정보 기록
