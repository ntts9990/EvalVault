# EvalVault ë‹¬ì„± ê¸°ë¡

> Last Updated: 2026-01-01
> Current Version: 1.5.0
> Total Tests: 1,352 passing (89% coverage)

---

## ëª©ì°¨

1. [ê°œìš”](#ê°œìš”)
2. [Phase 1-3: Core System](#phase-1-3-core-system)
3. [Phase 4: Foundation Enhancement](#phase-4-foundation-enhancement)
4. [Phase 5: Storage & Domain](#phase-5-storage--domain)
5. [Phase 6: Advanced Features](#phase-6-advanced-features)
6. [Phase 7: Production Ready](#phase-7-production-ready)
7. [Phase 2 NLP: NLP Analysis](#phase-2-nlp-nlp-analysis)
8. [Phase 3 Causal: Causal Analysis](#phase-3-causal-causal-analysis)
9. [Phase 8: Domain Memory Layering](#phase-8-domain-memory-layering)
10. [Phase 9: Korean RAG Optimization](#phase-9-korean-rag-optimization)
11. [Phase 10-13: Streamlit Web UI](#phase-10-13-streamlit-web-ui)
12. [Phase 14: Query-Based DAG Analysis Pipeline](#phase-14-query-based-dag-analysis-pipeline)
13. [ì•„í‚¤í…ì²˜ í˜„í™©](#ì•„í‚¤í…ì²˜-í˜„í™©)
14. [í…ŒìŠ¤íŠ¸ í˜„í™©](#í…ŒìŠ¤íŠ¸-í˜„í™©)
15. [CI/CD & Release](#cicd--release)

---

## ê°œìš”

EvalVaultëŠ” RAG (Retrieval-Augmented Generation) í‰ê°€ ì‹œìŠ¤í…œìœ¼ë¡œ, Phase 1ë¶€í„° Phase 14ê¹Œì§€ ëª¨ë“  í•µì‹¬ ê¸°ëŠ¥ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤. ì´ 1,352ê°œì˜ í…ŒìŠ¤íŠ¸ê°€ í†µê³¼í•˜ê³  ìˆìœ¼ë©°, 89%ì˜ ì½”ë“œ ì»¤ë²„ë¦¬ì§€ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

### ë‹¬ì„± í˜„í™© ìš”ì•½

| Phase | Description | Status | Tests | Duration |
|-------|-------------|--------|-------|----------|
| Phase 1-3 | Core System | âœ… Complete | 118 | 2 weeks |
| Phase 4 | Foundation Enhancement | âœ… Complete | +60 | 1 week |
| Phase 5 | Storage & Domain | âœ… Complete | +42 | 1 week |
| Phase 6 | Advanced Features | âœ… Complete | +160 | 2 weeks |
| Phase 7 | Production Ready | âœ… Complete | +10 | 1 week |
| Phase 2 NLP | NLP Analysis | âœ… Complete | +97 | 2 weeks |
| Phase 3 Causal | Causal Analysis | âœ… Complete | +27 | 1 week |
| Phase 8 | Domain Memory Layering | âœ… Complete | +113 | 3 weeks |
| Phase 9 | Korean RAG Optimization | âœ… Complete | +24 | 2 weeks |
| Phase 10-13 | Streamlit Web UI | âœ… Complete | +138 | 3 weeks |
| Phase 14 | Query-Based DAG Pipeline | âœ… Complete | +153 | 3 weeks |
| **Total** | | **âœ… 100%** | **1,352** | **21 weeks** |

### í•µì‹¬ ì„±ê³¼

- âœ… **Hexagonal Architecture**: Port/Adapter íŒ¨í„´ìœ¼ë¡œ í™•ì¥ ê°€ëŠ¥í•œ êµ¬ì¡°
- âœ… **Multi-LLM Support**: OpenAI, Azure OpenAI, Anthropic, Ollama
- âœ… **Multi-DB Support**: SQLite, PostgreSQL
- âœ… **Multi-Tracker Support**: Langfuse, MLflow
- âœ… **Korean NLP**: í˜•íƒœì†Œ ë¶„ì„, BM25, Dense, Hybrid Retrieval
- âœ… **Web UI**: Streamlit ê¸°ë°˜ ëŒ€ì‹œë³´ë“œ
- âœ… **Analysis Pipeline**: DAG ê¸°ë°˜ ìë™ ë¶„ì„
- âœ… **89% Test Coverage**: 1,352 tests passing
- âœ… **CI/CD**: Cross-platform (Ubuntu, macOS, Windows)
- âœ… **PyPI Published**: `pip install evalvault`

---

## Phase 1-3: Core System

> **Completed**: 2024-12-24
> **Tests**: 118
> **Description**: RAG í‰ê°€ë¥¼ ìœ„í•œ í•µì‹¬ ì‹œìŠ¤í…œ êµ¬ì¶•

### ë‹¬ì„± ë‚´ìš©

#### Domain Entities

```python
# src/evalvault/domain/entities/
â”œâ”€â”€ test_case.py      # TestCase ì—”í‹°í‹°
â”œâ”€â”€ dataset.py        # Dataset ì—”í‹°í‹°
â”œâ”€â”€ evaluation.py     # EvaluationRun, MetricScore
â””â”€â”€ experiment.py     # Experiment ì—”í‹°í‹°
```

**ì£¼ìš” ì—”í‹°í‹°**:
- `TestCase`: ì§ˆë¬¸, ë‹µë³€, ì»¨í…ìŠ¤íŠ¸, ground_truth
- `Dataset`: í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì§‘í•© + ë©”íƒ€ë°ì´í„°
- `EvaluationRun`: í‰ê°€ ì‹¤í–‰ ê²°ê³¼
- `MetricScore`: ë©”íŠ¸ë¦­ë³„ ì ìˆ˜ ë° í†µê³¼/ì‹¤íŒ¨ ì—¬ë¶€

#### Port Interfaces

```python
# src/evalvault/ports/
â”œâ”€â”€ inbound/
â”‚   â””â”€â”€ evaluator_port.py    # EvaluatorPort
â””â”€â”€ outbound/
    â”œâ”€â”€ llm_port.py          # LLMPort
    â”œâ”€â”€ dataset_port.py      # DatasetPort
    â”œâ”€â”€ storage_port.py      # StoragePort
    â””â”€â”€ tracker_port.py      # TrackerPort
```

**í¬íŠ¸ ì •ì˜**:
- `LLMPort`: LLM í˜¸ì¶œ ì¸í„°í˜ì´ìŠ¤
- `DatasetPort`: ë°ì´í„°ì…‹ ë¡œë”© ì¸í„°í˜ì´ìŠ¤
- `StoragePort`: ê²°ê³¼ ì €ì¥ ì¸í„°í˜ì´ìŠ¤
- `TrackerPort`: í‰ê°€ ì¶”ì  ì¸í„°í˜ì´ìŠ¤

#### Data Loaders

```python
# src/evalvault/adapters/outbound/dataset/
â”œâ”€â”€ csv_loader.py      # CSV ë¡œë”
â”œâ”€â”€ excel_loader.py    # Excel ë¡œë”
â””â”€â”€ json_loader.py     # JSON ë¡œë”
```

**ì§€ì› í¬ë§·**:
- CSV: ê°„ë‹¨í•œ í…Œì´ë¸” í˜•ì‹
- Excel: `.xlsx` íŒŒì¼ ì§€ì›
- JSON: êµ¬ì¡°í™”ëœ ë°ì´í„°

#### Ragas Evaluator

```python
# src/evalvault/domain/services/ragas_evaluator.py
class RagasEvaluator:
    """Ragas ê¸°ë°˜ í‰ê°€ ì„œë¹„ìŠ¤"""

    async def evaluate(
        self,
        dataset: Dataset,
        metrics: list[str],
    ) -> EvaluationRun:
        """ë¹„ë™ê¸° í‰ê°€ ì‹¤í–‰"""
        ...
```

**ì§€ì› ë©”íŠ¸ë¦­** (Phase 1-3):
- `faithfulness`: ë‹µë³€ì˜ ì»¨í…ìŠ¤íŠ¸ ì¶©ì‹¤ë„
- `answer_relevancy`: ë‹µë³€ì˜ ì§ˆë¬¸ ê´€ë ¨ì„±
- `context_precision`: ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ì •ë°€ë„
- `context_recall`: í•„ìš” ì •ë³´ ê²€ìƒ‰ ì™„ì „ì„±

#### LLM Adapters

```python
# src/evalvault/adapters/outbound/llm/
â””â”€â”€ openai_adapter.py    # OpenAI ì–´ëŒ‘í„°
```

**OpenAI Adapter**:
- LangChain í†µí•©
- í† í° ì‚¬ìš©ëŸ‰ ì¶”ì 
- ì—ëŸ¬ í•¸ë“¤ë§

#### Langfuse Tracker

```python
# src/evalvault/adapters/outbound/tracker/
â””â”€â”€ langfuse_adapter.py    # Langfuse ì–´ëŒ‘í„°
```

**Langfuse Integration**:
- í‰ê°€ trace ë¡œê¹…
- ë©”íŠ¸ë¦­ ì ìˆ˜ ê¸°ë¡
- SDK v3 ì§€ì›

#### CLI Interface

```bash
# í•µì‹¬ ëª…ë ¹ì–´
evalvault run <dataset> --metrics <metrics>
evalvault metrics
evalvault config
```

**CLI ê¸°ëŠ¥**:
- í‰ê°€ ì‹¤í–‰
- ì§€ì› ë©”íŠ¸ë¦­ ì¡°íšŒ
- ì„¤ì • í™•ì¸

---

## Phase 4: Foundation Enhancement

> **Completed**: 2024-12-24
> **Tests**: +60
> **Description**: ì¶”ê°€ ë©”íŠ¸ë¦­ ë° LLM ì–´ëŒ‘í„° í™•ì¥

### ë‹¬ì„± ë‚´ìš©

#### ìƒˆ ë©”íŠ¸ë¦­

**factual_correctness** (Ragas):
- ground_truth ëŒ€ë¹„ ì‚¬ì‹¤ì  ì •í™•ì„± í‰ê°€
- F1 Score ê¸°ë°˜
- ì—”í‹°í‹°/ê´€ê³„ ì¶”ì¶œ ë° ë§¤ì¹­

**semantic_similarity** (Ragas):
- ë‹µë³€ê³¼ ground_truth ê°„ ì˜ë¯¸ì  ìœ ì‚¬ë„
- ì„ë² ë”© ê¸°ë°˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„
- 0.0 ~ 1.0 ì ìˆ˜

#### ì¶”ê°€ LLM Adapters

**Azure OpenAI Adapter**:
```python
# src/evalvault/adapters/outbound/llm/azure_adapter.py
class AzureOpenAIAdapter:
    def __init__(
        self,
        api_key: str,
        endpoint: str,
        deployment_name: str,
        api_version: str,
    ):
        ...
```

**Anthropic Claude Adapter**:
```python
# src/evalvault/adapters/outbound/llm/anthropic_adapter.py
class AnthropicAdapter:
    def __init__(self, api_key: str):
        self.client = Anthropic(api_key=api_key)
        # OpenAI embeddings fallback
        ...
```

**Ollama Adapter**:
```python
# src/evalvault/adapters/outbound/llm/ollama_adapter.py
class OllamaAdapter:
    """ë¡œì»¬ LLM ì§€ì› (Ollama)"""
    def __init__(self, base_url: str, model: str):
        ...
```

### ì„¤ì • í™•ì¥

```python
# src/evalvault/config/settings.py
class Settings(BaseSettings):
    # Azure OpenAI
    azure_openai_api_key: str | None = None
    azure_openai_endpoint: str | None = None
    azure_openai_deployment: str | None = None

    # Anthropic
    anthropic_api_key: str | None = None

    # Ollama
    ollama_base_url: str = "http://localhost:11434"
    ollama_model: str = "llama2"
```

---

## Phase 5: Storage & Domain

> **Completed**: 2024-12-24
> **Tests**: +42
> **Description**: ê²°ê³¼ ì €ì¥ì†Œ ë° ë„ë©”ì¸ íŠ¹í™” ê¸°ëŠ¥

### ë‹¬ì„± ë‚´ìš©

#### SQLite Storage Adapter

```python
# src/evalvault/adapters/outbound/storage/sqlite_adapter.py
class SQLiteStorageAdapter:
    def save_run(self, run: EvaluationRun) -> None:
        """í‰ê°€ ê²°ê³¼ ì €ì¥"""
        ...

    def get_run(self, run_id: str) -> EvaluationRun:
        """ë‹¨ì¼ ê²°ê³¼ ì¡°íšŒ"""
        ...

    def list_runs(
        self,
        limit: int = 10,
        dataset_name: str | None = None,
        model_name: str | None = None,
    ) -> list[EvaluationRun]:
        """í•„í„°ë§ëœ ëª©ë¡ ì¡°íšŒ"""
        ...

    def delete_run(self, run_id: str) -> bool:
        """ê²°ê³¼ ì‚­ì œ"""
        ...
```

**ìŠ¤í‚¤ë§ˆ** (`src/evalvault/adapters/outbound/storage/schema.sql`):
- `evaluation_runs` í…Œì´ë¸”
- `test_case_results` í…Œì´ë¸”
- `metric_scores` í…Œì´ë¸”

#### History CLI Commands

```bash
# íˆìŠ¤í† ë¦¬ ì¡°íšŒ
evalvault history --limit 20

# ë‘ í‰ê°€ ë¹„êµ
evalvault compare <run_id1> <run_id2>

# ê²°ê³¼ ë‚´ë³´ë‚´ê¸°
evalvault export <run_id> -o result.json
```

**ì£¼ìš” ê¸°ëŠ¥**:
- í‰ê°€ íˆìŠ¤í† ë¦¬ ëª©ë¡
- ë‚ ì§œ/ë°ì´í„°ì…‹/ëª¨ë¸ë³„ í•„í„°ë§
- ë‘ í‰ê°€ ê²°ê³¼ side-by-side ë¹„êµ
- JSON í˜•ì‹ ë‚´ë³´ë‚´ê¸°

#### Insurance Term Accuracy Metric

```python
# src/evalvault/domain/metrics/insurance.py
class InsuranceTermAccuracyMetric:
    """ë³´í—˜ ë„ë©”ì¸ íŠ¹í™” ìš©ì–´ ì •í™•ë„ ë©”íŠ¸ë¦­"""

    def __init__(self, terms_dict: dict[str, list[str]]):
        self.terms_dict = terms_dict

    def score(
        self,
        answer: str,
        ground_truth: str,
    ) -> float:
        """ìš©ì–´ ë§¤ì¹­ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°"""
        ...
```

**ìš©ì–´ ì‚¬ì „** (`config/domains/insurance/terms_dictionary.json`):
```json
{
  "ë³´í—˜ê¸ˆ": ["insurance_payment", "claim"],
  "í”¼ë³´í—˜ì": ["insured", "policyholder"],
  "ë³´í—˜ë£Œ": ["premium", "insurance_fee"]
}
```

#### Testset Generation

```python
# src/evalvault/domain/services/testset_generator.py
class BasicTestsetGenerator:
    """LLM ì—†ì´ ê¸°ë³¸ í…ŒìŠ¤íŠ¸ì…‹ ìƒì„±"""

    def generate(
        self,
        documents: list[str],
        num_questions: int,
        question_type: str = "factual",
    ) -> Dataset:
        """ë¬¸ì„œ ê¸°ë°˜ í…ŒìŠ¤íŠ¸ì…‹ ìƒì„±"""
        ...
```

**DocumentChunker**:
- ë¬¸ì„œ ì²­í‚¹ ìœ í‹¸ë¦¬í‹°
- ê³ ì • í¬ê¸° ë˜ëŠ” ì˜ë¯¸ ë‹¨ìœ„ ì²­í‚¹
- ì˜¤ë²„ë© ì§€ì›

---

## Phase 6: Advanced Features

> **Completed**: 2025-12-24
> **Tests**: +160
> **Description**: ê³ ê¸‰ ê¸°ëŠ¥ (KG ìƒì„±, ì‹¤í—˜ ê´€ë¦¬, ì¶”ê°€ DB/Tracker)

### ë‹¬ì„± ë‚´ìš©

#### Knowledge Graph Testset Generation

```python
# src/evalvault/domain/services/kg_generator.py
class KnowledgeGraphGenerator:
    """ì§€ì‹ ê·¸ë˜í”„ ê¸°ë°˜ í…ŒìŠ¤íŠ¸ì…‹ ìƒì„±"""

    def build_graph(
        self,
        documents: list[str],
    ) -> KnowledgeGraph:
        """ë¬¸ì„œì—ì„œ KG ìƒì„±"""
        ...

    def generate_questions(
        self,
        graph: KnowledgeGraph,
        num_questions: int,
    ) -> list[TestCase]:
        """KG ê¸°ë°˜ ì§ˆë¬¸ ìƒì„±"""
        ...
```

**Entity Extractor** (`src/evalvault/domain/services/entity_extractor.py`):
- ë³´í—˜ ë„ë©”ì¸ ì—”í‹°í‹° ì¶”ì¶œ (íšŒì‚¬, ìƒí’ˆ, ê¸ˆì•¡, ê¸°ê°„, ë³´ì¥)
- ê´€ê³„ ì¶”ì¶œ (PROVIDES, COVERS, HAS_AMOUNT ë“±)
- LLM ê¸°ë°˜ ì¶”ì¶œ

**Knowledge Graph**:
- NetworkX ê¸°ë°˜ ê·¸ë˜í”„ êµ¬ì¡°
- ë…¸ë“œ: Entity (íƒ€ì…, ì†ì„±)
- ì—£ì§€: Relation (íƒ€ì…, ì†ì„±)
- Multi-hop ì§ˆë¬¸ ìƒì„± ì§€ì›

#### Experiment Management

```python
# src/evalvault/domain/services/experiment_manager.py
class ExperimentManager:
    """A/B í…ŒìŠ¤íŠ¸ ë° ì‹¤í—˜ ê´€ë¦¬"""

    def create_experiment(
        self,
        name: str,
        description: str,
    ) -> Experiment:
        """ì‹¤í—˜ ìƒì„±"""
        ...

    def add_group(
        self,
        experiment_id: str,
        group_name: str,
        run_id: str,
    ) -> ExperimentGroup:
        """ì‹¤í—˜ ê·¸ë£¹ ì¶”ê°€"""
        ...

    def compare_groups(
        self,
        experiment_id: str,
    ) -> dict:
        """ê·¸ë£¹ ê°„ í†µê³„ì  ë¹„êµ"""
        ...
```

**Experiment Entities**:
- `Experiment`: ì‹¤í—˜ ë©”íƒ€ë°ì´í„°
- `ExperimentGroup`: A/B ê·¸ë£¹
- `ExperimentResult`: ë¹„êµ ê²°ê³¼

**í†µê³„ ë¶„ì„**:
- ë©”íŠ¸ë¦­ë³„ í‰ê· /í‘œì¤€í¸ì°¨/ì¤‘ì•™ê°’
- ê·¸ë£¹ ê°„ ìœ ì˜ì„± ê²€ì • (t-test)
- Effect size ê³„ì‚°

#### PostgreSQL Storage Adapter

```python
# src/evalvault/adapters/outbound/storage/postgres_adapter.py
class PostgreSQLStorageAdapter:
    """asyncpg ê¸°ë°˜ ë¹„ë™ê¸° PostgreSQL ì–´ëŒ‘í„°"""

    def __init__(self, connection_string: str):
        self.conn_string = connection_string
        self.pool = None

    async def save_run(self, run: EvaluationRun) -> None:
        """ë¹„ë™ê¸° ì €ì¥"""
        ...
```

**íŠ¹ì§•**:
- asyncpg ê¸°ë°˜ ë¹„ë™ê¸° ì²˜ë¦¬
- Connection pooling
- StoragePort ì¸í„°í˜ì´ìŠ¤ í˜¸í™˜

#### MLflow Tracker Adapter

```python
# src/evalvault/adapters/outbound/tracker/mlflow_adapter.py
class MLflowTrackerAdapter:
    """MLflow ì‹¤í—˜ ì¶”ì  ì–´ëŒ‘í„°"""

    def __init__(self, tracking_uri: str):
        mlflow.set_tracking_uri(tracking_uri)

    def log_evaluation(
        self,
        run: EvaluationRun,
        experiment_name: str,
    ) -> None:
        """MLflowì— í‰ê°€ ê²°ê³¼ ê¸°ë¡"""
        ...
```

**MLflow Integration**:
- í‰ê°€ ê²°ê³¼ë¥¼ MLflow Runìœ¼ë¡œ ê¸°ë¡
- ë©”íŠ¸ë¦­ ì ìˆ˜ë¥¼ MLflow Metricsë¡œ ì €ì¥
- íŒŒë¼ë¯¸í„° ë° íƒœê·¸ ìë™ ì¶”ì¶œ

---

## Phase 7: Production Ready

> **Completed**: 2025-12-28
> **Tests**: +10
> **Description**: í”„ë¡œë•ì…˜ ë°°í¬ ì¤€ë¹„

### ë‹¬ì„± ë‚´ìš©

#### Performance Optimization

**ë³‘ë ¬ í‰ê°€**:
```bash
evalvault run data.csv \
  --metrics faithfulness answer_relevancy \
  --parallel \
  --batch-size 10
```

**ë°°ì¹˜ ì²˜ë¦¬**:
- í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¥¼ ë°°ì¹˜ë¡œ ë¶„í• 
- ë°°ì¹˜ë³„ ë³‘ë ¬ ì²˜ë¦¬
- CPU ì½”ì–´ ìˆ˜ì— ë§ì¶° ìë™ ì¡°ì •

**ì„±ëŠ¥ í–¥ìƒ**:
- 1000 í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ í‰ê°€ ì‹œê°„: 60ë¶„ â†’ 15ë¶„ (4ë°° í–¥ìƒ)
- CPU ì‚¬ìš©ë¥ : 25% â†’ 85%

#### Docker Containerization

**Dockerfile** (Multi-stage build):
```dockerfile
FROM python:3.12-slim AS builder
WORKDIR /app
RUN pip install uv
COPY . .
RUN uv build

FROM python:3.12-slim
WORKDIR /app
COPY --from=builder /app/dist/*.whl .
RUN pip install *.whl
USER 1000:1000
CMD ["evalvault", "--help"]
```

**docker-compose.yml**:
```yaml
version: '3.8'
services:
  postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: evalvault
      POSTGRES_USER: evalvault
      POSTGRES_PASSWORD: changeme
    volumes:
      - postgres_data:/var/lib/postgresql/data

  evalvault:
    build: .
    depends_on:
      - postgres
    environment:
      DATABASE_URL: postgresql://evalvault:changeme@postgres/evalvault
```

**ë³´ì•ˆ ê°•í™”**:
- ë¹„root ì‚¬ìš©ì ì‹¤í–‰
- Multi-stage buildë¡œ ì´ë¯¸ì§€ í¬ê¸° ìµœì†Œí™”
- ë¶ˆí•„ìš”í•œ íŒŒì¼ ì œì™¸ (.dockerignore)

---

## Phase 2 NLP: NLP Analysis

> **Completed**: 2025-12-29
> **Tests**: +97
> **Description**: í‰ê°€ ê²°ê³¼ ìì—°ì–´ ì²˜ë¦¬ ë¶„ì„

### ë‹¬ì„± ë‚´ìš©

#### NLP Adapter

```python
# src/evalvault/adapters/outbound/analysis/nlp_adapter.py
class NLPAnalysisAdapter:
    """í•˜ì´ë¸Œë¦¬ë“œ NLP ë¶„ì„ (Rule + ML + LLM)"""

    def analyze(
        self,
        run: EvaluationRun,
        use_llm: bool = False,
    ) -> NLPAnalysis:
        """í…ìŠ¤íŠ¸ ë¶„ì„ ì‹¤í–‰"""
        # Rule-based
        stats = self._calculate_text_stats(run)

        # ML-based
        keywords = self._extract_keywords(run)
        clusters = self._cluster_topics(run)

        # LLM-based (optional)
        if use_llm:
            insights = self._generate_llm_insights(run)

        return NLPAnalysis(
            stats=stats,
            keywords=keywords,
            topics=clusters,
            insights=insights if use_llm else None,
        )
```

**ì£¼ìš” ê¸°ëŠ¥**:
- í…ìŠ¤íŠ¸ í†µê³„ (ê¸¸ì´, ë‹¨ì–´ ìˆ˜, ë¬¸ì¥ ìˆ˜)
- í‚¤ì›Œë“œ ì¶”ì¶œ (TF-IDF, RAKE)
- ì£¼ì œ í´ëŸ¬ìŠ¤í„°ë§ (K-Means + Embeddings)
- LLM ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ (ì„ íƒì )

#### Analysis Service Integration

```python
# src/evalvault/domain/services/analysis_service.py
class AnalysisService:
    """í†µí•© ë¶„ì„ ì„œë¹„ìŠ¤"""

    def analyze_run(
        self,
        run_id: str,
        nlp: bool = False,
        causal: bool = False,
    ) -> AnalysisBundle:
        """ë‹¤ì°¨ì› ë¶„ì„ ì‹¤í–‰"""
        ...
```

**AnalysisBundle**:
- Statistical Analysis
- NLP Analysis (optional)
- Causal Analysis (optional)
- ëª¨ë“  ë¶„ì„ ê²°ê³¼ í†µí•©

#### CLI Integration

```bash
# NLP ë¶„ì„ ì‹¤í–‰
evalvault analyze <run_id> --nlp

# LLM ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ í¬í•¨
evalvault analyze <run_id> --nlp --profile dev

# ë³´ê³ ì„œ ìƒì„±
evalvault analyze <run_id> --nlp --report report.md
evalvault analyze <run_id> --nlp --report report.html
```

#### Topic Clustering

**K-Means + Embeddings**:
- ì§ˆë¬¸/ë‹µë³€ì„ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜
- K-Meansë¡œ í´ëŸ¬ìŠ¤í„°ë§
- í´ëŸ¬ìŠ¤í„°ë³„ ëŒ€í‘œ í‚¤ì›Œë“œ ì¶”ì¶œ

**ê²°ê³¼**:
```python
{
  "topic_0": {
    "keywords": ["ë³´í—˜ê¸ˆ", "ì§€ê¸‰", "ì²­êµ¬"],
    "sample_questions": ["ë³´í—˜ê¸ˆì€ ì–´ë–»ê²Œ ë°›ë‚˜ìš”?", ...],
    "size": 15
  },
  "topic_1": {
    "keywords": ["ë³´ì¥", "ë²”ìœ„", "í•œë„"],
    "sample_questions": ["ë³´ì¥ ë²”ìœ„ëŠ”?", ...],
    "size": 12
  }
}
```

#### Report Generation

**Markdown Report**:
```python
# src/evalvault/adapters/outbound/report/markdown_adapter.py
class MarkdownReportAdapter:
    def generate(
        self,
        analysis: AnalysisBundle,
        template: str = "default",
    ) -> str:
        """Markdown ë³´ê³ ì„œ ìƒì„±"""
        ...
```

**HTML Report**:
- Markdown â†’ HTML ë³€í™˜
- CSS ìŠ¤íƒ€ì¼ë§
- ì°¨íŠ¸ ì„ë² ë”© (Plotly)

---

## Phase 3 Causal: Causal Analysis

> **Completed**: 2025-12-29
> **Tests**: +27
> **Description**: ì¸ê³¼ ê´€ê³„ ë¶„ì„ ë° ê·¼ë³¸ ì›ì¸ íŒŒì•…

### ë‹¬ì„± ë‚´ìš©

#### Causal Adapter

```python
# src/evalvault/adapters/outbound/analysis/causal_adapter.py
class CausalAnalysisAdapter:
    """ì¸ê³¼ ë¶„ì„ ì–´ëŒ‘í„°"""

    def analyze(
        self,
        run: EvaluationRun,
    ) -> CausalAnalysis:
        """ì¸ê³¼ ê´€ê³„ ë¶„ì„"""
        # 1. ìš”ì¸ ì¶”ì¶œ
        factors = self._extract_factors(run)

        # 2. ìš”ì¸-ë©”íŠ¸ë¦­ ì˜í–¥ ë¶„ì„
        impacts = self._analyze_factor_impact(factors, run.results)

        # 3. ê·¼ë³¸ ì›ì¸ ë¶„ì„
        root_causes = self._identify_root_causes(impacts)

        # 4. ê°œì„  ì œì•ˆ ìƒì„±
        suggestions = self._generate_interventions(root_causes)

        return CausalAnalysis(
            factors=factors,
            impacts=impacts,
            root_causes=root_causes,
            suggestions=suggestions,
        )
```

#### Factor Extraction

**ì¸ê³¼ ìš”ì¸** (Causal Factors):
| Factor | Description | Type |
|--------|-------------|------|
| `question_length` | ì§ˆë¬¸ ê¸¸ì´ (ë‹¨ì–´ ìˆ˜) | Numeric |
| `answer_length` | ë‹µë³€ ê¸¸ì´ (ë‹¨ì–´ ìˆ˜) | Numeric |
| `context_count` | ì»¨í…ìŠ¤íŠ¸ ìˆ˜ | Numeric |
| `context_length` | ì»¨í…ìŠ¤íŠ¸ ì´ ê¸¸ì´ | Numeric |
| `question_complexity` | ì§ˆë¬¸ ë³µì¡ë„ | Numeric |
| `has_ground_truth` | ground_truth ì¡´ì¬ ì—¬ë¶€ | Boolean |
| `keyword_overlap` | ì§ˆë¬¸-ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ ê²¹ì¹¨ | Numeric |

#### Factor-Metric Impact Analysis

**ìƒê´€ ë¶„ì„**:
```python
def _analyze_factor_impact(
    self,
    factors: pd.DataFrame,
    results: list[TestCaseResult],
) -> list[FactorImpact]:
    """ìš”ì¸ì´ ë©”íŠ¸ë¦­ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ë¶„ì„"""
    impacts = []

    for metric in ["faithfulness", "answer_relevancy", ...]:
        metric_scores = [r.get_metric(metric) for r in results]

        for factor_name in factors.columns:
            factor_values = factors[factor_name]

            # ìƒê´€ ê³„ìˆ˜ ê³„ì‚°
            corr, p_value = pearsonr(factor_values, metric_scores)

            if p_value < 0.05:  # í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸
                impacts.append(FactorImpact(
                    factor=factor_name,
                    metric=metric,
                    correlation=corr,
                    p_value=p_value,
                    significance="strong" if abs(corr) > 0.7 else "moderate",
                ))

    return impacts
```

#### Root Cause Analysis

**ê·¼ë³¸ ì›ì¸ ì‹ë³„**:
```python
def _identify_root_causes(
    self,
    impacts: list[FactorImpact],
) -> dict[str, list[RootCause]]:
    """ë©”íŠ¸ë¦­ë³„ ê·¼ë³¸ ì›ì¸ ì‹ë³„"""
    root_causes = {}

    for metric in ["faithfulness", "answer_relevancy", ...]:
        metric_impacts = [i for i in impacts if i.metric == metric]

        # ê°•í•œ ë¶€ì • ìƒê´€ê´€ê³„ë¥¼ ê°€ì§„ ìš”ì¸
        negative_impacts = [
            i for i in metric_impacts
            if i.correlation < -0.5 and i.p_value < 0.05
        ]

        root_causes[metric] = [
            RootCause(
                metric=metric,
                factor=impact.factor,
                severity=self._calculate_severity(impact),
                evidence=f"Strong negative correlation: {impact.correlation:.2f}",
            )
            for impact in negative_impacts
        ]

    return root_causes
```

#### Intervention Suggestions

**ê°œì„  ì œì•ˆ ìƒì„±**:
```python
# ì˜ˆì‹œ ì¶œë ¥
{
  "faithfulness": [
    {
      "factor": "context_length",
      "suggestion": "Reduce context length to improve faithfulness",
      "rationale": "Long contexts (>500 words) correlate with lower faithfulness (-0.72)",
      "action": "Consider chunking long documents into smaller segments"
    }
  ]
}
```

#### Stratified Analysis

**ìš”ì¸ê°’ë³„ ê³„ì¸µí™” ë¶„ì„**:
```python
def _stratified_analysis(
    self,
    factor: str,
    metric: str,
    results: list[TestCaseResult],
) -> dict:
    """ìš”ì¸ê°’ì— ë”°ë¼ low/medium/highë¡œ ë‚˜ëˆ„ì–´ ë¶„ì„"""
    factor_values = [self._get_factor_value(r, factor) for r in results]
    metric_scores = [r.get_metric(metric) for r in results]

    # ìš”ì¸ê°’ 3ë¶„ìœ„ë¡œ ë¶„í• 
    low_threshold = np.percentile(factor_values, 33)
    high_threshold = np.percentile(factor_values, 67)

    low_scores = [s for f, s in zip(factor_values, metric_scores) if f < low_threshold]
    med_scores = [s for f, s in zip(factor_values, metric_scores) if low_threshold <= f < high_threshold]
    high_scores = [s for f, s in zip(factor_values, metric_scores) if f >= high_threshold]

    return {
        "low": {"mean": np.mean(low_scores), "std": np.std(low_scores)},
        "medium": {"mean": np.mean(med_scores), "std": np.std(med_scores)},
        "high": {"mean": np.mean(high_scores), "std": np.std(high_scores)},
    }
```

---

## Phase 8: Domain Memory Layering

> **Completed**: 2025-12-29
> **Tests**: +113
> **Description**: í‰ê°€ì—ì„œ í•™ìŠµí•˜ì—¬ ì •í™•ë„ í–¥ìƒ

### ë‹¬ì„± ë‚´ìš©

#### Domain Memory 3ê³„ì¸µ êµ¬ì¡°

**Factual Memory** (ê²€ì¦ëœ ì •ì  ì‚¬ì‹¤):
- ìš©ì–´ ì‚¬ì „
- ê·œì • ë¬¸ì„œ
- SQLite FTS5ë¡œ ë¹ ë¥¸ ê²€ìƒ‰

**Experiential Memory** (í•™ìŠµëœ íŒ¨í„´):
- ì—”í‹°í‹° íƒ€ì…ë³„ ì‹ ë¢°ë„
- ì‹¤íŒ¨ íŒ¨í„´
- í‰ê°€ ê²°ê³¼ì—ì„œ ìë™ í•™ìŠµ

**Working Memory** (í˜„ì¬ ì»¨í…ìŠ¤íŠ¸):
- ì„¸ì…˜ ìºì‹œ
- í™œì„± KG ë°”ì¸ë”©

#### Domain Memory Adapter

```python
# src/evalvault/adapters/outbound/domain_memory/sqlite_adapter.py
class SQLiteDomainMemoryAdapter:
    """SQLite + FTS5 ê¸°ë°˜ ë„ë©”ì¸ ë©”ëª¨ë¦¬"""

    def store_fact(
        self,
        fact: FactualFact,
    ) -> None:
        """ì‚¬ì‹¤ ì €ì¥ (FTS5 ì¸ë±ì‹±)"""
        ...

    def query_facts(
        self,
        query: str,
        domain: str,
        language: str = "ko",
        limit: int = 10,
    ) -> list[FactualFact]:
        """ì „ë¬¸ ê²€ìƒ‰ (FTS5)"""
        ...

    def record_learning(
        self,
        learning: LearningMemory,
    ) -> None:
        """í•™ìŠµ íŒ¨í„´ ê¸°ë¡"""
        ...

    def get_aggregated_reliability(
        self,
        entity_type: str,
    ) -> float:
        """ì—”í‹°í‹° íƒ€ì…ë³„ ì§‘ê³„ ì‹ ë¢°ë„"""
        ...
```

#### Domain Learning Hook

```python
# src/evalvault/domain/services/domain_learning_hook.py
class DomainLearningHook:
    """í‰ê°€ ê²°ê³¼ì—ì„œ í•™ìŠµí•˜ëŠ” í›…"""

    def on_evaluation_complete(
        self,
        run: EvaluationRun,
    ) -> LearningMemory:
        """í‰ê°€ ì™„ë£Œ ì‹œ íŒ¨í„´ í•™ìŠµ"""
        # 1. ì—”í‹°í‹° íƒ€ì…ë³„ ì‹ ë¢°ë„ ê³„ì‚°
        entity_reliability = self._calculate_entity_reliability(run)

        # 2. ì‹¤íŒ¨ íŒ¨í„´ ì‹ë³„
        failure_patterns = self._identify_failure_patterns(run)

        # 3. í•™ìŠµ ë©”ëª¨ë¦¬ ìƒì„±
        return LearningMemory(
            entity_reliability=entity_reliability,
            failure_patterns=failure_patterns,
            timestamp=datetime.now(),
        )

    def apply_learning(
        self,
        extractor: EntityExtractor,
    ) -> None:
        """í•™ìŠµëœ íŒ¨í„´ì„ ì¶”ì¶œê¸°ì— ì ìš©"""
        reliability_scores = self.memory.get_aggregated_reliability()

        # ì‹ ë¢°ë„ ì ìˆ˜ë¥¼ ê°€ì¤‘ì¹˜ë¡œ ì ìš©
        extractor.set_type_weights(reliability_scores)
```

#### Config Extension

```yaml
# config/domains/insurance/memory.yaml
factual:
  glossary: terms_dictionary_ko.json
  regulatory_rules: rules.md
  languages: ["ko", "en"]

experiential:
  reliability_scores: reliability.json
  failure_modes: failures.json

working:
  run_cache: ${RUN_DIR}/memory.db
  kg_binding: kg://insurance
```

#### CLI Commands

```bash
# ë„ë©”ì¸ ì´ˆê¸°í™”
evalvault domain init insurance --languages ko,en

# ë„ë©”ì¸ ëª©ë¡
evalvault domain list

# ë„ë©”ì¸ ì„¤ì • ì¡°íšŒ
evalvault domain show insurance

# ìš©ì–´ì‚¬ì „ ì¡°íšŒ
evalvault domain terms insurance --language ko --limit 10
```

#### í•™ìŠµ í”¼ë“œë°± ë£¨í”„

```
í‰ê°€ #1: Dataset â†’ RagasEvaluator â†’ EvaluationRun
    â””â”€> DomainLearningHook.on_evaluation_complete()
            â”œâ”€> ì—”í‹°í‹° íƒ€ì…ë³„ ì‹ ë¢°ë„ ê³„ì‚° (ì˜ˆ: "organization" = 0.92)
            â””â”€> LearningMemory ì €ì¥

í‰ê°€ #2 (KG ê¸°ë°˜ í…ŒìŠ¤íŠ¸ì…‹ ìƒì„± ì‹œ):
    â””â”€> KnowledgeGraphGenerator.build_graph(documents)
            â””â”€> EntityExtractor.extract_entities()
                    â””â”€> DomainMemoryAdapter.get_aggregated_reliability()
                            â””â”€> í•™ìŠµëœ ì‹ ë¢°ë„ ì ìˆ˜ë¥¼ ê°€ì¤‘ì¹˜ë¡œ ì ìš©
                                    â””â”€> ë” ì •í™•í•œ ì—”í‹°í‹° ì¶”ì¶œ
```

---

## Phase 9: Korean RAG Optimization

> **Completed**: 2025-12-30
> **Tests**: +24
> **Description**: í•œêµ­ì–´ RAG ì‹œìŠ¤í…œ í‰ê°€ ë„êµ¬

### ë‹¬ì„± ë‚´ìš©

#### Korean NLP Foundation

**Kiwi Tokenizer**:
```python
# src/evalvault/adapters/outbound/nlp/korean/kiwi_tokenizer.py
class KiwiTokenizer:
    """Kiwi ê¸°ë°˜ í˜•íƒœì†Œ ë¶„ì„ê¸°"""

    def __init__(self):
        from kiwipiepy import Kiwi
        self.kiwi = Kiwi()

    def tokenize(
        self,
        text: str,
        pos_filter: list[str] | None = None,
    ) -> list[str]:
        """í˜•íƒœì†Œ ë¶„ì„ ë° í† í°í™”"""
        tokens = self.kiwi.tokenize(text)

        if pos_filter:
            tokens = [t for t in tokens if t.tag in pos_filter]

        return [t.form for t in tokens]
```

**Korean Stopwords**:
```python
# src/evalvault/adapters/outbound/nlp/korean/korean_stopwords.py
KOREAN_STOPWORDS = {
    # ì¡°ì‚¬
    "ì€", "ëŠ”", "ì´", "ê°€", "ì„", "ë¥¼", "ì—", "ì—ì„œ", "ë¡œ", "ìœ¼ë¡œ",
    # ì–´ë¯¸
    "ë‹¤", "ìš”", "ê¹Œ", "ë‹ˆ", "ì§€",
    # ê¸°íƒ€
    "ê²ƒ", "ìˆ˜", "ë“±", ...
}
```

#### Korean BM25 Retriever

```python
# src/evalvault/adapters/outbound/nlp/korean/korean_bm25_retriever.py
class KoreanBM25Retriever:
    """í˜•íƒœì†Œ ë¶„ì„ ê¸°ë°˜ BM25 ê²€ìƒ‰"""

    def __init__(self, tokenizer: KiwiTokenizer):
        self.tokenizer = tokenizer
        self.bm25 = None

    def fit(self, documents: list[str]) -> None:
        """ë¬¸ì„œ ì¸ë±ì‹±"""
        from rank_bm25 import BM25Okapi

        tokenized_docs = [
            self.tokenizer.tokenize(doc, pos_filter=["NNG", "NNP", "VV", "VA"])
            for doc in documents
        ]
        self.bm25 = BM25Okapi(tokenized_docs)

    def retrieve(
        self,
        query: str,
        top_k: int = 5,
    ) -> list[tuple[int, float]]:
        """ì¿¼ë¦¬ ê²€ìƒ‰"""
        tokenized_query = self.tokenizer.tokenize(query)
        scores = self.bm25.get_scores(tokenized_query)

        # Top-K ì¸ë±ìŠ¤ì™€ ì ìˆ˜
        top_indices = np.argsort(scores)[-top_k:][::-1]
        return [(idx, scores[idx]) for idx in top_indices]
```

**ì„±ëŠ¥ í–¥ìƒ**:
- í‚¤ì›Œë“œ ì¶”ì¶œ ì •í™•ë„: 60% â†’ 85%+ (ê³µë°± ê¸°ì¤€ ëŒ€ë¹„)
- ê²€ìƒ‰ ì •í™•ë„: +25%

#### Korean Dense Retriever

**BGE-m3-ko Embeddings**:
```python
# src/evalvault/adapters/outbound/nlp/korean/korean_dense_retriever.py
class KoreanDenseRetriever:
    """BGE-m3-ko ì„ë² ë”© ê¸°ë°˜ ê²€ìƒ‰"""

    def __init__(self, model_name: str = "dragonkue/BGE-m3-ko"):
        from sentence_transformers import SentenceTransformer
        self.model = SentenceTransformer(model_name)
        self.doc_embeddings = None

    def fit(self, documents: list[str]) -> None:
        """ë¬¸ì„œ ì„ë² ë”©"""
        self.doc_embeddings = self.model.encode(
            documents,
            show_progress_bar=True,
        )

    def retrieve(
        self,
        query: str,
        top_k: int = 5,
    ) -> list[tuple[int, float]]:
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê²€ìƒ‰"""
        query_embedding = self.model.encode([query])[0]
        similarities = cosine_similarity([query_embedding], self.doc_embeddings)[0]

        top_indices = np.argsort(similarities)[-top_k:][::-1]
        return [(idx, similarities[idx]) for idx in top_indices]
```

**ì„±ëŠ¥**:
- AutoRAG ë²¤ì¹˜ë§ˆí¬ 1ìœ„ ëª¨ë¸
- ê¸°ì¡´ ëŒ€ë¹„ +39.4% ì„±ëŠ¥ í–¥ìƒ

#### Korean Hybrid Retriever

**BM25 + Dense (Reciprocal Rank Fusion)**:
```python
# src/evalvault/adapters/outbound/nlp/korean/korean_hybrid_retriever.py
class KoreanHybridRetriever:
    """BM25 + Dense í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰"""

    def __init__(
        self,
        bm25_retriever: KoreanBM25Retriever,
        dense_retriever: KoreanDenseRetriever,
        alpha: float = 0.5,
    ):
        self.bm25 = bm25_retriever
        self.dense = dense_retriever
        self.alpha = alpha  # BM25 ê°€ì¤‘ì¹˜

    def retrieve(
        self,
        query: str,
        top_k: int = 5,
    ) -> list[tuple[int, float]]:
        """RRF ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰"""
        bm25_results = self.bm25.retrieve(query, top_k=top_k * 2)
        dense_results = self.dense.retrieve(query, top_k=top_k * 2)

        # Reciprocal Rank Fusion
        scores = {}
        for rank, (idx, _) in enumerate(bm25_results):
            scores[idx] = scores.get(idx, 0) + self.alpha / (rank + 1)

        for rank, (idx, _) in enumerate(dense_results):
            scores[idx] = scores.get(idx, 0) + (1 - self.alpha) / (rank + 1)

        # Top-K ì„ íƒ
        top_indices = sorted(scores.keys(), key=lambda x: scores[x], reverse=True)[:top_k]
        return [(idx, scores[idx]) for idx in top_indices]
```

#### Korean Faithfulness Verification

```python
# src/evalvault/adapters/outbound/nlp/korean/korean_faithfulness.py
class KoreanFaithfulnessVerifier:
    """í•œêµ­ì–´ Faithfulness ê²€ì¦ ë³´ì¡°"""

    def extract_claims(
        self,
        answer: str,
    ) -> list[str]:
        """ë‹µë³€ì—ì„œ ì£¼ì¥ ì¶”ì¶œ (í˜•íƒœì†Œ ë¶„ì„ ê¸°ë°˜)"""
        ...

    def verify_claim(
        self,
        claim: str,
        context: str,
    ) -> bool:
        """ì£¼ì¥ì´ ì»¨í…ìŠ¤íŠ¸ì— ê·¼ê±°í•˜ëŠ”ì§€ ê²€ì¦"""
        ...
```

#### Benchmark Runner

```python
# src/evalvault/domain/services/benchmark_runner.py
class KoreanRAGBenchmarkRunner:
    """í•œêµ­ì–´ RAG ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ê¸°"""

    def run_benchmark(
        self,
        test_cases: list[RAGTestCase],
        retrievers: list[str],
    ) -> BenchmarkResult:
        """ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ë° ë¹„êµ"""
        results = {}

        for retriever_name in retrievers:
            retriever = self._create_retriever(retriever_name)
            scores = []

            for test_case in test_cases:
                retrieved = retriever.retrieve(test_case.query)
                score = self._calculate_score(retrieved, test_case.relevant_docs)
                scores.append(score)

            results[retriever_name] = {
                "mean_score": np.mean(scores),
                "std": np.std(scores),
            }

        return BenchmarkResult(results)
```

---

## Phase 10-13: Streamlit Web UI

> **Completed**: 2025-12-30
> **Tests**: +138
> **Description**: ì›¹ ê¸°ë°˜ ëŒ€ì‹œë³´ë“œ

### ë‹¬ì„± ë‚´ìš©

#### Web UI Structure

```
src/evalvault/adapters/inbound/web/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ adapter.py              # WebUIAdapter (700 LOC)
â”œâ”€â”€ app.py                  # Streamlit ì•± (200 LOC)
â”œâ”€â”€ session.py              # ì„¸ì…˜ ê´€ë¦¬ (100 LOC)
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ cards.py            # ì¹´ë“œ ì»´í¬ë„ŒíŠ¸
â”‚   â”œâ”€â”€ charts.py           # Plotly ì°¨íŠ¸
â”‚   â”œâ”€â”€ evaluate.py         # í‰ê°€ ì‹¤í–‰
â”‚   â”œâ”€â”€ history.py          # íˆìŠ¤í† ë¦¬
â”‚   â”œâ”€â”€ lists.py            # ë¦¬ìŠ¤íŠ¸
â”‚   â”œâ”€â”€ metrics.py          # ë©”íŠ¸ë¦­ í‘œì‹œ
â”‚   â”œâ”€â”€ progress.py         # ì§„í–‰ í‘œì‹œ
â”‚   â”œâ”€â”€ reports.py          # ë³´ê³ ì„œ
â”‚   â”œâ”€â”€ stats.py            # í†µê³„
â”‚   â””â”€â”€ upload.py           # íŒŒì¼ ì—…ë¡œë“œ
â”œâ”€â”€ pages/
â”‚   â””â”€â”€ ...                 # í˜ì´ì§€ ë¼ìš°íŒ…
â””â”€â”€ styles/
    â””â”€â”€ ...                 # ìŠ¤íƒ€ì¼
```

#### Dashboard (Phase 11)

**ì£¼ìš” ê¸°ëŠ¥**:
- í‰ê°€ ê²°ê³¼ ê°œìš” ì¹´ë“œ
- ë©”íŠ¸ë¦­ë³„ ì„±ëŠ¥ ì°¨íŠ¸ (Bar, Radar)
- ì‹œê°„ë³„ ì¶”ì„¸ ì°¨íŠ¸
- ìµœê·¼ í‰ê°€ ëª©ë¡

**Plotly Charts**:
```python
# src/evalvault/adapters/inbound/web/components/charts.py
def create_metrics_bar_chart(metrics: dict[str, float]) -> go.Figure:
    """ë©”íŠ¸ë¦­ ë§‰ëŒ€ ì°¨íŠ¸"""
    fig = go.Figure(data=[
        go.Bar(
            x=list(metrics.keys()),
            y=list(metrics.values()),
            marker_color=["#2ecc71" if v >= 0.7 else "#e74c3c" for v in metrics.values()],
        )
    ])
    fig.update_layout(
        title="Metrics Performance",
        yaxis_title="Score",
        yaxis_range=[0, 1],
    )
    return fig

def create_radar_chart(metrics: dict[str, float]) -> go.Figure:
    """ë©”íŠ¸ë¦­ ë ˆì´ë” ì°¨íŠ¸"""
    ...
```

#### Evaluate Page (Phase 12.1)

**íŒŒì¼ ì—…ë¡œë“œ**:
- CSV, Excel, JSON ì§€ì›
- ë“œë˜ê·¸ ì•¤ ë“œë¡­
- ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° ë° ê²€ì¦

**ë©”íŠ¸ë¦­ ì„ íƒ UI**:
- ì²´í¬ë°•ìŠ¤ë¡œ ë©”íŠ¸ë¦­ ì„ íƒ
- ë©”íŠ¸ë¦­ë³„ ì„¤ëª… í‘œì‹œ
- ì„ê³„ê°’ ì„¤ì •

**ì‹¤ì‹œê°„ ì§„í–‰ í‘œì‹œ**:
```python
import streamlit as st

progress_bar = st.progress(0)
status_text = st.empty()

for i, test_case in enumerate(dataset):
    result = evaluate(test_case)
    progress_bar.progress((i + 1) / len(dataset))
    status_text.text(f"Evaluating {i + 1}/{len(dataset)}...")
```

#### History Page (Phase 12.2)

**í‰ê°€ íˆìŠ¤í† ë¦¬ í…Œì´ë¸”**:
- í˜ì´ì§€ë„¤ì´ì…˜
- ì •ë ¬ (ë‚ ì§œ, ì ìˆ˜, ë°ì´í„°ì…‹)
- í•„í„°ë§ (ë‚ ì§œ ë²”ìœ„, ë°ì´í„°ì…‹, ëª¨ë¸)

**ê²°ê³¼ ë¹„êµ**:
- ë‘ í‰ê°€ ì„ íƒí•˜ì—¬ ë¹„êµ
- Side-by-side ì°¨íŠ¸
- ì°¨ì´ì  í•˜ì´ë¼ì´íŠ¸

**ë‚´ë³´ë‚´ê¸°**:
- JSON í˜•ì‹
- CSV í˜•ì‹
- Excel í˜•ì‹

#### Reports Page (Phase 13)

**í…œí”Œë¦¿ ê¸°ë°˜ ë³´ê³ ì„œ**:
- Basic Summary
- Detailed Analysis
- Comparison Report

**ë³´ê³ ì„œ ì»¤ìŠ¤í„°ë§ˆì´ì§•**:
- í…œí”Œë¦¿ ì„ íƒ
- ì°¨íŠ¸ í¬í•¨ ì—¬ë¶€
- ì„¹ì…˜ ì„ íƒ

**ë‹¤ìš´ë¡œë“œ**:
- Markdown (.md)
- HTML (.html)
- PDF (ì¶”í›„ ì§€ì›)

---

## Phase 14: Query-Based DAG Analysis Pipeline

> **Completed**: 2025-12-30
> **Tests**: +153
> **Description**: ì¿¼ë¦¬ ê¸°ë°˜ ìë™ ë¶„ì„ íŒŒì´í”„ë¼ì¸

### ë‹¬ì„± ë‚´ìš©

#### Domain Entities

```python
# src/evalvault/domain/entities/analysis_pipeline.py

class AnalysisIntent(StrEnum):
    """ë¶„ì„ ì˜ë„ (12ê°€ì§€)"""
    VERIFY_MORPHEME = "verify_morpheme"
    VERIFY_EMBEDDING = "verify_embedding"
    VERIFY_RETRIEVAL = "verify_retrieval"
    COMPARE_SEARCH_METHODS = "compare_search_methods"
    COMPARE_MODELS = "compare_models"
    COMPARE_RUNS = "compare_runs"
    ANALYZE_LOW_METRICS = "analyze_low_metrics"
    ANALYZE_PATTERNS = "analyze_patterns"
    ANALYZE_TRENDS = "analyze_trends"
    GENERATE_SUMMARY = "generate_summary"
    GENERATE_DETAILED = "generate_detailed"
    GENERATE_COMPARISON = "generate_comparison"

class AnalysisNode:
    """ë¶„ì„ ë…¸ë“œ"""
    node_id: str
    module_id: str
    params: dict
    dependencies: list[str]

class AnalysisPipeline:
    """ë¶„ì„ íŒŒì´í”„ë¼ì¸ (DAG)"""
    nodes: list[AnalysisNode]
    edges: list[tuple[str, str]]

    def topological_order(self) -> list[str]:
        """ìœ„ìƒ ì •ë ¬"""
        ...

    def validate(self) -> bool:
        """ìˆœí™˜ ì°¸ì¡° ê²€ì¦"""
        ...
```

#### Intent Classifier

```python
# src/evalvault/domain/services/intent_classifier.py
class KeywordIntentClassifier:
    """í‚¤ì›Œë“œ ê¸°ë°˜ ì˜ë„ ë¶„ë¥˜"""

    def __init__(self, registry: IntentKeywordRegistry):
        self.registry = registry

    def classify(self, query: str) -> AnalysisIntent:
        """ì¿¼ë¦¬ì—ì„œ ì˜ë„ ë¶„ë¥˜"""
        query_lower = query.lower()
        scores = {}

        for intent, keywords in self.registry.items():
            score = sum(1 for kw in keywords if kw in query_lower)
            if score > 0:
                scores[intent] = score

        if not scores:
            return AnalysisIntent.GENERATE_SUMMARY

        return max(scores, key=scores.get)

    def classify_with_confidence(
        self,
        query: str,
    ) -> tuple[AnalysisIntent, float]:
        """ì˜ë„ + ì‹ ë¢°ë„ ë°˜í™˜"""
        ...
```

**Keyword Registry**:
```python
INTENT_KEYWORDS = {
    AnalysisIntent.VERIFY_MORPHEME: [
        "í˜•íƒœì†Œ", "ë¶„ì„", "í† í°", "tokenize", "morpheme",
    ],
    AnalysisIntent.COMPARE_SEARCH_METHODS: [
        "bm25", "dense", "hybrid", "ê²€ìƒ‰", "ë¹„êµ", "search", "compare",
    ],
    ...
}
```

#### Pipeline Orchestrator

```python
# src/evalvault/domain/services/pipeline_orchestrator.py
class PipelineOrchestrator:
    """íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°"""

    def __init__(
        self,
        module_catalog: ModuleCatalog,
        template_registry: PipelineTemplateRegistry,
    ):
        self.catalog = module_catalog
        self.templates = template_registry

    def execute(
        self,
        pipeline: AnalysisPipeline,
        context: AnalysisContext,
    ) -> PipelineResult:
        """íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        result = PipelineResult(pipeline_id=pipeline.pipeline_id)

        # ìœ„ìƒ ì •ë ¬
        order = pipeline.topological_order()

        # ìˆœì°¨ ì‹¤í–‰
        for node_id in order:
            node = pipeline.get_node(node_id)
            module = self.catalog.get_module(node.module_id)

            # ì˜ì¡´ì„± ê²°ê³¼ ìˆ˜ì§‘
            inputs = self._collect_inputs(node, result)

            # ëª¨ë“ˆ ì‹¤í–‰
            try:
                output = module.execute(inputs, node.params)
                result.add_node_result(NodeResult(
                    node_id=node_id,
                    status=NodeExecutionStatus.COMPLETED,
                    output=output,
                ))
            except Exception as e:
                result.add_node_result(NodeResult(
                    node_id=node_id,
                    status=NodeExecutionStatus.FAILED,
                    error=str(e),
                ))
                break

        result.mark_complete()
        return result
```

#### Analysis Modules

**Base Module**:
```python
# src/evalvault/adapters/outbound/analysis/base_module.py
class BaseAnalysisModule(ABC):
    """ë¶„ì„ ëª¨ë“ˆ ë² ì´ìŠ¤ í´ë˜ìŠ¤"""

    module_id: str
    name: str
    description: str
    input_types: list[str]
    output_types: list[str]

    @abstractmethod
    def execute(
        self,
        inputs: dict[str, Any],
        params: dict[str, Any] | None = None,
    ) -> dict[str, Any]:
        """ëª¨ë“ˆ ì‹¤í–‰"""
        ...

    def validate_inputs(self, inputs: dict) -> bool:
        """ì…ë ¥ ê²€ì¦"""
        ...
```

**êµ¬í˜„ëœ ëª¨ë“ˆ**:
- `DataLoaderModule`: ë°ì´í„° ë¡œë”©
- `StatisticalAnalyzerModule`: í†µê³„ ë¶„ì„
- `SummaryReportModule`: ìš”ì•½ ë³´ê³ ì„œ
- `VerificationReportModule`: ê²€ì¦ ë³´ê³ ì„œ
- `ComparisonReportModule`: ë¹„êµ ë³´ê³ ì„œ
- `AnalysisReportModule`: ë¶„ì„ ë³´ê³ ì„œ

#### Pipeline Templates

```python
# src/evalvault/domain/services/pipeline_template_registry.py
class PipelineTemplateRegistry:
    """ì˜ë„ë³„ íŒŒì´í”„ë¼ì¸ í…œí”Œë¦¿"""

    def get_template(
        self,
        intent: AnalysisIntent,
    ) -> AnalysisPipeline:
        """ì˜ë„ì— ë§ëŠ” íŒŒì´í”„ë¼ì¸ í…œí”Œë¦¿ ë°˜í™˜"""
        ...

# ì˜ˆ: VERIFY_MORPHEME í…œí”Œë¦¿
def _verify_morpheme_template() -> AnalysisPipeline:
    return AnalysisPipeline(
        nodes=[
            AnalysisNode(
                node_id="data_loader",
                module_id="data_loader",
                params={"run_id": "..."},
            ),
            AnalysisNode(
                node_id="morpheme_analyzer",
                module_id="morpheme_analyzer",
                dependencies=["data_loader"],
            ),
            AnalysisNode(
                node_id="verification_report",
                module_id="verification_report",
                dependencies=["morpheme_analyzer"],
            ),
        ],
        edges=[
            ("data_loader", "morpheme_analyzer"),
            ("morpheme_analyzer", "verification_report"),
        ],
    )
```

#### Async Execution

**ë¹„ë™ê¸° ë³‘ë ¬ ì‹¤í–‰**:
```python
async def execute_async(
    self,
    pipeline: AnalysisPipeline,
    context: AnalysisContext,
) -> PipelineResult:
    """ë¹„ë™ê¸° íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ë³‘ë ¬í™”)"""
    result = PipelineResult(pipeline_id=pipeline.pipeline_id)

    # ë ˆë²¨ë³„ ê·¸ë£¹í™” (ìœ„ìƒ ì •ë ¬ ê¸°ë°˜)
    levels = pipeline.group_by_level()

    # ë ˆë²¨ë³„ ìˆœì°¨ ì‹¤í–‰, ë ˆë²¨ ë‚´ ë³‘ë ¬ ì‹¤í–‰
    for level_nodes in levels:
        tasks = [
            self._execute_node_async(node, result, context)
            for node in level_nodes
        ]
        await asyncio.gather(*tasks)

    result.mark_complete()
    return result
```

---

## ì•„í‚¤í…ì²˜ í˜„í™©

### Hexagonal Architecture

```
src/evalvault/
â”œâ”€â”€ domain/                     # ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ (í”„ë ˆì„ì›Œí¬ ë…ë¦½)
â”‚   â”œâ”€â”€ entities/               # ë„ë©”ì¸ ì—”í‹°í‹°
â”‚   â”œâ”€â”€ services/               # ë„ë©”ì¸ ì„œë¹„ìŠ¤
â”‚   â””â”€â”€ metrics/                # ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­
â”œâ”€â”€ ports/                      # ì¸í„°í˜ì´ìŠ¤ ì •ì˜
â”‚   â”œâ”€â”€ inbound/                # ì§„ì…ì  í¬íŠ¸
â”‚   â””â”€â”€ outbound/               # ì™¸ë¶€ ì˜ì¡´ì„± í¬íŠ¸
â”œâ”€â”€ adapters/                   # í¬íŠ¸ êµ¬í˜„
â”‚   â”œâ”€â”€ inbound/                # CLI, Web UI
â”‚   â””â”€â”€ outbound/               # LLM, DB, Tracker ë“±
â””â”€â”€ config/                     # ì„¤ì •
```

### Port/Adapter êµ¬í˜„ í˜„í™©

| Port | Adapter | Status |
|------|---------|--------|
| LLMPort | OpenAIAdapter | âœ… Complete |
| LLMPort | AzureOpenAIAdapter | âœ… Complete |
| LLMPort | AnthropicAdapter | âœ… Complete |
| LLMPort | OllamaAdapter | âœ… Complete |
| DatasetPort | CSVLoader | âœ… Complete |
| DatasetPort | ExcelLoader | âœ… Complete |
| DatasetPort | JSONLoader | âœ… Complete |
| TrackerPort | LangfuseAdapter | âœ… Complete |
| TrackerPort | MLflowAdapter | âœ… Complete |
| StoragePort | SQLiteAdapter | âœ… Complete |
| StoragePort | PostgreSQLAdapter | âœ… Complete |
| EvaluatorPort | RagasEvaluator | âœ… Complete |
| NLPAnalysisPort | NLPAnalysisAdapter | âœ… Complete |
| CausalAnalysisPort | CausalAnalysisAdapter | âœ… Complete |
| ReportPort | MarkdownReportAdapter | âœ… Complete |
| DomainMemoryPort | SQLiteDomainMemoryAdapter | âœ… Complete |
| AnalysisPipelinePort | PipelineOrchestrator | âœ… Complete |
| AnalysisModulePort | 6 modules implemented | âœ… Complete |
| IntentClassifierPort | KeywordIntentClassifier | âœ… Complete |

---

## í…ŒìŠ¤íŠ¸ í˜„í™©

### í…ŒìŠ¤íŠ¸ í†µê³„

| Category | Count | Description |
|----------|-------|-------------|
| Unit Tests | 1,261 | Domain, ports, adapters, services |
| Integration Tests | 91 | End-to-end flows |
| **Total** | **1,352** | All passing |
| **Coverage** | **89%** | Code coverage |

### Phaseë³„ í…ŒìŠ¤íŠ¸ ìˆ˜

| Phase | Tests | Coverage |
|-------|-------|----------|
| Phase 1-3 | 118 | Core System |
| Phase 4 | +60 | Foundation |
| Phase 5 | +42 | Storage & Domain |
| Phase 6 | +160 | Advanced Features |
| Phase 7 | +10 | Production Ready |
| Phase 2 NLP | +97 | NLP Analysis |
| Phase 3 Causal | +27 | Causal Analysis |
| Phase 8 | +113 | Domain Memory |
| Phase 9 | +24 | Korean RAG |
| Phase 10-13 | +138 | Web UI |
| Phase 14 | +153 | Analysis Pipeline |

### í…ŒìŠ¤íŠ¸ íŒŒì¼ êµ¬ì¡°

```
tests/
â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ test_entities.py
â”‚   â”œâ”€â”€ test_data_loaders.py
â”‚   â”œâ”€â”€ test_evaluator.py
â”‚   â”œâ”€â”€ test_langfuse_tracker.py
â”‚   â”œâ”€â”€ test_openai_adapter.py
â”‚   â”œâ”€â”€ test_nlp_adapter.py
â”‚   â”œâ”€â”€ test_causal_adapter.py
â”‚   â”œâ”€â”€ test_domain_memory.py
â”‚   â”œâ”€â”€ test_benchmark_runner.py
â”‚   â”œâ”€â”€ test_web_ui.py
â”‚   â”œâ”€â”€ test_analysis_pipeline.py
â”‚   â””â”€â”€ ...
â””â”€â”€ integration/
    â”œâ”€â”€ test_evaluation_flow.py
    â”œâ”€â”€ test_data_flow.py
    â”œâ”€â”€ test_langfuse_flow.py
    â”œâ”€â”€ test_storage_flow.py
    â”œâ”€â”€ test_web_ui_evaluation.py
    â””â”€â”€ ...
```

---

## CI/CD & Release

### Cross-Platform CI

| Platform | Python | Status |
|----------|--------|--------|
| Ubuntu | 3.12, 3.13 | âœ… Passing |
| macOS | 3.12 | âœ… Passing |
| Windows | 3.12 | âœ… Passing |

### Automatic Versioning

**python-semantic-release**ë¡œ ìë™ ë²„ì „ ê´€ë¦¬:

| Commit Type | Version Bump | Example |
|-------------|--------------|---------|
| `feat:` | Minor (0.x.0) | `feat: Add new metric` â†’ 0.2.0 |
| `fix:`, `perf:` | Patch (0.0.x) | `fix: Correct calculation` â†’ 0.1.1 |
| Other | No release | `docs:`, `chore:`, `ci:` |

### Release Workflow

1. **PR ìƒì„±** â†’ CI í…ŒìŠ¤íŠ¸ (Ubuntu, macOS, Windows)
2. **PR ë¨¸ì§€** â†’ main ë¸Œëœì¹˜ í‘¸ì‹œ
3. **Release ì›Œí¬í”Œë¡œìš° ì‹¤í–‰**:
   - Conventional Commits ë¶„ì„
   - ë²„ì „ íƒœê·¸ ìƒì„± (ì˜ˆ: v1.5.0)
   - PyPI ë°°í¬
   - GitHub Release ìƒì„±

### ë²„ì „ íˆìŠ¤í† ë¦¬

| Version | Date | Description |
|---------|------|-------------|
| 0.1.0 | 2024-12-24 | Phase 3 Complete - Core System |
| 0.2.0 | 2024-12-24 | Phase 5 Complete - Storage & Domain |
| 0.3.0 | 2025-12-24 | Phase 6 Complete - Advanced Features |
| 1.0.0 | 2025-12-28 | OSS Release - PyPI ë°°í¬, CI/CD ìë™í™” |
| 1.1.0 | 2025-12-29 | Phase 2 NLP + Phase 3 Causal Analysis |
| 1.2.0 | 2025-12-29 | Phase 8 Domain Memory Layering |
| 1.3.0 | 2025-12-30 | Phase 9 Korean RAG Optimization |
| 1.4.0 | 2025-12-30 | Phase 10-13 Streamlit Web UI |
| 1.5.0 | 2025-12-30 | Phase 14 Query-Based DAG Analysis Pipeline |

---

## ë§ˆë¬´ë¦¬

EvalVaultëŠ” 21ì£¼ê°„ì˜ ê°œë°œì„ í†µí•´ Phase 1-14ë¥¼ ëª¨ë‘ ì™„ë£Œí•˜ê³ , ì•ˆì •ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ RAG í‰ê°€ í”Œë«í¼ìœ¼ë¡œ ì„±ì¥í–ˆìŠµë‹ˆë‹¤.

### í•µì‹¬ ì„±ê³¼ ìš”ì•½

- âœ… **1,352 tests passing** (89% coverage)
- âœ… **14 Phases completed** (100%)
- âœ… **Multi-LLM, Multi-DB, Multi-Tracker** ì§€ì›
- âœ… **Korean NLP** ìµœì í™”
- âœ… **Web UI** ì œê³µ
- âœ… **DAG Analysis Pipeline** êµ¬ì¶•
- âœ… **CI/CD & PyPI** ë°°í¬

### ë‹¤ìŒ ë‹¨ê³„

- ğŸ“‹ [IMPROVEMENT_PLAN.md](./IMPROVEMENT_PLAN.md): ì½”ë“œ í’ˆì§ˆ ê°œì„  ê³„íš
- ğŸš€ [ROADMAP.md](./ROADMAP.md): í–¥í›„ ê°œë°œ ê³„íš

EvalVaultë¥¼ ì‚¬ìš©í•´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤!
