## ì œ4ë¶€: ì£¼ìš” ì»´í¬ë„ŒíŠ¸ ìƒì„¸

### 4.1 ë„ë©”ì¸ ì—”í‹°í‹° (Domain Entities)

### 4.1.1 Dataset ì—”í‹°í‹°

**Whitepaper Reference**:
- Section 4.1: ë„ë©”ì¸ ì—”í‹°í‹° - Dataset ì—”í‹°í‹°
- Section 3.1: í‰ê°€ ì‹¤í–‰ íë¦„ - ë°ì´í„°ì…‹ ë¡œë“œ ë‹¨ê³„

#### 4.1.1.1 ê°œìš”

`Dataset` ì—”í‹°í‹°ëŠ” í‰ê°€ìš© ë°ì´í„°ì…‹ì„ í‘œí˜„í•˜ëŠ” Rich Domain Modelì…ë‹ˆë‹¤. ë°ì´í„°ì…‹ì˜ ë©”íƒ€ë°ì´í„°(name, version), í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ëª©ë¡, ê·¸ë¦¬ê³  ë©”íŠ¸ë¦­ë³„ ì„ê³„ê°’(thresholds)ì„ í¬í•¨í•©ë‹ˆë‹¤.

#### 4.1.1.2 ì •ì˜

```python
# src/evalvault/domain/entities/dataset.py
@dataclass
class Dataset:
    """í‰ê°€ìš© ë°ì´í„°ì…‹.

    Whitepaper Reference:
        - Section 4.1: ë„ë©”ì¸ ì—”í‹°í‹° - Dataset ì—”í‹°í‹°
        - Section 3.1: í‰ê°€ ì‹¤í–‰ íë¦„ - ë°ì´í„°ì…‹ ë¡œë“œ ë‹¨ê³„

    ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™:
        - ë¶ˆë³€ì„± ë³´ì¥ (@frozen=Trueë¡œ ì œì–´)
        - ì„ê³„ê°’ ì¡°íšŒ ë©”ì„œë“œ ì œê³µ
        - ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ìº¡ìŠí™”

    Last Updated: 2026-01-10
    """

    name: str
    version: str
    test_cases: list[TestCase]
    thresholds: dict[str, float] = field(default_factory=dict)

    # ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™: ì„ê³„ê°’ ì¡°íšŒ
    def get_threshold(self, metric_name: str, default: float = 0.7) -> float:
        """ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™: ë©”íŠ¸ë¦­ë³„ ì„ê³„ê°’ ì¡°íšŒ

        Args:
            metric_name: ë©”íŠ¸ë¦­ ì´ë¦„
            default: ê¸°ë³¸ ì„ê³„ê°’

        Returns:
            ë“±ë¡ëœ ì„ê³„ê°’, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ë°˜í™˜
        """
        return self.thresholds.get(metric_name, default)

    # ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™: ë¶ˆë³€ì„± ë³´ì¥
    def with_threshold(self, metric_name: str, value: float) -> 'Dataset':
        """ìƒˆë¡œìš´ ì„ê³„ê°’ì„ ê°€ì§„ ë³µì‚¬ë³¸ ë°˜í™˜

        Args:
            metric_name: ë©”íŠ¸ë¦­ ì´ë¦„
            value: ì„ê³„ê°’

        Returns:
            ìƒˆë¡œìš´ ì„ê³„ê°’ì„ ê°€ì§„ Dataset ë³µì‚¬ë³¸
        """
        new_thresholds = {**self.thresholds, metric_name: value}
        return dataclasses.replace(self, thresholds=new_thresholds)
```

#### 4.1.1.3 ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™

| ê·œì¹™ | ì„¤ëª… | êµ¬í˜„ |
|------|------|------|
| **ë¶ˆë³€ì„± ë³´ì¥** | Dataset ìƒì„± í›„ ë¶ˆë³€ | `@dataclass(frozen=True)` (ì„¹ì…˜ 4.1.2 ì°¸ì¡°) |
| **ì„ê³„ê°’ ê´€ë¦¬** | ë©”íŠ¸ë¦­ë³„ ì„ê³„ê°’ ì¡°íšŒ/ì—…ë°ì´íŠ¸ | `get_threshold()`, `with_threshold()` ë©”ì„œë“œ |
| **ê²€ì¦** | test_casesê°€ ìµœì†Œ 1ê°œ ì´ìƒ | ìƒì„±ì ê²€ì¦ ë¡œì§ (ì„¹ì…˜ 4.1.2ì—ëŠ” ìƒëµë¨) |

#### 4.1.1.4 ì‚¬ìš©ë²•

```python
# ë°ì´í„°ì…‹ ìƒì„±
dataset = Dataset(
    name="insurance-qa",
    version="1.0.0",
    test_cases=[
        TestCase(
            id="tc-001",
            question="ë³´ì¥ê¸ˆì•¡ì€ ì–¼ë§ˆì¸ê°€ìš”?",
            answer="ë³´ì¥ê¸ˆì•¡ì€ 1ì–µì›ì…ë‹ˆë‹¤.",
            contexts=["ë³´ì¥ê¸ˆì•¡ì€ 1ì–µì›ì…ë‹ˆë‹¤."],
            ground_truth="1ì–µì›",
        ),
    ],
    thresholds={
        "faithfulness": 0.8,
        "answer_relevancy": 0.7,
    },
)

# ì„ê³„ê°’ ì¡°íšŒ
faithfulness_threshold = dataset.get_threshold("faithfulness")  # 0.8
answer_relevancy_threshold = dataset.get_threshold("answer_relevancy")  # 0.7
unknown_threshold = dataset.get_threshold("unknown", 0.7)  # 0.7 (ê¸°ë³¸ê°’)

# ë¶ˆë³€ì„± ë³´ì¥: ìˆ˜ì • ì‹œ ìƒˆ ê°ì²´ ë°˜í™˜
updated_dataset = dataset.with_threshold("faithfulness", 0.85)  # ìƒˆ Dataset ê°ì²´
```

#### 4.1.1.5 ì „ë¬¸ê°€ ê´€ì  ì ìš©

**[ì¸ì§€ì‹¬ë¦¬í•™ì ê´€ì ]**
- **ì ìš© ì›ì¹™**: ë¶ˆë³€ì„± ë³´ì¥ìœ¼ë¡œ ì¸ì§€ ë¶€í•˜ ê°ì†Œ (ë°ì´í„° ë³€ê²½ ì—¬ëŸ¬ ê³³ í™•ì¸ í•„ìš” ì—†ìŒ)
- **ì‹¤ì œ êµ¬í˜„**: `@dataclass(frozen=True)`ì™€ ë³µì‚¬ë³¸ ë°˜í™˜ ë©”ì„œë“œë¡œ ë¶ˆë³€ì„± ë³´ì¥

**[ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œ ì „ë¬¸ê°€ ê´€ì ]**
- **ì ìš© ì›ì¹™**: SOLID ë‹¨ì¼ ì±…ì„ ì›ì¹™ (SRP) ì ìš©
- **ì‹¤ì œ êµ¬í˜„**:
  - ë‹¨ì¼ ì±…ì„: ë°ì´í„°ì…‹ ê´€ë¦¬ë§Œ ë‹´ë‹¹
  - ì¸í„°í˜ì´ìŠ¤ ë¶„ë¦¬: ì™¸ë¶€ ì–´ëŒ‘í„°ì™€ ë…ë¦½ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥

**[ì•„í‚¤í…íŠ¸ ê´€ì ]**
- **ì ìš© ì›ì¹™**: Rich Domain Model (ë¶ˆë³€ ê°ì²´ + ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ìº¡ìŠí™”)
- **ì‹¤ì œ êµ¬í˜„**: `@dataclass`ë¡œ ë¶ˆë³€ì„± ë³´ì¥, ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì„ ë©”ì„œë“œë¡œ ìº¡ìŠí™”

### 4.1.2 EvaluationRun ì—”í‹°í‹°

**Whitepaper Reference**:
- Section 4.1: ë„ë©”ì¸ ì—”í‹°í‹° - EvaluationRun ì—”í‹°í‹°
- Section 3.1: í‰ê°€ ì‹¤í–‰ íë¦„ - ê²°ê³¼ ì§‘ê³„ ë‹¨ê³„

#### 4.1.2.1 ê°œìš”

`EvaluationRun` ì—”í‹°í‹°ëŠ” ì „ì²´ í‰ê°€ ì‹¤í–‰ ê²°ê³¼ë¥¼ í‘œí˜„í•©ë‹ˆë‹¤. ê° í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ì— ëŒ€í•œ ê²°ê³¼(TestCaseResult), í‰ê°€ëœ ë©”íŠ¸ë¦­ ëª©ë¡, í†µê³¼ìœ¨(pass_rate) ë“± í†µê³„ ì •ë³´ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.

#### 4.1.2.2 ì •ì˜

```python
# src/evalvault/domain/entities/result.py
@dataclass
class EvaluationRun:
    """ì „ì²´ í‰ê°€ ì‹¤í–‰ ê²°ê³¼.

    Whitepaper Reference:
        - Section 4.1: ë„ë©”ì¸ ì—”í‹°í‹° - EvaluationRun ì—”í‹°í‹°
        - Section 3.1: í‰ê°€ ì‹¤í–‰ íë¦„ - ê²°ê³¼ ì§‘ê³„ ë‹¨ê³„

    ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™:
        - ê²°ê³¼ ì§‘ê³„ ë° í†µê³„ ê³„ì‚°
        - í†µê³¼/ì‹¤íŒ¨ íŒì •
        - ë©”íŠ¸ë¦­ í‰ê·  ì ìˆ˜ ê³„ì‚°

    Last Updated: 2026-01-10
    """

    run_id: str
    dataset_name: str
    model_name: str
    results: list[TestCaseResult]
    metrics_evaluated: list[str]
    thresholds: dict[str, float]

    # ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™: í†µê³¼ìœ¨ ê³„ì‚°
    @property
    def passed_test_cases(self) -> int:
        """ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™: í†µê³¼í•œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìˆ˜"""
        return sum(1 for r in self.results if r.passed)

    @property
    def total_test_cases(self) -> int:
        """ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™: ì „ì²´ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìˆ˜"""
        return len(self.results)

    @property
    def pass_rate(self) -> float:
        """ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™: í†µê³¼ìœ¨ ê³„ì‚°

        Returns:
            í†µê³¼ìœ¨ (0.0 ~ 1.0)
        """
        if not self.results:
            return 0.0
        return self.passed_test_cases / self.total_test_cases

    # ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™: ë©”íŠ¸ë¦­ í‰ê·  ì ìˆ˜ ê³„ì‚°
    def get_avg_score(self, metric_name: str) -> float | None:
        """ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™: ë©”íŠ¸ë¦­ë³„ í‰ê·  ì ìˆ˜ ê³„ì‚°

        Args:
            metric_name: ë©”íŠ¸ë¦­ ì´ë¦„

        Returns:
            í‰ê·  ì ìˆ˜ (í•´ë‹¹ ë©”íŠ¸ë¦­ì´ ì—†ìœ¼ë©´ None)
        """
        scores = [
            r.get_metric(metric_name).score
            for r in self.results
            if r.get_metric(metric_name) is not None
        ]
        return sum(scores) / len(scores) if scores else None

    # ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™: ë©”íŠ¸ë¦­ë³„ í†µê³¼/ì‹¤íŒ¨ ì¼€ì´ìŠ¤ í•„í„°ë§
    def get_failed_test_cases(self, metric_name: str) -> list[TestCaseResult]:
        """ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™: ë©”íŠ¸ë¦­ë³„ ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ í•„í„°ë§

        Args:
            metric_name: ë©”íŠ¸ë¦­ ì´ë¦„

        Returns:
            ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ëª©ë¡
        """
        return [
            r for r in self.results
            if r.get_metric(metric_name) and not r.get_metric(metric_name).passed
        ]
```

#### 4.1.2.3 ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™

| ê·œì¹™ | ì„¤ëª… | êµ¬í˜„ |
|------|------|------|
| **í†µê³¼ìœ¨ ê³„ì‚°** | í†µê³¼í•œ ì¼€ì´ìŠ¤ / ì „ì²´ ì¼€ì´ìŠ¤ | `@property pass_rate` |
| **ë©”íŠ¸ë¦­ í‰ê· ** | ë©”íŠ¸ë¦­ë³„ í‰ê·  ì ìˆ˜ ê³„ì‚° | `get_avg_score()` ë©”ì„œë“œ |
| **í•„í„°ë§** | ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ë“± ì¡°ê±´ë¶€ í•„í„°ë§ | `get_failed_test_cases()` ë©”ì„œë“œ |

#### 4.1.2.4 ì‚¬ìš©ë²•

```python
# í‰ê°€ ì‹¤í–‰ ê²°ê³¼ ìƒì„±
run = EvaluationRun(
    run_id="run-abc123",
    dataset_name="insurance-qa",
    model_name="gpt-4o-mini",
    results=[
        TestCaseResult(
            test_case_id="tc-001",
            question="ë³´ì¥ê¸ˆì•¡ì€ ì–¼ë§ˆì¸ê°€ìš”?",
            answer="ë³´ì¥ê¸ˆì•¡ì€ 1ì–µì›ì…ë‹ˆë‹¤.",
            contexts=["ë³´ì¥ê¸ˆì•¡ì€ 1ì–µì›ì…ë‹ˆë‹¤."],
            metrics=[
                MetricScore(
                    name="faithfulness",
                    score=0.9,
                    threshold=0.8,
                    passed=True,
                ),
                MetricScore(
                    name="answer_relevancy",
                    score=0.75,
                    threshold=0.7,
                    passed=True,
                ),
            ],
        ),
        # ... ë” ë§ì€ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ê²°ê³¼
    ],
    metrics_evaluated=["faithfulness", "answer_relevancy"],
    thresholds={"faithfulness": 0.8, "answer_relevancy": 0.7},
)

# í†µê³¼ìœ¨ ê³„ì‚°
print(f"í†µê³¼ìœ¨: {run.pass_rate:.2%}")  # ì˜ˆ: 85.0%

# ë©”íŠ¸ë¦­ í‰ê·  ì ìˆ˜ ê³„ì‚°
faithfulness_avg = run.get_avg_score("faithfulness")  # ì˜ˆ: 0.85
answer_relevancy_avg = run.get_avg_score("answer_relevancy")  # ì˜ˆ: 0.78

# ì‹¤íŒ¨ ì¼€ì´ìŠ¤ í•„í„°ë§
failed_faithfulness = run.get_failed_test_cases("faithfulness")  # faithfulness ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ëª©ë¡
```

#### 4.1.2.5 ì „ë¬¸ê°€ ê´€ì  ì ìš©

**[ì •ë³´ê³µí•™ ì „ë¬¸ê°€ ê´€ì ]**
- **ì ìš© ì›ì¹™**: ì •ë³´ ì•„í‚¤í…ì²˜ (ê³„ì¸µ êµ¬ì¡°ì™€ ì§‘ê³„)
- **ì‹¤ì œ êµ¬í˜„**:
  - Run â†’ Test Case â†’ Metric ê³„ì¸µ êµ¬ì¡°
  - ì§‘ê³„ ë©”ì„œë“œë¡œ í†µê³„ ì •ë³´ ì œê³µ

**[ì¸ì§€ì‹¬ë¦¬í•™ì ê´€ì ]**
- **ì ìš© ì›ì¹™**: ì ì§„ì  ì •ë³´ ê³µê°œ (Progressive Disclosure)
- **ì‹¤ì œ êµ¬í˜„**:
  - `pass_rate` í”„ë¡œí¼í‹°ë¡œ ìš”ì•½ ì •ë³´ ì œê³µ
  - `get_avg_score()` ë“± ë©”ì„œë“œë¡œ ìƒì„¸ ì •ë³´ ì œê³µ

**[UI/UX ì „ë¬¸ê°€ ê´€ì ]**
- **ì ìš© ì›ì¹™**: í”¼ë“œë°± ì œê³µ (í†µê³¼/ì‹¤íŒ¨ ì—¬ë¶€ ëª…í™•)
- **ì‹¤ì œ êµ¬í˜„**: `passed` ì†ì„±ìœ¼ë¡œ ê° ë©”íŠ¸ë¦­ ì ìˆ˜ë³„ í†µê³¼ ì—¬ë¶€ ëª…í™•íˆ í‘œì‹œ

### 4.2 ë„ë©”ì¸ ì„œë¹„ìŠ¤ (Domain Services)

### 4.2.1 RagasEvaluator ì„œë¹„ìŠ¤

**Whitepaper Reference**:
- Section 4.2: ë„ë©”ì¸ ì„œë¹„ìŠ¤ - RagasEvaluator ì„œë¹„ìŠ¤
- Section 3.1: í‰ê°€ ì‹¤í–‰ íë¦„ - í‰ê°€ ì‹¤í–‰ ë‹¨ê³„
- Section 2.3: ì˜ì¡´ì„± ê´€ë¦¬

#### 4.2.1.1 ê°œìš”

`RagasEvaluator`ëŠ” Ragas ê¸°ë°˜ RAG í‰ê°€ë¥¼ ì‹¤í–‰í•˜ëŠ” í•µì‹¬ ë„ë©”ì¸ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤. Ragas ë©”íŠ¸ë¦­ê³¼ ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ì„ ì‹¤í–‰í•˜ê³ , ê²°ê³¼ë¥¼ ì§‘ê³„í•˜ë©°, ì„ê³„ê°’ì„ íŒì •í•©ë‹ˆë‹¤.

#### 4.2.1.2 ì •ì˜

```python
# src/evalvault/domain/services/evaluator.py
class RagasEvaluator:
    """Ragas ê¸°ë°˜ RAG í‰ê°€ ì„œë¹„ìŠ¤.

    Whitepaper Reference:
        - Section 4.2: ë„ë©”ì¸ ì„œë¹„ìŠ¤ - RagasEvaluator ì„œë¹„ìŠ¤
        - Section 3.1: í‰ê°€ ì‹¤í–‰ íë¦„ - í‰ê°€ ì‹¤í–‰ ë‹¨ê³„

    ì±…ì„:
        - Ragas ë©”íŠ¸ë¦­ ì‹¤í–‰
        - ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ ì‹¤í–‰
        - ê²°ê³¼ ì§‘ê³„ ë° ì„ê³„ê°’ íŒì •
        - í† í° ì‚¬ìš©ëŸ‰ ë° ë¹„ìš© ì¶”ì 

    ì˜ì¡´ì„±:
        - LLMPort: LLM ì¶”ìƒí™” (êµ¬ì²´ì ì¸ LLM êµ¬í˜„ì— ì˜ì¡´í•˜ì§€ ì•ŠìŒ)
        - Dataset: ë„ë©”ì¸ ì—”í‹°í‹°

    Last Updated: 2026-01-10
    """

    # ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ ë§¤í•‘
    CUSTOM_METRIC_MAP: dict[str, type] = {
        "insurance_term_accuracy": InsuranceTermAccuracy,
        "entity_preservation": EntityPreservation,
    }

    def __init__(self, llm: LLMPort):
        """ì´ˆê¸°í™”

        Args:
            llm: LLM í¬íŠ¸ ì¸í„°í˜ì´ìŠ¤
        """
        self._llm = llm

    async def evaluate(
        self,
        dataset: Dataset,
        metrics: list[str],
        thresholds: dict[str, float] | None = None,
        parallel: bool = True,
        batch_size: int = 10,
    ) -> EvaluationRun:
        """í‰ê°€ ì‹¤í–‰ - í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§

        Args:
            dataset: í‰ê°€ìš© ë°ì´í„°ì…‹
            metrics: ì‹¤í–‰í•  ë©”íŠ¸ë¦­ ëª©ë¡
            thresholds: ë©”íŠ¸ë¦­ë³„ ì„ê³„ê°’ (Noneì´ë©´ ë°ì´í„°ì…‹ ê¸°ë³¸ê°’ ì‚¬ìš©)
            parallel: ë³‘ë ¬ ì‹¤í–‰ ì—¬ë¶€
            batch_size: ë°°ì¹˜ í¬ê¸°

        Returns:
            EvaluationRun: í‰ê°€ ì‹¤í–‰ ê²°ê³¼

        ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™:
            1. ì„ê³„ê°’ í•´ì„
            2. Ragas ë©”íŠ¸ë¦­ ì‹¤í–‰
            3. ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ ì‹¤í–‰
            4. ê²°ê³¼ ì§‘ê³„
        """
        # 1. ì„ê³„ê°’ í•´ì„ (ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™)
        resolved_thresholds = self._resolve_thresholds(dataset, metrics, thresholds)

        # 2. í‰ê°€ ì‹¤í–‰ (Ragas + ì»¤ìŠ¤í…€)
        eval_results = await self._evaluate_with_ragas(dataset, metrics, parallel, batch_size)

        # 3. ê²°ê³¼ ì§‘ê³„ (ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§)
        run = self._aggregate_results(dataset, metrics, eval_results, resolved_thresholds)

        return run

    def _resolve_thresholds(
        self,
        dataset: Dataset,
        metrics: list[str],
        thresholds: dict[str, float] | None,
    ) -> dict[str, float]:
        """ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™: ì„ê³„ê°’ í•´ì„

        ìš°ì„ ìˆœìœ„:
        1. ëª…ì‹œëœ thresholds (í•¨ìˆ˜ ì¸ì)
        2. ë°ì´í„°ì…‹ thresholds
        3. ê¸°ë³¸ê°’ (0.7)
        """
        resolved = {}

        for metric in metrics:
            # ëª…ì‹œëœ thresholds ìš°ì„ 
            if thresholds and metric in thresholds:
                resolved[metric] = thresholds[metric]
            # ë°ì´í„°ì…‹ thresholds ë‹¤ìŒ
            elif metric in dataset.thresholds:
                resolved[metric] = dataset.thresholds[metric]
            # ê¸°ë³¸ê°’
            else:
                resolved[metric] = 0.7

        return resolved

    async def _evaluate_with_ragas(
        self,
        dataset: Dataset,
        metrics: list[str],
        parallel: bool = True,
        batch_size: int = 10,
    ) -> dict[str, list[MetricScore]]:
        """Ragas ë©”íŠ¸ë¦­ ì‹¤í–‰"""
        # Ragas í‰ê°€ ë¡œì§...
        pass  # ì‹¤ì œ êµ¬í˜„ì€ ì½”ë“œì—ì„œ í™•ì¸ ê°€ëŠ¥

    def _aggregate_results(
        self,
        dataset: Dataset,
        metrics: list[str],
        eval_results: dict[str, list[MetricScore]],
        thresholds: dict[str, float],
    ) -> EvaluationRun:
        """ê²°ê³¼ ì§‘ê³„ (ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§)"""
        results = []

        # ê° í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë³„ ê²°ê³¼ ìƒì„±
        for i, test_case in enumerate(dataset.test_cases):
            metric_scores = []

            # ê° ë©”íŠ¸ë¦­ë³„ ì ìˆ˜ ì¶”ì¶œ
            for metric in metrics:
                if metric in eval_results:
                    metric_score = MetricScore(
                        name=metric,
                        score=eval_results[metric][i].score,
                        threshold=thresholds[metric],
                        passed=eval_results[metric][i].score >= thresholds[metric],
                    )
                    metric_scores.append(metric_score)

            # TestCaseResult ìƒì„±
            result = TestCaseResult(
                test_case_id=test_case.id,
                question=test_case.question,
                answer=test_case.answer,
                contexts=test_case.contexts,
                ground_truth=test_case.ground_truth,
                metrics=metric_scores,
            )
            results.append(result)

        # EvaluationRun ìƒì„±
        return EvaluationRun(
            run_id=f"run-{uuid.uuid4()[:8]}",
            dataset_name=dataset.name,
            model_name=self._llm.get_model_name(),
            results=results,
            metrics_evaluated=metrics,
            thresholds=thresholds,
        )
```

#### 4.2.1.3 ì˜ì¡´ì„±

```python
# ì˜ì¡´ì„± ì£¼ì… (ìƒì„±ì ì£¼ì…)
class RagasEvaluator:
    def __init__(self, llm: LLMPort):
        """
        ì˜ì¡´ì„± ë°©í–¥: í¬íŠ¸ â†’ ë„ë©”ì¸

        LLMPort: ì¶”ìƒí™”ëœ ì¸í„°í˜ì´ìŠ¤
        - êµ¬ì²´ì ì¸ LLM êµ¬í˜„(OpenAI, Anthropic ë“±)ì— ì˜ì¡´í•˜ì§€ ì•ŠìŒ
        - í…ŒìŠ¤íŠ¸ ì‹œ MockLLMAdapterë¡œ ì‰½ê²Œ êµì²´ ê°€ëŠ¥
        """
        self._llm = llm
```

#### 4.2.1.4 ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™

| ê·œì¹™ | ì„¤ëª… | êµ¬í˜„ |
|------|------|------|
| **ì„ê³„ê°’ ìš°ì„ ìˆœìœ„** | ëª…ì‹œ â†’ ë°ì´í„°ì…‹ â†’ ê¸°ë³¸ê°’ | `_resolve_thresholds()` ë©”ì„œë“œ |
| **ë©”íŠ¸ë¦­ ì¡°í•©** | Ragas + ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ ì‹¤í–‰ | `evaluate()` ë©”ì„œë“œ |
| **ê²°ê³¼ ì§‘ê³„** | í†µê³¼/ì‹¤íŒ¨ íŒì •, í‰ê·  ì ìˆ˜ ê³„ì‚° | `_aggregate_results()` ë©”ì„œë“œ |

#### 4.2.1.5 ì‚¬ìš©ë²•

```python
# LLM ì–´ëŒ‘í„° ìƒì„±
settings = Settings()
llm = get_llm_adapter(settings, "dev")  # OpenAIAdapter ë“± LLMPort êµ¬í˜„ì²´

# í‰ê°€ì ìƒì„±
evaluator = RagasEvaluator(llm=llm)

# ë°ì´í„°ì…‹ ë¡œë“œ
loader = get_loader("tests/fixtures/insurance_qa.json")
dataset = loader.load("tests/fixtures/insurance_qa.json")

# í‰ê°€ ì‹¤í–‰
run = await evaluator.evaluate(
    dataset=dataset,
    metrics=["faithfulness", "answer_relevancy"],
    thresholds={"faithfulness": 0.9},  # ëª…ì‹œëœ ì„ê³„ê°’
    parallel=True,
    batch_size=10,
)

# ê²°ê³¼ í™•ì¸
print(f"ì‹¤í–‰ ID: {run.run_id}")
print(f"í†µê³¼ìœ¨: {run.pass_rate:.2%}")
print(f"Faithfulness í‰ê· : {run.get_avg_score('faithfulness'):.3f}")
print(f"Answer Relevancy í‰ê· : {run.get_avg_score('answer_relevancy'):.3f}")
```

#### 4.2.1.6 ì „ë¬¸ê°€ ê´€ì  ì ìš©

**[SOLID ë‹¨ì¼ ì±…ì„ ì›ì¹™ (SRP) ê´€ì ]**
- **ì ìš© ì›ì¹™**: í‰ê°€ ì‹¤í–‰ë§Œ ë‹´ë‹¹
- **ì‹¤ì œ êµ¬í˜„**:
  - Ragas ë©”íŠ¸ë¦­ ì‹¤í–‰, ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ ì‹¤í–‰, ê²°ê³¼ ì§‘ê³„ë§Œ ë‹´ë‹¹
  - ë°ì´í„°ì…‹ ë¡œë“œ, ì €ì¥ì†Œ ì €ì¥ì€ ë‹¤ë¥¸ ì„œë¹„ìŠ¤ ì±…ì„

**[ì˜ì¡´ì„± ì—­ì „ ì›ì¹™ (DIP) ê´€ì ]**
- **ì ìš© ì›ì¹™**: ì¶”ìƒí™”(LLMPort)ì— ì˜ì¡´
- **ì‹¤ì œ êµ¬í˜„**:
  - ìƒì„±ì ì£¼ì…ìœ¼ë¡œ LLMPort ì¶”ìƒí™”ì— ì˜ì¡´
  - êµ¬ì²´ì ì¸ OpenAIAdapter, AnthropicAdapter ë“±ì— ì˜ì¡´í•˜ì§€ ì•ŠìŒ

**[í…ŒìŠ¤íŠ¸ ê°€ëŠ¥ì„± ê´€ì ]**
- **ì ìš© ì›ì¹™**: í¬íŠ¸ ì¸í„°í˜ì´ìŠ¤ë¥¼ í†µí•œ ëª¨í‚¹
- **ì‹¤ì œ êµ¬í˜„**:
  - í…ŒìŠ¤íŠ¸ ì‹œ MockLLMAdapterë¡œ êµì²´ ê°€ëŠ¥
  - ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ì—ì„œ ì™¸ë¶€ API í˜¸ì¶œ ë¶ˆí•„ìš”

### 4.2.2 ExperimentManager ì„œë¹„ìŠ¤

**Whitepaper Reference**:
- Section 4.2: ë„ë©”ì¸ ì„œë¹„ìŠ¤ - ExperimentManager ì„œë¹„ìŠ¤
- Section 3.2: ì‹¤í—˜ ê´€ë¦¬ íë¦„

#### 4.2.2.1 ê°œìš”

`ExperimentManager`ëŠ” A/B í…ŒìŠ¤íŠ¸ ì‹¤í—˜ì„ ê´€ë¦¬í•˜ëŠ” ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤. ì—¬ëŸ¬ í‰ê°€ ì‹¤í–‰ì„ ê·¸ë£¹ìœ¼ë¡œ ë¬¶ì–´ ë¹„êµí•˜ê³ , ìµœì ì˜ ì„¤ì •ì„ ì‹ë³„í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.

#### 4.2.2.2 ì •ì˜

```python
# src/evalvault/domain/services/experiment_manager.py
from dataclasses import dataclass, field
from typing import Dict, List

@dataclass
class ExperimentGroup:
    """ì‹¤í—˜ ê·¸ë£¹"""
    group_id: str
    name: str
    description: str = ""
    baseline: bool = False
    run_ids: List[str] = field(default_factory=list)

@dataclass
class MetricComparison:
    """ë©”íŠ¸ë¦­ ë¹„êµ ê²°ê³¼"""
    metric_name: str
    group_scores: Dict[str, float]
    best_group: str
    improvement: float

@dataclass
class Experiment:
    """ì‹¤í—˜"""
    experiment_id: str
    name: str
    description: str = ""
    metrics_to_compare: List[str] = field(default_factory=list)
    groups: Dict[str, ExperimentGroup] = field(default_factory=dict)
    created_at: datetime.datetime = field(default_factory=datetime.datetime.now)

class ExperimentManager:
    """ì‹¤í—˜ ê´€ë¦¬ ì„œë¹„ìŠ¤.

    Whitepaper Reference:
        - Section 4.2: ë„ë©”ì¸ ì„œë¹„ìŠ¤ - ExperimentManager ì„œë¹„ìŠ¤
        - Section 3.2: ì‹¤í—˜ ê´€ë¦¬ íë¦„

    ì±…ì„:
        - ì‹¤í—˜ ìƒì„± ë° ê´€ë¦¬
        - ê·¸ë£¹ ì¶”ê°€ ë° ê´€ë¦¬
        - ê·¸ë£¹ ê°„ ë©”íŠ¸ë¦­ ë¹„êµ

    ì˜ì¡´ì„±:
        - StoragePort: ì €ì¥ì†Œ ì¶”ìƒí™”

    Last Updated: 2026-01-10
    """

    def __init__(self, storage: StoragePort):
        """ì´ˆê¸°í™”

        Args:
            storage: ì €ì¥ì†Œ í¬íŠ¸ ì¸í„°í˜ì´ìŠ¤
        """
        self._storage = storage
        self._experiments: Dict[str, Experiment] = {}

    def create_experiment(
        self,
        experiment_id: str,
        name: str,
        description: str,
        metrics_to_compare: List[str],
    ) -> Experiment:
        """ì‹¤í—˜ ìƒì„± (ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§)

        Args:
            experiment_id: ì‹¤í—˜ ID
            name: ì‹¤í—˜ ì´ë¦„
            description: ì‹¤í—˜ ì„¤ëª…
            metrics_to_compare: ë¹„êµí•  ë©”íŠ¸ë¦­ ëª©ë¡

        Returns:
            Experiment: ìƒì„±ëœ ì‹¤í—˜

        ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™:
            - Experiment ì—”í‹°í‹° ìƒì„±
            - ì¤‘ë³µ ID ê²€ì‚¬
        """
        if experiment_id in self._experiments:
            raise ValueError(f"ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ì‹¤í—˜ ID: {experiment_id}")

        experiment = Experiment(
            experiment_id=experiment_id,
            name=name,
            description=description,
            metrics_to_compare=metrics_to_compare,
            groups={},
        )

        self._experiments[experiment_id] = experiment
        return experiment

    def add_group(
        self,
        experiment_id: str,
        group_id: str,
        name: str,
        baseline: bool = False,
    ) -> ExperimentGroup:
        """ê·¸ë£¹ ì¶”ê°€ (ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§)

        Args:
            experiment_id: ì‹¤í—˜ ID
            group_id: ê·¸ë£¹ ID
            name: ê·¸ë£¹ ì´ë¦„
            baseline: ê¸°ì¤€ ê·¸ë£¹ ì—¬ë¶€

        Returns:
            ExperimentGroup: ìƒì„±ëœ ê·¸ë£¹

        ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™:
            - ì‹¤í—˜ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            - ê·¸ë£¹ ì—”í‹°í‹° ìƒì„±
        """
        experiment = self.get_experiment(experiment_id)

        if group_id in experiment.groups:
            raise ValueError(f"ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê·¸ë£¹ ID: {group_id}")

        group = ExperimentGroup(
            group_id=group_id,
            name=name,
            baseline=baseline,
            run_ids=[],
        )

        experiment.groups[group_id] = group
        return group

    def add_run_to_group(
        self,
        experiment_id: str,
        group_id: str,
        run_id: str,
    ):
        """ê·¸ë£¹ì— ì‹¤í–‰ ì¶”ê°€ (ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§)

        Args:
            experiment_id: ì‹¤í—˜ ID
            group_id: ê·¸ë£¹ ID
            run_id: ì‹¤í–‰ ID

        ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™:
            - ì‹¤í—˜ê³¼ ê·¸ë£¹ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            - ì¤‘ë³µ ì‹¤í–‰ ID ê²€ì‚¬
        """
        experiment = self.get_experiment(experiment_id)
        group = experiment.groups[group_id]

        if run_id in group.run_ids:
            raise ValueError(f"ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ì‹¤í–‰ ID: {run_id}")

        group.run_ids.append(run_id)

    def compare_groups(self, experiment_id: str) -> List[MetricComparison]:
        """ê·¸ë£¹ ê°„ ë©”íŠ¸ë¦­ ë¹„êµ (ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§)

        Args:
            experiment_id: ì‹¤í—˜ ID

        Returns:
            List[MetricComparison]: ë©”íŠ¸ë¦­ë³„ ë¹„êµ ê²°ê³¼

        ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™:
            - ê° ê·¸ë£¹ì˜ run ë°ì´í„° ìˆ˜ì§‘
            - ë©”íŠ¸ë¦­ë³„ í‰ê·  ì ìˆ˜ ê³„ì‚°
            - ìµœê³  ê·¸ë£¹ ë° ê°œì„ ìœ¨ ê³„ì‚°
        """
        experiment = self.get_experiment(experiment_id)

        # ê° ê·¸ë£¹ì˜ run ë°ì´í„° ìˆ˜ì§‘
        group_runs = self._collect_group_runs(experiment)

        # ë©”íŠ¸ë¦­ë³„ ë¹„êµ (ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™)
        comparisons = []
        for metric in experiment.metrics_to_compare:
            group_scores = self._calculate_group_scores(group_runs, metric)
            best_group = max(group_scores, key=group_scores.get)
            improvement = self._calculate_improvement(group_scores, best_group)

            comparisons.append(MetricComparison(
                metric_name=metric,
                group_scores=group_scores,
                best_group=best_group,
                improvement=improvement,
            ))

        return comparisons

    def _collect_group_runs(
        self,
        experiment: Experiment,
    ) -> Dict[str, Dict[str, EvaluationRun]]:
        """ê° ê·¸ë£¹ì˜ run ë°ì´í„° ìˆ˜ì§‘ (ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§)"""
        group_runs = {}

        for group_id, group in experiment.groups.items():
            runs = {}
            for run_id in group.run_ids:
                # StoragePortì—ì„œ run ë°ì´í„° ì¡°íšŒ
                run = self._storage.get_run(run_id)
                if run:
                    runs[run_id] = run

            group_runs[group_id] = runs

        return group_runs

    def _calculate_group_scores(
        self,
        group_runs: Dict[str, Dict[str, EvaluationRun]],
        metric: str,
    ) -> Dict[str, float]:
        """ë©”íŠ¸ë¦­ë³„ ê·¸ë£¹ í‰ê·  ì ìˆ˜ ê³„ì‚° (ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§)"""
        group_scores = {}

        for group_id, runs in group_runs.items():
            if not runs:
                continue

            scores = [
                run.get_avg_score(metric)
                for run in runs.values()
                if run.get_avg_score(metric) is not None
            ]

            if scores:
                avg_score = sum(scores) / len(scores)
                group_scores[group_id] = avg_score

        return group_scores

    def _calculate_improvement(
        self,
        group_scores: Dict[str, float],
        best_group: str,
    ) -> float:
        """ê°œì„ ìœ¨ ê³„ì‚° (ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§)"""
        best_score = group_scores[best_group]

        # ê¸°ì¤€ ê·¸ë£¹ ì°¾ê¸°
        baseline_group = None
        for group_id, scores in group_scores.items():
            if group_id in self._experiments.values():
                if self._experiments.values().groups[group_id].baseline:
                    baseline_group = group_id
                    break

        if baseline_group and baseline_group in group_scores:
            baseline_score = group_scores[baseline_group]
            improvement = (best_score - baseline_score) / baseline_score * 100
            return improvement
        else:
            return 0.0
```

#### 4.2.2.3 ì˜ì¡´ì„±

```python
# ì˜ì¡´ì„± ì£¼ì… (ìƒì„±ì ì£¼ì…)
class ExperimentManager:
    def __init__(self, storage: StoragePort):
        """
        ì˜ì¡´ì„± ë°©í–¥: í¬íŠ¸ â†’ ë„ë©”ì¸

        StoragePort: ì¶”ìƒí™”ëœ ì €ì¥ì†Œ ì¸í„°í˜ì´ìŠ¤
        - êµ¬ì²´ì ì¸ SQLite, PostgreSQL ë“±ì— ì˜ì¡´í•˜ì§€ ì•ŠìŒ
        - í…ŒìŠ¤íŠ¸ ì‹œ MockStorageAdapterë¡œ ì‰½ê²Œ êµì²´ ê°€ëŠ¥
        """
        self._storage = storage
```

#### 4.2.2.4 ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™

| ê·œì¹™ | ì„¤ëª… | êµ¬í˜„ |
|------|------|------|
| **ì‹¤í—˜ ê´€ë¦¬** | ì‹¤í—˜ ìƒì„±, ê·¸ë£¹ ì¶”ê°€, ì‹¤í–‰ ì¶”ê°€ | `create_experiment()`, `add_group()`, `add_run_to_group()` |
| **ê·¸ë£¹ ë¹„êµ** | ë©”íŠ¸ë¦­ë³„ ê·¸ë£¹ ê°„ ë¹„êµ ë° ê°œì„ ìœ¨ ê³„ì‚° | `compare_groups()` ë©”ì„œë“œ |
| **ë°ì´í„° ê²€ì¦** | ì¤‘ë³µ ID ê²€ì‚¬, ì¡´ì¬ì„± í™•ì¸ | ìƒì„±ì/ë©”ì„œë“œì—ì„œ ê²€ì¦ ë¡œì§ |

#### 4.2.2.5 ì‚¬ìš©ë²•

```python
# ì €ì¥ì†Œ ì–´ëŒ‘í„° ìƒì„±
settings = Settings()
storage = get_storage_adapter(settings)  # SQLiteStorageAdapter ë“± StoragePort êµ¬í˜„ì²´

# ì‹¤í—˜ ê´€ë¦¬ì ìƒì„±
manager = ExperimentManager(storage=storage)

# ì‹¤í—˜ ìƒì„±
experiment = manager.create_experiment(
    experiment_id="exp-001",
    name="í”„ë¡¬í”„íŠ¸ A/B í…ŒìŠ¤íŠ¸",
    description="í”„ë¡¬í”„íŠ¸ v1 vs v2 ë¹„êµ",
    metrics_to_compare=["faithfulness", "answer_relevancy"],
)

# ê·¸ë£¹ ì¶”ê°€
group_a = manager.add_group(experiment_id="exp-001", group_id="group-a", name="Prompt v1", baseline=True)
group_b = manager.add_group(experiment_id="exp-001", group_id="group-b", name="Prompt v2", baseline=False)

# ì‹¤í–‰ ì¶”ê°€ (ê°€ì •: run-a-001, run-b-001ì´ ì €ì¥ì†Œì— ìˆë‹¤ê³  ê°€ì •)
manager.add_run_to_group(experiment_id="exp-001", group_id="group-a", run_id="run-a-001")
manager.add_run_to_group(experiment_id="exp-001", group_id="group-b", run_id="run-b-001")

# ê·¸ë£¹ ë¹„êµ
comparisons = manager.compare_groups(experiment_id="exp-001")

# ë¹„êµ ê²°ê³¼ í™•ì¸
for comp in comparisons:
    print(f"\në©”íŠ¸ë¦­: {comp.metric_name}")
    print(f"  ê·¸ë£¹ ì ìˆ˜: {comp.group_scores}")
    print(f"  ìµœê³  ê·¸ë£¹: {comp.best_group}")
    print(f"  ê°œì„ ìœ¨: {comp.improvement:.2f}%")
```

#### 4.2.2.6 ì „ë¬¸ê°€ ê´€ì  ì ìš©

**[SOLID ë‹¨ì¼ ì±…ì„ ì›ì¹™ (SRP) ê´€ì ]**
- **ì ìš© ì›ì¹™**: ì‹¤í—˜ ê´€ë¦¬ë§Œ ë‹´ë‹¹
- **ì‹¤ì œ êµ¬í˜„**:
  - ì‹¤í—˜ ìƒì„±, ê·¸ë£¹ ê´€ë¦¬, ë¹„êµ ë¡œì§ë§Œ ë‹´ë‹¹
  - ë°ì´í„°ì…‹ ë¡œë“œ, í‰ê°€ ì‹¤í–‰ì€ ë‹¤ë¥¸ ì„œë¹„ìŠ¤ ì±…ì„

**[ì •ë³´ê³µí•™ ì „ë¬¸ê°€ ê´€ì ]**
- **ì ìš© ì›ì¹™**: ì •ë³´ êµ¬ì¡°í™” (ì‹¤í—˜ â†’ ê·¸ë£¹ â†’ ë©”íŠ¸ë¦­)
- **ì‹¤ì œ êµ¬í˜„**:
  - Experiment â†’ ExperimentGroup â†’ EvaluationRun ê³„ì¸µ êµ¬ì¡°
  - MetricComparisonìœ¼ë¡œ ì •ëŸ‰ì  ë¹„êµ ê²°ê³¼ ì œê³µ

**[êµìœ¡ê³µí•™ ì „ë¬¸ê°€ ê´€ì ]**
- **ì ìš© ì›ì¹™**: A/B í…ŒìŠ¤íŠ¸ êµìœ¡ì  íš¨ê³¼
- **ì‹¤ì œ êµ¬í˜„**:
  - Baseline ê·¸ë£¹ ëª…ì‹œ
  - ê°œì„ ìœ¨ ê³„ì‚°ìœ¼ë¡œ í•™ìŠµ íš¨ê³¼ ì œê³µ
  - ëª…í™•í•œ ë¹„êµ ê²°ê³¼ë¡œ ê²°ë¡  ë„ì¶œ

### 4.3 í¬íŠ¸ ì¸í„°í˜ì´ìŠ¤ (Port Interfaces)

### 4.3.1 LLMPort

**Whitepaper Reference**:
- Section 2.3: í¬íŠ¸ ê³„ì¸µ - Outbound Ports
- Section 4.2.1: ë„ë©”ì¸ ì„œë¹„ìŠ¤ - RagasEvaluator ì„œë¹„ìŠ¤

#### 4.3.1.1 ê°œìš”

`LLMPort`ëŠ” LLM ì„œë¹„ìŠ¤ì— ëŒ€í•œ ì¶”ìƒí™”ëœ ì¸í„°í˜ì´ìŠ¤ì…ë‹ˆë‹¤. OpenAI, Anthropic, Azure, Ollama, vLLM ë“± ë‹¤ì–‘í•œ LLM ì œê³µìë¥¼ í†µì¼ëœ ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.

#### 4.3.1.2 ì •ì˜

```python
# src/evalvault/ports/outbound/llm_port.py
from abc import ABC, abstractmethod

class LLMPort(ABC):
    """LLM adapter interface for Ragas metrics evaluation.

    Whitepaper Reference:
        - Section 2.3: í¬íŠ¸ ê³„ì¸µ - Outbound Ports
        - Section 4.2.1: ë„ë©”ì¸ ì„œë¹„ìŠ¤ - RagasEvaluator ì„œë¹„ìŠ¤

    ê³„ì•½(Contract):
        - LLM ì œê³µìë¥¼ ìœ„í•œ í†µí•© ì¸í„°í˜ì´ìŠ¤
        - Ragas í˜¸í™˜ LLM ì¸ìŠ¤í„´ìŠ¤ ì œê³µ

    í™•ì¥ì (Extensibility):
        - ìƒˆë¡œìš´ LLM ì œê³µì ì¶”ê°€ ì‹œ ì´ ì¸í„°í˜ì´ìŠ¤ë§Œ êµ¬í˜„

    Last Updated: 2026-01-10
    """

    @abstractmethod
    def get_model_name(self) -> str:
        """ëª¨ë¸ ì´ë¦„ ë°˜í™˜

        Returns:
            ëª¨ë¸ ì´ë¦„ (ì˜ˆ: "gpt-4o-mini")
        """
        pass

    @abstractmethod
    def as_ragas_llm(self):
        """Ragas í˜¸í™˜ LLM ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜

        Returns:
            Ragasì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” LLM ì¸ìŠ¤í„´ìŠ¤
        """
        pass

    @abstractmethod
    def get_token_count(self) -> int:
        """í† í° ì‚¬ìš©ëŸ‰ ë°˜í™˜

        Returns:
            ì‚¬ìš©ëœ í† í° ìˆ˜ (0ì´ë©´ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
        """
        pass

    @abstractmethod
    def get_cost_estimate(self) -> float:
        """ë¹„ìš© ì¶”ì • ë°˜í™˜

        Returns:
            ì¶”ì • ë¹„ìš© (USD)
        """
        pass
```

#### 4.3.1.3 êµ¬í˜„ ì˜ˆì‹œ

```python
# OpenAI êµ¬í˜„
# src/evalvault/adapters/outbound/llm/openai_adapter.py
class OpenAIAdapter(LLMPort):
    """OpenAI LLM adapter."""

    def __init__(self, settings: Settings):
        self._settings = settings
        self._token_count = 0

        # Ragas í˜¸í™˜ LLM ìƒì„±
        from langchain_openai import ChatOpenAI
        self._ragas_llm = llm_factory(
            model=settings.openai_model,
            provider="openai",
            api_key=settings.openai_api_key,
        )

        # ë¹„ìš© ì¶”ì ì„ ìœ„í•œ wrapper
        self._llm_wrapper = TokenAwareChat(
            llm=ChatOpenAI(
                model=settings.openai_model,
                api_key=settings.openai_api_key,
            ),
            token_callback=self._update_token_count,
        )

    def get_model_name(self) -> str:
        """ëª¨ë¸ ì´ë¦„ ë°˜í™˜"""
        return self._settings.openai_model

    def as_ragas_llm(self):
        """Ragas í˜¸í™˜ LLM ë°˜í™˜"""
        return self._ragas_llm

    def get_token_count(self) -> int:
        """í† í° ì‚¬ìš©ëŸ‰ ë°˜í™˜"""
        return self._token_count

    def get_cost_estimate(self) -> float:
        """ë¹„ìš© ì¶”ì • ë°˜í™˜"""
        # OpenAI ê°€ê²©í‘œ ê¸°ë°˜ ë¹„ìš© ê³„ì‚°
        # ì…ë ¥ í† í° + ì¶œë ¥ í† í° * ê°€ê²©/1M í† í°
        return self._token_count * 0.00015  # ì˜ˆì‹œ ê°€ê²©

# Ollama êµ¬í˜„
# src/evalvault/adapters/outbound/llm/ollama_adapter.py
class OllamaAdapter(LLMPort):
    """Ollama LLM adapter."""

    def __init__(self, settings: Settings):
        self._settings = settings
        self._token_count = 0

        # Ragas í˜¸í™˜ LLM ìƒì„±
        from langchain_community.llms import OllamaLLM
        self._ragas_llm = llm_factory(
            model=settings.ollama_model,
            provider="ollama",
            base_url=settings.ollama_base_url,
        )

    def get_model_name(self) -> str:
        return self._settings.ollama_model

    def as_ragas_llm(self):
        return self._ragas_llm

    def get_token_count(self) -> int:
        return self._token_count

    def get_cost_estimate(self) -> float:
        # OllamaëŠ” ë¡œì»¬ì´ë¯€ë¡œ ë¹„ìš© 0
        return 0.0
```

#### 4.3.1.4 ì „ë¬¸ê°€ ê´€ì  ì ìš©

**[ì•„í‚¤í…íŠ¸ ê´€ì ]**
- **ì ìš© ì›ì¹™**: í¬íŠ¸ ê¸°ë°˜ ì„¤ê³„ (Ports & Adapters)
- **ì‹¤ì œ êµ¬í˜„**:
  - LLMPort ì¸í„°í˜ì´ìŠ¤ë¡œ ì¶”ìƒí™”
  - ê° ì œê³µì(Adapter)ê°€ ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„
  - ì˜ì¡´ì„± ë°©í–¥: Adapter â†’ Port â†’ Domain

**[í™•ì¥ì„± ê´€ì ]**
- **ì ìš© ì›ì¹™**: ìƒˆë¡œìš´ LLM ì¶”ê°€ ìš©ì´
- **ì‹¤ì œ êµ¬í˜„**:
  - ìƒˆë¡œìš´ LLM ì œê³µì ì¶”ê°€ ì‹œ LLMPortë§Œ êµ¬í˜„
  - ë„ë©”ì¸ ì½”ë“œ ìˆ˜ì • ì—†ìŒ (RagasEvaluatorëŠ” LLMPort ì¸í„°í˜ì´ìŠ¤ë§Œ ì‚¬ìš©)

**[í…ŒìŠ¤íŠ¸ ê°€ëŠ¥ì„± ê´€ì ]**
- **ì ìš© ì›ì¹™**: í¬íŠ¸ ì¸í„°í˜ì´ìŠ¤ë¥¼ í†µí•œ ëª¨í‚¹
- **ì‹¤ì œ êµ¬í˜„**:
  - í…ŒìŠ¤íŠ¸ ì‹œ MockLLMAdapterë¡œ êµì²´ ê°€ëŠ¥
  - ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ì—ì„œ ì‹¤ì œ LLM API í˜¸ì¶œ ë¶ˆí•„ìš”

### 4.3.2 StoragePort

**Whitepaper Reference**:
- Section 2.3: í¬íŠ¸ ê³„ì¸µ - Outbound Ports
- Section 4.2.1: ë„ë©”ì¸ ì„œë¹„ìŠ¤ - RagasEvaluator ì„œë¹„ìŠ¤

#### 4.3.2.1 ê°œìš”

`StoragePort`ëŠ” í‰ê°€ ê²°ê³¼ë¥¼ ì €ì¥í•˜ëŠ” ì¶”ìƒí™”ëœ ì¸í„°í˜ì´ìŠ¤ì…ë‹ˆë‹¤. SQLite, PostgreSQL ë“± ë‹¤ì–‘í•œ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ í†µì¼ëœ ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.

#### 4.3.2.2 ì •ì˜

```python
# src/evalvault/ports/outbound/storage_port.py
from typing import Protocol

class StoragePort(Protocol):
    """í‰ê°€ ê²°ê³¼ ì €ì¥ì„ ìœ„í•œ í¬íŠ¸ ì¸í„°í˜ì´ìŠ¤.

    Whitepaper Reference:
        - Section 2.3: í¬íŠ¸ ê³„ì¸µ - Outbound Ports
        - Section 4.2.1: ë„ë©”ì¸ ì„œë¹„ìŠ¤ - RagasEvaluator ì„œë¹„ìŠ¤

    ê³„ì•½(Contract):
        - EvaluationRun ì €ì¥ ë° ì¡°íšŒë¥¼ ìœ„í•œ í†µí•© ì¸í„°í˜ì´ìŠ¤

    í™•ì¥ì (Extensibility):
        - ìƒˆë¡œìš´ ì €ì¥ì†Œ(MySQL, MongoDB ë“±) ì¶”ê°€ ì‹œ ì´ ì¸í„°í˜ì´ìŠ¤ë§Œ êµ¬í˜„

    Last Updated: 2026-01-10
    """

    def save_run(self, run: EvaluationRun) -> str:
        """í‰ê°€ ì‹¤í–‰ ê²°ê³¼ ì €ì¥

        Args:
            run: EvaluationRun ì—”í‹°í‹°

        Returns:
            ì €ì¥ëœ run_id

        ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™:
            - EvaluationRunì„ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ì €ì¥
            - run_id ë°˜í™˜
        """
        pass

    def get_run(self, run_id: str) -> EvaluationRun:
        """ì €ì¥ëœ í‰ê°€ ì‹¤í–‰ ê²°ê³¼ ì¡°íšŒ

        Args:
            run_id: ì‹¤í–‰ ID

        Returns:
            EvaluationRun: ì¡°íšŒëœ ì—”í‹°í‹°

        ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™:
            - ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆì—ì„œ EvaluationRun ì¬êµ¬ì„±
            - ì¡´ì¬í•˜ì§€ ì•ŠëŠ” run_idì¼ ê²½ìš° None ë°˜í™˜
        """
        pass

    def list_runs(
        self,
        limit: int = 100,
        dataset_name: str | None = None,
        model_name: str | None = None,
    ) -> list[EvaluationRun]:
        """ì €ì¥ëœ í‰ê°€ ì‹¤í–‰ ê²°ê³¼ ëª©ë¡ ì¡°íšŒ

        Args:
            limit: ìµœëŒ€ ê²°ê³¼ ìˆ˜
            dataset_name: ë°ì´í„°ì…‹ ì´ë¦„ í•„í„° (ì„ íƒ)
            model_name: ëª¨ë¸ ì´ë¦„ í•„í„° (ì„ íƒ)

        Returns:
            List[EvaluationRun]: ì¡°íšŒëœ ì—”í‹°í‹° ëª©ë¡

        ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™:
            - ì§€ì •ëœ ì¡°ê±´ì— ë§ëŠ” ì—”í‹°í‹° í•„í„°ë§
            - ìµœì‹ ìˆœ ì •ë ¬
        """
        pass

    def delete_run(self, run_id: str) -> bool:
        """í‰ê°€ ì‹¤í–‰ ê²°ê³¼ ì‚­ì œ

        Args:
            run_id: ì‹¤í–‰ ID

        Returns:
            ì‚­ì œ ì„±ê³µ ì—¬ë¶€
        """
        pass
```

#### 4.3.2.3 êµ¬í˜„ ì˜ˆì‹œ

```python
# SQLite êµ¬í˜„
# src/evalvault/adapters/outbound/storage/sqlite_adapter.py
import sqlite3
from typing import Optional
from evalvault.domain.entities.result import EvaluationRun

class SQLiteStorageAdapter(StoragePort):
    """SQLite ì €ì¥ì†Œ ì–´ëŒ‘í„°."""

    def __init__(self, db_path: str):
        self._db_path = db_path
        self._init_schema()

    def _init_schema(self):
        """ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™”"""
        with sqlite3.connect(self._db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS evaluations (
                    run_id TEXT PRIMARY KEY,
                    dataset_name TEXT,
                    model_name TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    metrics_evaluated TEXT
                )
            """)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS test_case_results (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    run_id TEXT,
                    test_case_id TEXT,
                    question TEXT,
                    answer TEXT,
                    contexts TEXT,
                    ground_truth TEXT,
                    passed INTEGER,
                    FOREIGN KEY (run_id) REFERENCES evaluations(run_id)
                )
            """)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS metric_scores (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    result_id INTEGER,
                    metric_name TEXT,
                    score REAL,
                    threshold REAL,
                    passed INTEGER,
                    FOREIGN KEY (result_id) REFERENCES test_case_results(id)
                )
            """)
            conn.commit()

    def save_run(self, run: EvaluationRun) -> str:
        """í‰ê°€ ì‹¤í–‰ ê²°ê³¼ ì €ì¥"""
        with sqlite3.connect(self._db_path) as conn:
            # evaluations í…Œì´ë¸”ì— ì €ì¥
            conn.execute("""
                INSERT INTO evaluations (run_id, dataset_name, model_name, metrics_evaluated)
                VALUES (?, ?, ?, ?)
            """, (run.run_id, run.dataset_name, run.model_name, ",".join(run.metrics_evaluated)))

            # test_case_results í…Œì´ë¸”ì— ì €ì¥
            for result in run.results:
                cursor = conn.execute("""
                    INSERT INTO test_case_results (run_id, test_case_id, question, answer, contexts, ground_truth, passed)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                """, (
                    run.run_id,
                    result.test_case_id,
                    result.question,
                    result.answer,
                    json.dumps(result.contexts),
                    result.ground_truth,
                    1 if result.passed else 0,
                ))
                result_id = cursor.lastrowid

                # metric_scores í…Œì´ë¸”ì— ì €ì¥
                for metric in result.metrics:
                    conn.execute("""
                        INSERT INTO metric_scores (result_id, metric_name, score, threshold, passed)
                        VALUES (?, ?, ?, ?, ?)
                    """, (
                        result_id,
                        metric.name,
                        metric.score,
                        metric.threshold,
                        1 if metric.passed else 0,
                    ))

            conn.commit()

        return run.run_id

    def get_run(self, run_id: str) -> Optional[EvaluationRun]:
        """ì €ì¥ëœ í‰ê°€ ì‹¤í–‰ ê²°ê³¼ ì¡°íšŒ"""
        with sqlite3.connect(self._db_path) as conn:
            conn.row_factory = sqlite3.Row

            # evaluations í…Œì´ë¸” ì¡°íšŒ
            eval_row = conn.execute("""
                SELECT * FROM evaluations WHERE run_id = ?
            """, (run_id,)).fetchone()

            if not eval_row:
                return None

            # test_case_results í…Œì´ë¸” ì¡°íšŒ
            cursor = conn.execute("""
                SELECT * FROM test_case_results WHERE run_id = ?
            """, (run_id,))
            rows = cursor.fetchall()

            results = []
            for row in rows:
                # metric_scores í…Œì´ë¸” ì¡°íšŒ
                metric_cursor = conn.execute("""
                    SELECT metric_name, score, threshold, passed
                    FROM metric_scores
                    WHERE result_id = ?
                """, (row["id"],))
                metrics = [
                    MetricScore(
                        name=metric_row["metric_name"],
                        score=metric_row["score"],
                        threshold=metric_row["threshold"],
                        passed=bool(metric_row["passed"]),
                    )
                    for metric_row in metric_cursor.fetchall()
                ]

                result = TestCaseResult(
                    test_case_id=row["test_case_id"],
                    question=row["question"],
                    answer=row["answer"],
                    contexts=json.loads(row["contexts"]) if row["contexts"] else [],
                    ground_truth=row["ground_truth"],
                    passed=bool(row["passed"]),
                    metrics=metrics,
                )
                results.append(result)

            return EvaluationRun(
                run_id=eval_row["run_id"],
                dataset_name=eval_row["dataset_name"],
                model_name=eval_row["model_name"],
                results=results,
                metrics_evaluated=eval_row["metrics_evaluated"].split(",") if eval_row["metrics_evaluated"] else [],
                thresholds={},
            )

# PostgreSQL êµ¬í˜„ (ì„ íƒ)
# src/evalvault/adapters/outbound/storage/postgres_adapter.py
class PostgreSQLStorageAdapter(StoragePort):
    """PostgreSQL ì €ì¥ì†Œ ì–´ëŒ‘í„°."""

    def __init__(self, connection_string: str):
        self._conn = psycopg2.connect(connection_string)
        self._init_schema()

    # ì €ì¥ì†Œ ì´ˆê¸°í™”, ë©”ì„œë“œ êµ¬í˜„...
    pass
```

#### 4.3.2.4 ì „ë¬¸ê°€ ê´€ì  ì ìš©

**[ì•„í‚¤í…íŠ¸ ê´€ì ]**
- **ì ìš© ì›ì¹™**: í¬íŠ¸ ê¸°ë°˜ ì„¤ê³„ (Ports & Adapters)
- **ì‹¤ì œ êµ¬í˜„**:
  - StoragePort ì¸í„°í˜ì´ìŠ¤ë¡œ ì¶”ìƒí™”
  - ê° ì €ì¥ì†Œ(Adapter)ê°€ ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„
  - ì˜ì¡´ì„± ë°©í–¥: Adapter â†’ Port â†’ Domain

**[í™•ì¥ì„± ê´€ì ]**
- **ì ìš© ì›ì¹™**: ìƒˆë¡œìš´ ì €ì¥ì†Œ ì¶”ê°€ ìš©ì´
- **ì‹¤ì œ êµ¬í˜„**:
  - ìƒˆë¡œìš´ ì €ì¥ì†Œ(MySQL, MongoDB, Redis ë“±) ì¶”ê°€ ì‹œ StoragePortë§Œ êµ¬í˜„
  - ë„ë©”ì¸ ì½”ë“œ ìˆ˜ì • ì—†ìŒ (ExperimentManagerëŠ” StoragePort ì¸í„°í˜ì´ìŠ¤ë§Œ ì‚¬ìš©)

**[ë°ì´í„° ë¬´ê²°ì„±ì„±(ACID) ê´€ì ]**
- **ì ìš© ì›ì¹™**: ë°ì´í„°ë² ì´ìŠ¤ êµì²´ ì‹œ ë„ë©”ì¸ ë¡œì§ ë¬´ê²°ì„±
- **ì‹¤ì œ êµ¬í˜„**:
  - SQLite â†’ PostgreSQL êµí™˜ ì‹œ ë„ë©”ì¸ ì½”ë“œ(RagasEvaluator, ExperimentManager) ìˆ˜ì • ë¶ˆí•„ìš”
  - StoragePort êµ¬í˜„ì²´ë§Œ êµí™˜

### 4.4 ì–´ëŒ‘í„° (Adapters)

### 4.4.1 CLI Adapter

**Whitepaper Reference**:
- Section 2.3: ì–´ëŒ‘í„° ê³„ì¸µ - Inbound Adapters
- Section 4.5: ì‚¬ìš© ê°€ì´ë“œ - CLI ì‚¬ìš©ë²•

#### 4.4.1.1 ê°œìš”

CLI AdapterëŠ” Typer ê¸°ë°˜ ì»¤ë§¨ë“œë¼ì¸ ì¸í„°í˜ì´ìŠ¤ì…ë‹ˆë‹¤. ì‚¬ìš©ìê°€ í„°ë¯¸ë„ì—ì„œ í‰ê°€ë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.

#### 4.4.1.2 ì£¼ìš” ëª…ë ¹

| ëª…ë ¹ | ì„¤ëª… | êµ¬í˜„ |
|------|------|------|
| `run` | í‰ê°€ ì‹¤í–‰ | `adapters/inbound/cli/commands/run.py` |
| `run-simple` | ì‹¬í”Œ ëª¨ë“œ í‰ê°€ ì‹¤í–‰ | `adapters/inbound/cli/commands/run.py` |
| `run-full` | ì „ì²´ ëª¨ë“œ í‰ê°€ ì‹¤í–‰ | `adapters/inbound/cli/commands/run.py` |
| `generate` | í…ŒìŠ¤íŠ¸ì…‹/ì§€ì‹ ê·¸ë˜í”„ ìƒì„± | `adapters/inbound/cli/commands/generate.py` |
| `history` | ì‹¤í–‰ íˆìŠ¤í† ë¦¬ ì¡°íšŒ | `adapters/inbound/cli/commands/history.py` |
| `compare` | ë‘ ì‹¤í–‰ ê²°ê³¼ ë¹„êµ | `adapters/inbound/cli/commands/compare.py` |
| `pipeline` | ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ | `adapters/inbound/cli/commands/pipeline.py` |
| `benchmark` | ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ | `adapters/inbound/cli/commands/benchmark.py` |
| `gate` | íšŒê·€ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ | `adapters/inbound/cli/commands/gate.py` |
| `domain` | ë„ë©”ì¸ ë©”ëª¨ë¦¬ ê´€ë¦¬ | `adapters/inbound/cli/commands/domain.py` |
| `stage` | Stage ë©”íŠ¸ë¦­ ì¡°íšŒ | `adapters/inbound/cli/commands/stage.py` |

#### 4.4.1.3 êµ¬í˜„ ì˜ˆì‹œ

```python
# src/evalvault/adapters/inbound/cli.py
import typer
from rich.console import Console
from rich.table import Table

app = typer.Typer(
    name="evalvault",
    help="RAG evaluation system using Ragas with Phoenix/Langfuse tracing",
    add_completion=False,
)

console = Console()

@app.command()
def run(
    dataset: Path,
    metrics: str,
    model: str | None = None,
    profile: str = "dev",
    use_domain_memory: bool = False,
    augment_context: bool = False,
    parallel: bool = True,
    batch_size: int = 10,
):
    """Run RAG evaluation on a dataset."""
    # 1. ì…ë ¥ íŒŒì‹±
    metric_list = [m.strip() for m in metrics.split(",")]

    # 2. ì„¤ì • ë¡œë“œ
    from evalvault.config.settings import Settings
    settings = Settings()

    # 3. ì–´ëŒ‘í„° ìƒì„± (Factory íŒ¨í„´)
    from evalvault.adapters.outbound.dataset.loader_factory import get_loader
    from evalvault.adapters.outbound.llm.llm_factory import get_llm_adapter
    from evalvault.domain.services.evaluator import RagasEvaluator

    loader = get_loader(dataset)
    llm = get_llm_adapter(settings, profile)

    # 4. ë„ë©”ì¸ ì„œë¹„ìŠ¤ í˜¸ì¶œ
    evaluator = RagasEvaluator(llm=llm)

    # Domain Memory ì´ˆê¸°í™” (ì„ íƒì )
    if use_domain_memory:
        from evalvault.adapters.outbound.domain_memory.sqlite_adapter import get_domain_memory_adapter
        from evalvault.domain.services.memory_aware_evaluator import MemoryAwareEvaluator

        memory_port = get_domain_memory_adapter(settings)
        evaluator = MemoryAwareEvaluator(evaluator, memory_port)

    # 5. í‰ê°€ ì‹¤í–‰
    import asyncio
    result = asyncio.run(evaluator.evaluate(
        loader.load(dataset),
        metric_list,
        parallel=parallel,
        batch_size=batch_size,
    ))

    # 6. ê²°ê³¼ í¬ë§·íŒ… ë° ì¶œë ¥
    _display_results(result)

    # 7. ì €ì¥ (ì„ íƒì )
    if settings.db_path:
        from evalvault.adapters.outbound.storage.storage_factory import get_storage_adapter
        storage = get_storage_adapter(settings)
        storage.save_run(result)

@app.command()
def history(
    db: str = "data/db/evalvault.db",
    limit: int = 10,
    dataset_name: str | None = None,
    model_name: str | None = None,
):
    """Show evaluation history."""
    from evalvault.adapters.outbound.storage.storage_factory import get_storage_adapter
    from evalvault.config.settings import Settings

    settings = Settings()
    storage = get_storage_adapter(settings, db_path=db)

    # ì €ì¥ì†Œì—ì„œ íˆìŠ¤í† ë¦¬ ì¡°íšŒ
    runs = storage.list_runs(limit=limit, dataset_name=dataset_name, model_name=model_name)

    # í…Œì´ë¸” í˜•ì‹ìœ¼ë¡œ í‘œì‹œ
    table = Table(title="Evaluation History")
    table.add_column("Run ID", style="cyan")
    table.add_column("Dataset", style="green")
    table.add_column("Model", style="blue")
    table.add_column("Metrics", style="yellow")
    table.add_column("Pass Rate", style="magenta")
    table.add_column("Created", style="dim")

    for run in runs:
        table.add_row(
            run.run_id[:8],
            run.dataset_name,
            run.model_name,
            ",".join(run.metrics_evaluated[:3]) + "..." if len(run.metrics_evaluated) > 3 else ",".join(run.metrics_evaluated),
            f"{run.pass_rate:.2%}",
            run.created_at,
        )

    console.print(table)

@app.command()
def compare(
    run_a: str,
    run_b: str,
    db: str = "data/db/evalvault.db",
):
    """Compare two evaluation runs."""
    from evalvault.adapters.outbound.storage.storage_factory import get_storage_adapter
    from evalvault.config.settings import Settings
    from evalvault.domain.services.experiment_comparator import ExperimentComparator

    settings = Settings()
    storage = get_storage_adapter(settings, db_path=db)

    # ë‘ ì‹¤í–‰ ì¡°íšŒ
    run_a_data = storage.get_run(run_a)
    run_b_data = storage.get_run(run_b)

    # ë¹„êµ ìˆ˜í–‰
    comparator = ExperimentComparator()
    comparison_result = comparator.compare(run_a_data, run_b_data)

    # ê²°ê³¼ í‘œì‹œ
    _display_comparison(comparison_result)

def _display_results(result: EvaluationRun):
    """ê²°ê³¼ë¥¼ í…Œì´ë¸” í˜•ì‹ìœ¼ë¡œ í‘œì‹œ"""
    # ë©”íŠ¸ë¦­ ìš”ì•½ í…Œì´ë¸”
    table = Table(title="Evaluation Results")
    table.add_column("Metric", style="cyan")
    table.add_column("Avg Score", style="green")
    table.add_column("Threshold", style="yellow")
    table.add_column("Pass Rate", style="magenta")

    for metric in result.metrics_evaluated:
        avg_score = result.get_avg_score(metric)
        threshold = result.thresholds.get(metric, 0.7)
        pass_count = sum(1 for r in result.results if r.get_metric(metric) and r.get_metric(metric).passed)
        pass_rate = pass_count / len(result.results)

        table.add_row(
            metric,
            f"{avg_score:.3f}",
            f"{threshold:.2f}",
            f"{pass_rate:.2%}",
        )

    console.print(table)

    # í†µê³¼ìœ¨ í‘œì‹œ
    console.print(f"\nâœ… Overall Pass Rate: {result.pass_rate:.2%}")
    console.print(f"ğŸ“Š Total Test Cases: {len(result.results)}")
```

---

## ì—…ë°ì´íŠ¸ ì´ë ¥

| ë²„ì „ | ë‚ ì§œ | ë³€ê²½ ì‚¬í•­ | ë‹´ë‹¹ |
|------|------|----------|------|
| 1.0.0 | 2026-01-10 | ì´ˆê¸° ì‘ì„± | EvalVault Team |

## ê´€ë ¨ ì„¹ì…˜

- ì„¹ì…˜ 1: í”„ë¡œì íŠ¸ ê°œìš”
- ì„¹ì…˜ 2: ì•„í‚¤í…ì²˜ ì„¤ê³„
- ì„¹ì…˜ 3: ë°ì´í„° íë¦„ ë¶„ì„
- ì„¹ì…˜ 5: ì „ë¬¸ê°€ ê´€ì  í†µí•© ì„¤ê³„
